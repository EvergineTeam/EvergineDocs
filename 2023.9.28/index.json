{
  "api/index.html": {
    "href": "api/index.html",
    "title": "Evergine API | Evergine Doc",
    "keywords": "Evergine API This section of the documentation contains details of the C# API that Evergine provides. To use this information, you should be familiar with the basic theory and practice of coding in Evergine which is explained in the Basics section of our manual. API are grouped by namespaces they belong to, and can be selected from the sidebar to the left."
  },
  "index.html": {
    "href": "index.html",
    "title": "Evergine Documentation | Evergine Doc",
    "keywords": "Evergine Documentation Manual Learn how to create applications with Evergine! C# API Reference Full C# scripting API reference."
  },
  "manual/addons/arr/getting_started.html": {
    "href": "manual/addons/arr/getting_started.html",
    "title": "Getting started | Evergine Doc",
    "keywords": "Getting started Follow the steps below to quickly start running Azure Remote Rendering on your Evergine project: Create an Azure Remote Rendering instance at Azure portal (you need an Azure account). Once created, copy your account id, key, and domain. You will need them to configure the session on your app. Create a new Evergine project, or open an existing one. Install the Evergine.ARR add-on using the add-on managemer in Evergine Studio. Add the Azure Remote Rendering service to your Application public partial class MainApplication : Application { public MainApplication() { ... this.Container.RegisterInstance(new AzureRemoteRenderingService()); ForegroundTaskScheduler.Foreground.Configure(this.Container); BackgroundTaskScheduler.Background.Configure(this.Container); } ... Go to Evergine Studio, add the ARRSessionManager component on the Environment manager entity and configure it using your credentials. Add the Azure Remote Rendering proxy camera component to the main camera. For each big model that you want to render remotely, add an entity with ARRModelLoader and ARREntitySync components. Note The Url of your model can be obtained from Azure Remote Rendering instance in Azure Portal. If you don't have any, you can use the default builtin://Engine. And that's it! The project should be ready to render the model(s) on windows or other platforms like UWP or Mixed Reality. Check out the demo for more information or the MRTK add-on to add AR controls to your project and start interacting with your models."
  },
  "manual/addons/arr/index.html": {
    "href": "manual/addons/arr/index.html",
    "title": "Azure Remote Rendering (ARR) | Evergine Doc",
    "keywords": "Azure Remote Rendering (ARR) Azure Remote Rendering brings your highest quality 3D content and interactive experiences to mixed reality. This service uses the computing power of Azure to render even the most complex models in the cloud and streams them in real time to your devices, so that users can interact and collaborate with 3D content in amazing detail. It’s an open-source project and it is located in this repository. In this section Getting started"
  },
  "manual/addons/index.html": {
    "href": "manual/addons/index.html",
    "title": "Add-ons | Evergine Doc",
    "keywords": "Add-ons Add-ons are Evergine packages that add specific functionalities and assets to your project. They are meant resolve some key issues that companies face when they create industrial 3D application using their common resources. They may contain assets, prefabs, behaviors, components and nuget packages that will be incorporated to your project after install. Available Add-ons MRTK ARR XRV Add-ons in Evergine Studio To check wich packages are installed in your application, you just need to take a look to the Project Explorer tab in Evergine Studio, and inspect the Dependencies section: Add-ons Manager The Add-ons manager help you installing new ones. There are 3 posibilities to open it: Use File menu, where you will find a new “Manage dependencies” menu item. Under Assets explorer, there is a node with name “Dependencies”. Right click on that will show a context menu with a menu entry with the same name in the point above. Access to “Project settings”, there are two tabs now, one for project profiles, and other for project add-ons. The manager contains two tabs: Browse, where you can find all available add-ons; and Installed, where you can check all add-ons that are currently in use in your project. Both tabs contain a similar user interface. Above those tabs, you will find a searching area: Text search, to filter results by name and tags. Source selector, to limit results to an specific packages source. Depending on search criteria, number of displayed add-ons may vary. Each add-on item within the list displays following information: Name, icon and description. Under the icon, in bottom left corner, it will display a tick inside a green circle if you are already using last available version, or an arrow inside a blue circle if you are using that add-on, but there is a newer version available. On top right corner, you will find latest available version vs installed version (if any). For those add-ons which last version is already in use, a single version label is displayed. Also, if you move your mouse over list items, some buttons to install last add-on version or remove an add-on should appear. When selecting an item in the list, a detail view is loaded with all the information related to an add-on. There, you can also install or uninstall the add-on, and you are even able to install a specific version. Note the NuGet dependencies section, that indicates minimal versions of engine and/or third-party NuGet packages. As it is mandatory to have aligned versions of the engine packages, if you try to install an add-on with an engine dependency higher than the one you are using, a project restart will be required and automatically scheduled. For add-ons which NuGet dependencies are not expressed as explicit version number, you should install those packages manually. In other hand, for third-party NuGet dependencies with explicit version number, they will be added or updated automatically once package is added. Customize assets By default, all assets included in a package cannot be modified. This is indicated by this lock icon: . However, you can modify an asset included in the package. When you modify a package asset, and try to save it, the following message apperas, indicating if you want to create a new copy of this asset in your project: Automatically a new copy of the asset will be created on your local folder project, overwriting the one provided by the package. By doing this allows you to modify or adapt Evergine core assets to your application requirements Develop new Add-ons The ability to include new add-ons to external companies will be available in future releases. For now only add-ons created by Evergine Team can be installed through Evergine Studio. Upcoming packages Evergine is on continuous development, and more upcoming packages are coming: Package Description Noesis GUI Using NoesisGUI with Evergine allows you to create appealing User Interfaces using powerful XAML tools like Microsoft Expression Blend. CAD Formats Take your industrial CAD/3D model and bring it into your Evergine application. Bing Maps Provides Bing Maps integration with Evergine, that allows you to utilize Bing Maps 3D mapping data in your application."
  },
  "manual/addons/mrtk/configurators.html": {
    "href": "manual/addons/mrtk/configurators.html",
    "title": "Configurators | Evergine Doc",
    "keywords": "Configurators The configurator components were created to ease the customization process and, in some cases, extend the functionality of certain controls. To use them, add them to the prefab root after instancing a prefab. There are some examples for all configurators in the test scenes in the MRTK Demo project. Standard button configurator This configurator can manage the configuration of certain aspects of a basic button, like the icon, the button text and the material plate. If the color should also be changed, the CreatesNewIconMaterialInstance and CreatesNewBackPlateMaterialInstance checkboxes will do so that the change does not affect other instances. ToggleButton When added to a button, the ToggleButton component extends its base functionality, so the button will have two different styles depending on whether it is in the ON or the OFF state. This component adds two ToggleButtonConfigurator components, which can be used to configure each state in a very similar way as the StandardButtonConfigurator. MultiState buttons The ToggleButton component adds the ToggleStateManager and two ToggleButtonConfigurators to the button. There's also an implementation for tristate buttons by way of adding a MultiStateStateManager and three MultiStateButtonConfigurator components to a button. All these *StateManager classes extend the BaseStateManager class using an enum with all the states a button can have. If more than three states are needed, a new class can be created using this base class and a custom enum. The configurator components also need to be implemented using this enum, so each one is associated to a single state. Slider configurator This configurator contains specific settings to change the track and thumb materials and the label text in a slider."
  },
  "manual/addons/mrtk/custom_controls.html": {
    "href": "manual/addons/mrtk/custom_controls.html",
    "title": "Creating custom controls | Evergine Doc",
    "keywords": "Creating custom controls All MRTK controls use custom interfaces in order to receive user interaction events. In order to receive these events, most components need to be added to an entity that has a BoxCollider component and a StaticBody component, so that they are properly managed in the physics engine. Focus events Components that implement the IMixedRealityFocusHandler interface will receive focus events. A control gets focus in the following scenarios: When a near pointer is close to the entity. When a far pointer is pointing to the entity using the ray. When the user is looking at the entity (using the GazeProvider). The default button style makes use of the focus event by raising the icon and text when acquiring focus. Touch events Components that implement the IMixedRealityTouchHandler interface will receive events when the near pointer is making contact with the entity's collider. These events are used in the PressableButton component (which is part of the standard button). It enables the main feature of the button. Pointer events Components that implement the IMixedRealityPointerHandler interface will receive pointer events: Pointer down: when the user grabs an object, either by near interaction or far interaction, using the air-tap gesture. Pointer dragged: when the user has already grabbed the object and is still interacting with it, so the object can receive position updates. Pointer up: when the user has stopped interacting with the object. These are used for example in the manipulation handlers, which allow the user to grab an object and move, rotate or scale it arbitrarily. Speech events Components that implement the IMixedRealitySpeechHandler interface will receive an event whenever the user performs voice commands. These can be used to remotely activate buttons or to perform global actions in the application. These components do not need to be attached to an entity with a collider, since they don't rely on the physics engine."
  },
  "manual/addons/mrtk/demo_project.html": {
    "href": "manual/addons/mrtk/demo_project.html",
    "title": "Demo project | Evergine Doc",
    "keywords": "Demo project There is a demo scene in the project's repository which showcases all currently implemented features. Press interaction This area contains some examples using the PressableButton class. This includes: Standard buttons Toggle buttons Some piano keys Touch interaction This area contains some simple examples using the HandInteractionTouch which implements a simple touch behavior. These work only with near interaction. Slider interaction This area includes some sliders which use the PinchSlider class. They are connected to another object and allow the user to change its color. Manipulation interaction These examples showcase the SimpleManipulationHandler component. Some of the examples are configured in such a way that manipulation is restricted in some ways. In addition, some objects interact with the physics engine, so they can be thrown around. If they get too far away, they will be placed back on their starting position. Bounding box The examples in this group use the BoundingBox component, which adds some handles to an object to aid with rotation and uniform and non-uniform scaling. These handles can be hidden if that kind of manipulation is not needed. This component can be combined with the SimpleManipulationHandler component for more flexible manipulation options. Axis manipulation handler There's an example of the AxisManipulationHandler component. With it, the user will see a 3-axis handle which can be moved in any axis or combination of axes independently, without affecting the rest. Pan and zoom control The HandInteractionPanZoom example shows this component in action. The user can pan and zoom its contents using both near and far interaction."
  },
  "manual/addons/mrtk/getting_started.html": {
    "href": "manual/addons/mrtk/getting_started.html",
    "title": "Getting started | Evergine Doc",
    "keywords": "Getting started These are the initial steps to follow in order to get started using our Mixed Reality Toolkit: Create a new Evergine project, or open an existing one. Install the Evergine.MRTK add-on using the add-on management option in Evergine Studio. Change all scenes that will use MRTK functionality so they extend MRTK's XRScene class instead of Evergine's Scene class. There are some material properties that the scene uses, which need to be filled in. using System; using Evergine.MRTK.Scenes; namespace Evergine.MRTK.Demo.Scenes { public class DemoScene : XRScene { protected override Guid CursorMatPressed => EvergineContent.MRTK.Materials.Cursor.CursorPinch; protected override Guid CursorMatReleased => EvergineContent.MRTK.Materials.Cursor.CursorBase; protected override Guid HoloHandsMat => EvergineContent.MRTK.Materials.HoloHands; protected override Guid SpatialMappingMat => Guid.Empty; protected override Guid HandRayTexture => EvergineContent.MRTK.Textures.line_dots_png; protected override Guid HandRaySampler => EvergineContent.MRTK.Samplers.LinearWrapSampler; } } Note There will probably be an error related to the CreateScene method, which is sealed in the XRScene class. If the scene needs to be modified via code, this can be done by overriding the OnPostCreateXRScene instead. Register an instance of the NoesisService class in the Application class. var noesisService = new NoesisService(); this.Container.RegisterInstance(noesisService); And that's it! The project should be ready to use MRTK components."
  },
  "manual/addons/mrtk/index.html": {
    "href": "manual/addons/mrtk/index.html",
    "title": "Mixed Reality Toolkit (MRTK) | Evergine Doc",
    "keywords": "Mixed Reality Toolkit (MRTK) Evergine’s Mixed Reality Toolkit (or MRTK for short) offers an array of components and functionalities to make Mixed Reality applications development easier. It’s an open-source project and it is located in this repository. In this section Getting started Pointers and control Using prefabs and customization Configurators Creating custom controls Demo project"
  },
  "manual/addons/mrtk/pointers_and_control.html": {
    "href": "manual/addons/mrtk/pointers_and_control.html",
    "title": "Pointers and control | Evergine Doc",
    "keywords": "Pointers and control The MRTK changes the way to interact with controls. Instead of using a mouse pointer to click on buttons, the hand tracking feature available in some devices can be used to interact with controls. There are two main ways of interaction: Near interaction: when near a control, the user can touch it with their index finger to interact with it. The mechanism that enables this is called a near pointer. Far interaction: if the user is far away but they want to interact with a control, they can use the far pointer. Using hand tracking, this appears as a light ray coming out of their hand, which can be used to point and click. The click gesture is know as air-tap. Example of near pointer usage Example of far pointer usage The default pointers for hand tracking enabled devices are created automatically in the XRScene. Desktop development These mechanisms can also be used in the desktop solution in an Evergine project. The pointers can be controlled using the keyboard. Press and hold either the left shift or space key to enable either pointer (right and left hand respectively). This enables moving the pointer using the mouse. Use the mouse wheel to move the enabled pointer closer or further away from the camera. Use the left mouse button to perform the air-tap gesture and interact with controls placed far from the cursor. Press and hold the left control key to rotate the enabled pointer using the mouse."
  },
  "manual/addons/mrtk/prefabs.html": {
    "href": "manual/addons/mrtk/prefabs.html",
    "title": "Using prefabs and customization | Evergine Doc",
    "keywords": "Using prefabs and customization The MRTK contains some controls in the form of prefabs that are useful when developing applications, the most basic ones being the button and the slider. They can be found in the Prefabs folder in the MRTK dependency. Buttons Sliders Customization When a control prefab is instantiated in Evergine, its hierarchy will be shown in Evergine Studio. This is one way to customize the look of that particular instance. Button prefab hierarchy Button before and after customization Slider prefab hierarchy Slider before and after customization"
  },
  "manual/addons/xrv/getting_started.html": {
    "href": "manual/addons/xrv/getting_started.html",
    "title": "Getting started | Evergine Doc",
    "keywords": "Getting started In this section we are presenting step-by-step instructions to be ready to execute XRV in your device. Project setup Create a new project using Evergine Launcher. You should include an extra template project, apart from Windows, for your target device. For example, with Meta Quest headset, you can choose between Xamarin and/or .NET 6 project templates. Once Evergine Studio is opened, add a MRTK add-on. You can check how to add add-ons to an existing project. With MRTK add-on installed, you need to add Evergine.XRV.Core add-on using project management dialog again. Note All XRV add-ons have an associated NuGet package. In the same way as Evergine packages, nightly builds of XRV are available on a public NuGet feed. Preview packages will be published in Nuget.org. So, for nightly builds, you should update your nuget.config file to include Evergine nightly feed: <?xml version=\"1.0\" encoding=\"utf-8\"?> <configuration> <packageSources> <add key=\"nuget.org\" value=\"https://api.nuget.org/v3/index.json\" /> <add key=\"Evergine Nightly\" value=\"https://pkgs.dev.azure.com/plainconcepts/Evergine.Nightly/_packaging/Evergine.NightlyBuilds/nuget/v3/index.json\" /> </packageSources> </configuration> Change default scene SunLight entity value for Illuminance to a value of 1 Code setup Register background scheduler in your Application constructor. public MyApplication() { this.Container.RegisterType<Settings>(); this.Container.RegisterType<Clock>(); this.Container.RegisterType<TimerFactory>(); this.Container.RegisterType<Random>(); this.Container.RegisterType<ErrorHandler>(); this.Container.RegisterType<ScreenContextManager>(); this.Container.RegisterType<GraphicsPresenter>(); this.Container.RegisterType<AssetsDirectory>(); this.Container.RegisterType<AssetsService>(); this.Container.RegisterType<ForegroundTaskSchedulerService>(); this.Container.RegisterType<WorkActionScheduler>(); BackgroundTaskScheduler.Background.Configure(this.Container); } Change your scene class to implement XRScene. public class MyScene : XRScene { protected override Guid CursorMatPressed => EvergineContent.MRTK.Materials.Cursor.CursorPinch; protected override Guid CursorMatReleased => EvergineContent.MRTK.Materials.Cursor.CursorBase; protected override Guid HoloHandsMat => EvergineContent.MRTK.Materials.Hands.QuestHands; protected override Guid SpatialMappingMat => Guid.Empty; protected override Guid HandRayTexture => EvergineContent.MRTK.Textures.line_dots_png; protected override Guid HandRaySampler => EvergineContent.MRTK.Samplers.LinearWrapSampler; protected override Guid LeftControllerModelPrefab => Guid.Empty; protected override Guid RightControllerModelPrefab => Guid.Empty; protected override float MaxFarCursorLength => 0.5f; //... } Add Microsoft.Bcl.AsyncInterfaces to shared project. <PackageReference Include=\"Microsoft.Bcl.AsyncInterfaces\" Version=\"7.0.0\" /> Finally, create XrvService instance and initialize it under OnPostCreateXRScene. MyApplication.cs public override void Initialize() { base.Initialize(); this.InitializeXrv(); // ... } private void InitializeXrv() { var xrv = new XrvService(); this.Container.RegisterInstance(xrv); } MyScene.cs protected override void OnPostCreateXRScene() { base.OnPostCreateXRScene(); var xrv = Application.Current.Container.Resolve<XrvService>(); xrv.Initialize(this); } Platform setup Android In some platforms like Android, you may find build errors like this. error XA2002: Can not resolve reference: `Evergine.Editor.Extension`, referenced by `Evergine.MRTK.Editor`. Please add a NuGet package or assembly reference for `Evergine.Editor.Extension`, or remove the reference to `Evergine.MRTK.Editor`. Just add Evergine.Editor.Extension to your project and it should work. Also, to make use of passthrough capability, remember to uncomment related parts of code in MainActivity.cs and in Android manifest file. UWP (Mixed Reality) In UWP you may find some PRI generation erros, that you can fix editing your project file and adding the following: <AppxGeneratePrisForPortableLibrariesEnabled>false</AppxGeneratePrisForPortableLibrariesEnabled> Also, if you add modules that require internet access or want to make use of voice commands, review that you enable Internet Client and Microphone capabilities. For voice commands, you should also add an explicit reference in Mixed Reality project to Evergine.Xrv.Core NuGet package. Add more modules With all this, you can run application, but the only thing you could do is open hand menu and its two default buttons to open Settings and Help windows. To add more functionalities, you can add any of the existing XRV modules, create your own module or just add new elements using XRV API. You can also take a look to our XRV sample that includes all our public modules."
  },
  "manual/addons/xrv/hand_menu.html": {
    "href": "manual/addons/xrv/hand_menu.html",
    "title": "Hand menu | Evergine Doc",
    "keywords": "Hand menu One of the main features that XRV offers is the hand menu. This is an element that shows a set of buttons that can be added or removed programmatically, and which container is attached to user's wrist. When user turns his palm, menu will be displayed or hidden depending on palm orientation. It works for both left and right hands. Each one of the configured modules for an XRV application, have the possibility to add a new button to this menu. For this, in module definition class just create an instance for HandMenuButton property. public class MyModule : Module { public override MenuButtonDescription HandMenuButton { get; protected set; } public override void Initialize(Scene scene) { this.HandMenuButton = new MenuButtonDescription { // Button configuration here }; } } If you set a null value to this property, no button will be automatically added to the hand menu. Hand menu button configuration Here you will find a table with elements that can be configured for each one of hand menu buttons using MenuButtonDescription. Property Description IconOn Material identifier to be used when button is in on state for a toggle button, or just button icon for a non-toggle button. IconOff Material identifier to be used when button is in off state (for toggle buttons only). IsToggle Indicates if button should be a toggle button or not. TextOn Button text to be used when button is in on state for a toggle button, or just button icon for a non-toggle button. This property is a Func<string> to let user to define different values depending on application display language. TextOff Button text to be used when button is in off state (for toggle buttons only). This property is a Func<string> to let user to define different values depending on application display language. VoiceCommandOn Voice command to activate button when it is in on state for a toggle button, or just button icon for a non-toggle button. VoiceCommandOff Voice command to activate button when it is in off state (for toggle buttons only). Attaching/detaching hand menu User has the possibility to detach menu from his hand, using Detach button located at the top of the menu. Doing this, menu will change its layout and behave like a standard window. User can make it to follow him or just stay pinned whenever user wants. To bring menu back to the wrist, user just have to press detached menu Close button. Adding buttons programmatically You can also add or remove buttons programmatically, apart from button that you may have added (or not) from your own application module definitions. Notice that XrvService exposes a HandMenu property that will let you apply some custom configuration to the menu. Menu buttons collection can be modified at runtime. var xrv = Application.Current.Container.Resolve<XrvService>(); var handMenu = xrv.HandMenu; var buttonDefinition = new MenuButtonDescription { // Button configuration here } // adding a new button handMenu.ButtonDescriptors.Add(buttonDefinition); // removing a defined button handMenu.ButtonDescriptors.Remove(buttonDefinition); Hand menu layout We don't have support to modify built-in hand menu shape or layout. You can just modify number of buttons per column. This property has a minimal size of 4 buttons per column, as this is necessary for detached menu state. To modify number of buttons per column just use following code snippet. handMenu.ButtonsPerColumn = 5; Hand menu starting tutorial An tutorial animation can be displayed every time application is started, to let users know that the should turn their hand if they want to display the menu and interact with the application. To hide this animation on application start, you can do it with this code snippet. Note that you should deactivate the flag after XRV initialization (hand menu will not available before that). var xrv = Application.Current.Container.Resolve<XrvService>(); xrv.Initialize(this); xrv.HandMenu.DisplayTutorial = false;"
  },
  "manual/addons/xrv/help_system.html": {
    "href": "manual/addons/xrv/help_system.html",
    "title": "Help System | Evergine Doc",
    "keywords": "Help System Other of the predefined windows that XRV offers is Help window. This window is intended to contain some text and images guidance for application users, to learn how to use it. It works in the same way as Settings System: you can associate a help section to your custom module, or you can add or remove items programmatically. To open Help window, just press button that you can find in hand menu. This window is a TabbedWindow and you have two ways of adding new elements. Adding a help section to your custom module public class MyModule : Module { public override TabItem Settings { get; protected set; } public override void Initialize(Scene scene) { this.Help = new TabItem() { Name = () => \"Module Name\", Contents = this.CreateContents() // Entity with help item contents. }; } } Using HelpSystem API var help = this.xrvService.HelpSystem; var item = new TabItem { Order = 1, Name = \"My item\", Contents = () => this.CreateContents(), }; help.AddTabItem(item); You can also remove an existing item using RemoveTabItem method."
  },
  "manual/addons/xrv/index.html": {
    "href": "manual/addons/xrv/index.html",
    "title": "Extended Reality Viewer (XRV) | Evergine Doc",
    "keywords": "Extended Reality Viewer (XRV) XRV is a library that we are using internally while developing custom XR experiences for our customers. It tries to gather in a single place different functionalities and features commonly required by applications we build, like floating windows, hand menu, voice commands, etc. It is based on our own MRTK add-on, so it uses the same input pointers, user controls like buttons or sliders, configuration components, etc. Based on that we have built all XRV infrastructure. Most of the features are supported for many platforms, but some of them are not for some of those platforms. You can run XRV for devices like: Meta Quest and Meta Quest 2 Meta Quest Pro Pico XR headsets Microsoft HoloLens 2 In this section Getting started Hand menu UI Windows System Tabs control Settings System Help System Voice commands Messaging Storage Themes System Localization Modules Image Gallery Models Viewer Painter Ruler Streaming Viewer Custom Modules"
  },
  "manual/addons/xrv/localization.html": {
    "href": "manual/addons/xrv/localization.html",
    "title": "Localization | Evergine Doc",
    "keywords": "Localization If you want to create an application that supports different target languages, you may find useful localization mechanism provided by XRV. It scans your assemblies looking for embedded resource files (.resx), and provides a set of Evergine components that let you choose a dictionary name-entry pair for a 3D texts or buttons. Lookup assemblies must be decorated with EvergineAssembly attribute with UserProject or Extension as value. Note In current state, we only support English (fallback) and Spanish as available languages for applications. We plan to add extension points in the future to allow developers adding new languages. You can easily change current UI culture var localization = this.xrvService.Localization; localization.CurrentCulture = CultureInfo.GetCultureInfo(\"es\"); When culture changes, a CurrentCultureChangeMessage message is published in PubSub, indicating the value of the new UI culture. It also changes current thread CurrentUICulture and CurrentCulture values. Built-in components for localization We provide a set of components to control localization for button texts and 3D texts: Text3dLocalization: to have localized text for Text3DMesh components. ButtonLocalization: you can localize buttons text for entities with StandardButtonConfigurator component. ToggleButtonLocalization: you can localize toggle buttons text for entities with ToggleStateManager component. You should have one component instance for each one of toggle states. For a toggle button, as we said, you must add one component for each one of toggle states. Get localized string from code To retrieve a localized string, just use of localization service. var localization = this.xrvService.Localization; var localizedString = this.localization.GetString(() => Resources.Strings.MyString); Hand menu buttons MenuButtonDescription has a way to set localized text for hand menu buttons. If your button is a toggle button, you can also indicate different strings for each one of the toggle states. var localization = this.xrvService.Localization; var description = new MenuButtonDescription() { TextOn = () => localization.GetString(() => Resources.Strings.MyString), TextOff = () => localization.GetString(() => Resources.Strings.MyString), }; Tab items TabItem lets you to set a Func that will be invoked on first run or when current culture changes. var localization = this.xrvService.Localization; var item = new TabItem() { Name = () => localization.GetString(() => Resources.Strings.MyString), Contents = this.HelpContent, }; Window title WindowConfigurator also uses an specific Func property to have localized text for window title. var windowsSystem = this.xrvService.WindowsSystem; var localization = this.xrvService.Localization; var window = windowsSystem.CreateWindow(config => { config.LocalizedTitle = () => localization.GetString(() => Resources.Strings.MyString), }); Alert dialogs WindowsSystem API has overload methods for both ShowAlertDialog and ShowConfirmationDialog, where you can assign Func callbacks to have localized dialogs. var windowsSystem = this.xrvService.WindowsSystem; var dialog = windowsSystem.ShowAlertDialog( () => localization.GetString(() => Resources.Strings.MyAlertTitle), () => localization.GetString(() => Resources.Strings.MyAlertMessage), () => localization.GetString(() => Resources.Strings.MyAlertOk));"
  },
  "manual/addons/xrv/logging.html": {
    "href": "manual/addons/xrv/logging.html",
    "title": "Logging | Evergine Doc",
    "keywords": "Logging We provide logging service that implements Microsoft.Extensions.Logging.ILogger and uses Serilog. We use LoggingConfiguration class to configure logging. Properties Description LogLevel Sets log level verbosity. EnableFileLogging If true save logs to file. FileOptions Log file name and max size Registration Use WithLogging method from XrvService. var xrvService = Application.Current.Container.Resolve<XrvService>(); var config = new LoggingConfiguration() { LogLevel = LogLevel.Debug } xrvService.WithLogging(config); Uses Obtain the service and use it. Get Logging anywhere // Get logger var log = Application.Current.Container.Resolve<ILogger>(); Get Logging in component [BindService] private ILogger log = null; Log // log debug log.Log(LogLevel.Debug, \"debug msg\"); Warning // log warning log.LogWarning(\"warning msg\"); Error // log error log.LogError(\"error\");"
  },
  "manual/addons/xrv/messaging.html": {
    "href": "manual/addons/xrv/messaging.html",
    "title": "Messaging | Evergine Doc",
    "keywords": "Messaging XRV counts with a simple implementation of publisher-subscriber pattern, that will let you communicate two separated parts of code using messages. You can use it in components, services or any other elements if that suits to your needs. You can emit any type of message using this channel, but the recommendation is to create specific message types, that can contain any information that you consider useful. To emit a message, just create a custom message class and use Publish method. public class MyMessage { public MyMessage(string data1, int data2) { this.Data1 = data1; this.Data2 = data2; } public string Data1 { get; private set; } public int Data2 { get; private set; } } var pubSub = this.xrvService.Services.Messaging; var message = new MyMessage(\"my-data\", 1234); pubSub.Publish(message); To receive messages of a given type, you should use Subscription method. It returns a subscription token, that you must save to be able to unsubscribe, depending on your how your code behaves. For example, if you want to use it in a Component, the most common pattern will be subscription in OnAttach/OnActivated and desubscribe in OnDetach/OnDeactivated. public class MyComponent : Component { [BindService] private XrvService xrvService = null; private PubSub pubSub => this.xrvService.Services.Messaging; private Guid subscription; protected override bool OnAttached() { bool attached = base.OnAttached(); if (attached) { this.subscription = this.pubSub.Subscribe<MyMessage>(this.OnMyMessageReceived); } return attached; } protected override void OnDetach() { base.OnDetach(); this.pubSub.Unsubscribe(this.subscription); } private void OnModuleActivationChange(MyMessage message) { // Do something } }"
  },
  "manual/addons/xrv/modules/customModule/index.html": {
    "href": "manual/addons/xrv/modules/customModule/index.html",
    "title": "Create your own XRV modules | Evergine Doc",
    "keywords": "Create your own XRV modules You can create your own module, you just need to extend Module abstract class. Depending on elements you instantiate here, XRV will automatically add a new hand menu button, a section in Settings window or a section in Help window. Also, you can define your own voice commands. As we comment below, you'll get notified through Run method when module's associated hand menu button is tapped. There, you can show or hide 3D elements that you have previously added to the scene in Initialize method. Module implementation public class MyCustomModule : Module { public override string Name => MyModuleName; public override MenuButtonDescription HandMenuButton => MyHandMenuButton; public override TabItem Help => MyHelp; public override TabItem Settings => MySettings; public override IEnumerable<string> VoiceCommands => MyVoiceCommands; public override void Initialize(Scene scene) { // Do initialize stuff. } public override void Run(bool turnOn) { // Code. } } Methods Description Initialize(Scene scene) If we need some initialization for our code, we can set it here. For example, adding 3D entities or creating module specific windows. Run(bool turnOn) This method will be called when we press hand menu button, turnOn represents button toggle state, or always true if it's not a toggle button Properties Required Description Name Yes Module Name. HandMenuButton No If provided will add a button in hand menu. Help No If provided will add a tab in the help section. Settings No If provided will add a tab in the settings section. VoiceCommands No If provided will add a this strings to the voice command service. Installation var xrv = new XrvService() .AddModule(new MyCustomModule());"
  },
  "manual/addons/xrv/modules/imageGallery/index.html": {
    "href": "manual/addons/xrv/modules/imageGallery/index.html",
    "title": "Image Gallery module | Evergine Doc",
    "keywords": "Image Gallery module With this module, you can display a gallery of images, that can be stored remotely. A new element will be added in hand menu to provide direct access to gallery window. This window counts with some UI elements like a slider or buttons that lets the user load an image or other. . If data source contains only one image, no navigation buttons will be displayed at all. A single images data source is supported in current module version. Note In current version, there is a restriction: all images should be of the same size. Properties Description ImagePixelsWidth Image width for the viewer in pixels. ImagePixelsHeight Image height for the viewer in pixels. FileAccess Images data source. Please refer to Storage section for more information. Installation This module is packaged as Evergine add-on. To use it in your project, just install it from Project Settings > Add-Ons window. Then, just register the module programmatically within your XRV service instance. FileAccess imagesDataSource = <Create FileAccess instance>; var xrv = new XrvService() .AddModule(new ImageGalleryModule { ImagePixelsWidth = 640, ImagePixelsHeight = 640, FileAccess = imagesDataSource, }); Usage To open gallery window, just tap on hand menu button. You can navigate between images by clicking on next or previous buttons. You can also change current image using attached slider."
  },
  "manual/addons/xrv/modules/index.html": {
    "href": "manual/addons/xrv/modules/index.html",
    "title": "XRV Modules | Evergine Doc",
    "keywords": "XRV Modules This is the list of our current public modules. You can see them working running our public XRV sample. Image Gallery With this module you can display images in XR space, with no limitation in their number. Images will be displayed one by one. Model Viewer This module lets you load 3D models from different sources. Once loaded you can scale them and move them, you also have a menu with more features. Painter This is a module where you can draw lines in XR space to help you to emphasize something in particular. You can choose from different colors and thickness. Ruler With this module you can take measurements from any present object in XR space. Move handlers and obtain a precise measurement between two points. Streaming Viewer Module that loads a panel with video images served by an IP camera. Create your own Custom Module If you have some specific requirements for your application, you can create a custom module that you can reuse in more applications."
  },
  "manual/addons/xrv/modules/modelViewer/index.html": {
    "href": "manual/addons/xrv/modules/modelViewer/index.html",
    "title": "Model Viewer module | Evergine Doc",
    "keywords": "Model Viewer module One of the most common tasks while developing a XR experience is loading 3D models. With this module, you have a way to load models from a remote location into your application. When loaded, a model can be moved, rotated and scaled thanks to its bounding box. Transformations can be performed by near or far interaction, indistinctly. This module lets you to define an unlimited number of model repositories, each one can contain an unlimited number of 3D models. Properties Description NormalizedModelEnabled If true will override original scale of the model and set the same for all models. NormalizedModelSize Size in meters for models when loaded Repositories Array of model repositories. A repository counts with following properties. Properties Description Name This name will be displayed on model load list. FileAccess Models data source. Please refer to Storage section for more information. Installation This module is packaged as Evergine add-on. To use it in your project, just install it from Project Settings > Add-Ons window. Then, just register the module programmatically within your XRV service instance. FileAccess modelsDataSource = <Create FileAccess instance>; var xrv = new XrvService() .AddModule(new ModelViewerModule { Repositories = new Repository[] { new Repository() { Name = \"Remote Sample Models\", FileAccess = loadModelFileAccess, } }, NormalizedModelEnabled = true, NormalizedModelSize = 0.2f, }); Usage To open model selection window, just tap on hand menu button. Select a model from Models list. Each repository can have a different set of models. Once you know which model you want to load, just press Load button. . Manipulation By using manipulators can move, scale and rotate models. Manipulators are shown over bounding box. We have marked interaction areas with colors, depending on their manipulation possibilities. Red: Scale, pinch on corners and drag for scale model. Green: Roll, pinch on upper middle manipulator and drag for roll rotation. Blue: Pitch, pinch on side middle manipulator and drag for pitch rotation. Pink: stretch, pinch on center manipulator and drag for stretch scale. Actions Each model has a submenu with a set of options. Tap on button to expand list of available actions. : Manipulation is disabled for the model. This is, it could not be moved, rotated or scaled until it is unlocked again. : Reset model to original scale and orientation. Position won't be modified. : Removes model from virtual space."
  },
  "manual/addons/xrv/modules/painter/index.html": {
    "href": "manual/addons/xrv/modules/painter/index.html",
    "title": "Painter module | Evergine Doc",
    "keywords": "Painter module If you need to draw 3D lines in virtual space, this module is a good option. It lets you draw lines with different colors and thickness, so it could be useful to draw attention to certain elements like, for example, 3D models. A remarkable capability of this drawing tool is that you can use both hands at the same time. Also, you can undo any drawing/deletion action, or even remove all drawn lines at any moment. Installation This module is packaged as Evergine add-on. To use it in your project, just install it from Project Settings > Add-Ons window. Then, just register the module programmatically within your XRV service instance. var xrv = new XrvService() .AddModule(new PainterModule()); Usage To open painter window, tap on hand menu button. Note Drawing or removing lines is only available while painter window is open. Color selection wheel lets user to change current color. Active color is marked with a selection indicator. You can choose between a set of thickness that will be applied when a new line is drawn. Thin. Medium. Thick. There are buttons to undo previous actions or remove all lines. Full set of actions is listed below. : Pinch fingers and drag to draw line. : Pinch fingers and drag to remove line. : Do nothing. : Undo last action. : Clean all lines."
  },
  "manual/addons/xrv/modules/ruler/index.html": {
    "href": "manual/addons/xrv/modules/ruler/index.html",
    "title": "Ruler module | Evergine Doc",
    "keywords": "Ruler module This module lets users to measure elements in virtual space. It creates a basic ruler element with manipulators in both ends. When user grabs any of these manipulators, distance from one side to the other will be automatically calculated. Installation This module is packaged as Evergine add-on. To use it in your project, just install it from Project Settings > Add-Ons window. Then, just register the module programmatically within your XRV service instance. var xrv = new XrvService() .AddModule(new RulerModule()); Usage button is added to hand menu. Tapping on this button will toggle ruler visibility. Drag line ends will update distance measurement. Access to Settings to change current measure units. You can choose between meters and feet."
  },
  "manual/addons/xrv/modules/streamingviewer/index.html": {
    "href": "manual/addons/xrv/modules/streamingviewer/index.html",
    "title": "Streaming Viewer module | Evergine Doc",
    "keywords": "Streaming Viewer module This module lets you load a video stream from a MJPEG source. This is the unique streaming protocol that we support in current version. Video size can't be configured: this means that window size may vary depending on returned size of images provided by the server. Note It's required provided JPEG responses counts with Content-Length header to make it work properly. Installation This module is packaged as Evergine add-on. To use it in your project, just install it from Project Settings > Add-Ons window. Then, just register the module programmatically within your XRV service instance. var xrv = new XrvService() .AddModule(new StreamingViewerModule { SourceURL = \"http://<HOST>/video.mjpg\" }); Android-based systems In Android-based systems, like Meta Quest, there are some constraints about clear text traffic. By default, it is not allowed. If you set a source that is not served over HTTPS, you must white-list camera domain or IP address using appropriate native mechanism. Please see Android documentation for more information. Add a XML file under Android resources folder. We are giving it a name like network_security_config.xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <network-security-config> <domain-config cleartextTrafficPermitted=\"true\"> <!-- Sample IP cameras for Streaming Viewer module --> <domain includeSubdomains=\"true\">IP address or domain name</domain> </domain-config> </network-security-config> Register security configuration within your application definition under Android manifest file <application android:allowBackup=\"true\" android:icon=\"@mipmap/ic_launcher\" android:label=\"@string/app_name\" android:roundIcon=\"@mipmap/ic_launcher_round\" android:supportsRtl=\"true\" android:networkSecurityConfig=\"@xml/network_security_config\"> <!-- ... --> </application> Usage Use hand menu button to open streaming window."
  },
  "manual/addons/xrv/settings_system.html": {
    "href": "manual/addons/xrv/settings_system.html",
    "title": "Settings System | Evergine Doc",
    "keywords": "Settings System XRV provides a default window where you can include specific settings for your application or modules. It also includes a section for general settings provided by the core library, like turning on or off voice commands. To open Configuration window, just press button that you can find in hand menu. This window is a TabbedWindow and you have two ways of adding new elements. Adding a configuration section to your custom module public class MyModule : Module { public override TabItem Settings { get; protected set; } public override void Initialize(Scene scene) { this.Settings = new TabItem() { Name = () => \"Module Name\", Contents = this.CreateContents() // Entity with configuration item contents. }; } } Using SettingsSystem API var settings = this.xrvService.SettingsSystem; var item = new TabItem { Order = 1, Name = \"My item\", Contents = () => this.CreateContents(), }; settings.AddTabItem(item); You can also remove an existing item using RemoveTabItem method."
  },
  "manual/addons/xrv/storage.html": {
    "href": "manual/addons/xrv/storage.html",
    "title": "Storage | Evergine Doc",
    "keywords": "Storage If you need an external repository to load your models, images or other type of files, that are required by your application, you can take a look to what XRV offers about files storage. Base class for storage systems is FileAccess class, that can be extended to create specific implementations for a full CRUD access to files in a repository or folder. File access We provide a set implementations for FileAccess that we are going to enumerate. Following set of methods will be available when using any file access instance from code. Method Description ClearAsync Clears all files and directories. CreateBaseDirectoryIfNotExistsAsync Ensure that base directory defined by BaseDirectory exits. If it does not exists, directory is created. CreateDirectoryAsync Creates a directory, indicated by relative path. DeleteDirectoryAsync Deletes a directory, indicated by relative path. DeleteFileAsync Deletes a file, indicated by relative path. GetFileAsync Gets file contents by relative path. GetFileItemAsync Gets file metadata by relative path. EnumerateDirectoriesAsync Enumerates directories existing in base directory or in a relative directory path. EnumerateFilesAsync Enumerates files existing in base directory or in a relative directory path. ExistsDirectoryAsync Checks if a directory, indicated by relative path, exits or not. WriteFileAsync Writes file contents by relative path. Local application data folder storage This is implemented by ApplicationDataFileAccess, which sets as base path the value of System.Environment.SpecialFolder.LocalApplicationData, that can change depending on target platform. If, you create a new file access instance of this type, just set BaseDirectory value to a name for a base directory for that instance. You can use this file access as data cache or to create temporary files needed by your application, but may not be externally available depending on target platform. var fileAccess = new ApplicationDataFileAccess() { BaseDirectory = \"my-folder\", }; Note We use some folders internally, so be careful and do not use a \"cache\" as base directory name. Azure Blobs data storage You can also use Azure Blob Storage as data source to download files required by your application. Only thing you need is to add some storage account configuration settings when instancing a file access of this type. You may find different static methods to create instances, depending on set of configuration data you want to provide. Please, note that some directory methods may not work for this file access, as Azure Blob Storage does not have the concept of directory. When using SAS, remember that you should add appropiate permissions to the token, if you want full CRUD access to blobs. var fileAccess = AzureBlobFileAccess.CreateFromConnectionString(\"Storage account connection string\", \"Container name\"); //or var fileAccess = AzureBlobFileAccess.CreateFromUri(\"https://<ACCOUNT>.blob.core.windows.net/container?sv=2021-08-06&st=2022-11-18T15%3A07%3A20Z&...\"); // Container URI containing SAS (or without SAS for public containers, if you need read-only operations) var fileAccess = AzureBlobFileAccess.CreateFromConnectionString(\"https://<ACCOUNT>.blob.core.windows.net/container\", \"sv=2021-08-06&st=2022-11-18T15%3A07%3A20Z&...\"); // Container URI with separated SAS Azure Files data storage We have support for other service, Azure Files that you can use. In the same way, you need to specify connection configuration data to make. You may find different static methods to create instances, depending on set of configuration data you want to provide. var fileAccess = AzureFileShareFileAccess.CreateFromConnectionString(\"Storage account connection string\", \"Share name\"); //or var fileAccess = AzureFileShareFileAccess.CreateFromUri(\"https://<ACCOUNT>.file.core.windows.net/share?sv=2021-08-06&st=2022-11-18T15%3A07%3A20Z&...\"); // Share URI containing SAS var fileAccess = AzureFileShareFileAccess.CreateFromConnectionString(\"https://<ACCOUNT>.file.core.windows.net/share\", \"sv=2021-08-06&st=2022-11-18T15%3A07%3A20Z&...\"); // Share URI with separated SAS Disk cache Any FileAccess can use an optional cache, that will check for files locally instead of downloading them again. To activate caching for a file access, just create an instance of DiskCache and assign it. var fileAccess = AzureFileShareFileAccess.CreateFromUri(...); fileAccess.Cache = new DiskCache(\"images\"); // indicate an unique cache name for your needs For cache, you can specify following settings. Property Description SizeLimit Cache size limit. Defaults to 100 MB. SlidingExpiration Maximum time an item remains in cache without being accessed."
  },
  "manual/addons/xrv/themes.html": {
    "href": "manual/addons/xrv/themes.html",
    "title": "Themes System | Evergine Doc",
    "keywords": "Themes System If you need to customize colors used by your application, you should know that XRV uses a system for application theming. A palette of colors is defined by generic color names. Color Description PrimaryColor1 Used for windows title bar plate, text color for light buttons, lists selected item background color. PrimaryColor2 Color applied to scroll bar color. PrimaryColor3 This color is for global texts and for active items in a tab control. SecondaryColor1 Color for inactive items in tab controls, backplate color for some buttons, text color for selections, some manipulators, etc. SecondaryColor2 Color applied to some manipulators. SecondaryColor3 Applies to accept option backplate in confirmation dialogs. SecondaryColor4 Windows front plate gradient start and plate color for dialog buttons. SecondaryColor5 Windows front plate gradient end. Associated to these built-in colors, we provide a set of materials that you should use in your custom UI elements if you need to make use of themes and change default set of colors provided by XRV. Depending on values indicated in the theme, these materials are modified at runtime. As material instances are shared by different MaterialComponent instances, new color will be applied application wide. If you create your own materials, of course, they will be not modified by theme system. On the other hand, if you apply any of default materials in your own UI elements, they will be modified at runtime by themes system. Modifying a theme color To change a theme color, just access to ThemesSystem and take a look to its CurrentTheme property, that you can use to modify values for current theme or assign a new Theme instance. A new Theme instance has default XRV colors by default. You can modify color properties to apply modifications on UI elements. var theme = this.xrvService.ThemesSystem.CurrentTheme; theme.PrimaryColor1 = Color.DarkCyan; theme.SecondaryColor4 = Color.Blue; theme.SecondaryColor5 = Color.DarkBlue; For piece of code above, following changes would be presented when loading XRV windows and rest of visual elements. You can have as many theme instances as you want, an switching them in CurrentTheme property should change colors at runtime. Modify a theme font Themes also provide a set of fonts used in different parts of UI elements. Font Description PrimaryFont1 Used for windows title, buttons, section labels and tab items. PrimaryFont2 Font used for context texts, hand menu and window buttons. In the same way as colors, you can change these default fonts. Just add a new font asset and assign its identifier to any of the predefined font properties. theme.PrimaryFont1 = EvergineContent.MyCustomFont_ttf; theme.PrimaryFont2 = EvergineContent.MyCustomFont_ttf; Shared text styles XRV also tries to provide helpful ways of having uniform UI definitions. For this purpose, we have defined text styles that are shared globally, and that let you to define a font type, size and color, and reuse this definition and apply it to any number of 3D texts within application. In each TextStyle instance, you can set the following. Property Description Font You can optionally set a font asset identifier here as text font. This will be applied if ThemeFont is null. TextColor Color for the text. This will be applied if ThemeColor is null. TextScale Text scale for the style. ThemeColor This will indicate that this style will use one of theme defined colors. ThemeFont This will indicate that this style will use one of theme defined fonts. You will also find a set of built-in components to make your own 3D text to apply global styles. Text3dStyle: Associates a Text3DMesh to a text style. ButtonTextStyle: Associates a text style to a StandardButtonConfigurator instance. ToggleButtonTextStyle: Associates a text style to a ToggleButton. You must set a component for each toggle state. In the next table you can see the list of default text styles. As you may notice, they are all attached to theme fonts and colors. Note Please, note that in this case, changing text style values will only work on application startup. We don't have support right now to do this at runtime. Style key Default values Xrv.Primary1.Size1 font: ThemeFont.PrimaryFont1, scale = 0.012f, color = ThemeColor.PrimaryColor3 Xrv.Primary1.Size2 font: ThemeFont.PrimaryFont1, scale = 0.01f, color = ThemeColor.PrimaryColor3 Xrv.Primary1.Size3 font: ThemeFont.PrimaryFont1, scale = 0.008f, color = ThemeColor.PrimaryColor3 Xrv.Primary2.Size1 font: ThemeFont.PrimaryFont2, scale = 0.007f, color = ThemeColor.PrimaryColor3 Xrv.Primary2.Size2 font: ThemeFont.PrimaryFont2, scale = 0.006f, color = ThemeColor.PrimaryColor3 Xrv.Primary2.Size3 font: ThemeFont.PrimaryFont2, scale = 0.005f, color = ThemeColor.PrimaryColor3 Add or modify an existing text style If you need to add new styles, or modify an existing one, you can do it implementing ITextStyleRegistration. Its single method named Register passes as parameter the global styles dictionary: you can add new entries or modify existing ones. public MyTextStylesRegistration : ITextStyleRegistration { public void Register(Dictionary<string, TextStyle> registrations) { // override a default style if (registrations.ContainsKey(DefaultTextStyles.XrvPrimary1Size1)) { var defaultStyle = registrations[DefaultTextStyles.XrvPrimary1Size1]; defaultStyle.TextScale = 0.015f; } // add a new style with theme-independent color registrations[\"RedStyle\"] = new TextStyle { ThemeFont = ThemeFont.PrimaryFont1, TextScale = 0.013f, Color = Color.Red, }; } } As this registrations are scanned by XRV, you can add your own text styles, and they will be available for text styling built-in components. How-to respond to theme changes You may be wondering what happens with custom defined materials and theme changes. As we said above, Theme System will not update these materials. Don't worry, as you can create your own code to control changes for themes at runtime. ThemeSystem counts with an event named ThemeUpdated that will be invoked on theme changes. It has arguments of type ThemeUpdatedEventArgs that contains information about theme changes. Property Description IsNewThemeInstance It indicates if message has been sent by a complete theme change (a new instance of theme has been set). Theme Theme instance that is being applied at the moment of message emission. UpdatedColor If just one of the theme colors has been updated, it indicates which one of themed colors has been involved So, if you want that your component listens changes on current theme (for example, to change a custom material tint color), you will need something like the following. public MyComponent : Component { [BindService] private XrvService xrvService = null; private ThemesSystem themes => this.xrvService.ThemesSystem; protected override bool OnAttached() { bool attached = base.OnAttached(); if (attached) { this.themes.ThemeUpdated += this.ThemesSystem_ThemeUpdated; } return attached; } protected override bool OnDetached() { base.OnDetached(); this.themes.ThemeUpdated -= this.ThemesSystem_ThemeUpdated; } private void ThemesSystem_ThemeUpdated(object sender, ThemeUpdatedEventArgs args) { if (args.IsNewThemeInstance) { // Respond to theme instance changes return; } switch (args.UpdatedColor) { case ThemeColor.PrimaryColor1: // Respond to PrimaryColor1 changes break; // ... default: break; } } }"
  },
  "manual/addons/xrv/ui/index.html": {
    "href": "manual/addons/xrv/ui/index.html",
    "title": "UI | Evergine Doc",
    "keywords": "UI XRV provides some built-in components and artifacts that makes life easier to developer about creating uniform and consistent user interfaces. In this section Windows system Tabs control"
  },
  "manual/addons/xrv/ui/tabs_control.html": {
    "href": "manual/addons/xrv/ui/tabs_control.html",
    "title": "Tabs Control | Evergine Doc",
    "keywords": "Tabs Control Tabs control gives basic infrastructure and functionality to add tabbed panels to your application. It provides a container and a tab navigation system. It provides following properties to customize the control. Properties Description Size Set size for the container. SelectedItem Sets selected tab, and show associated content. Builder returns TabControlBuilder Create a tab control programmatically Easiest way to create a tab instance is to use TabControlBuilder provides features related to add content in the tab control. Methods Description AddItem Adds a single item. AddItems Adds a set of items. WithSize Specifies a size for tab control WithActiveItemTextColor Specifies active text color for tab item. WithInactiveItemTextColor Specifies inactive text color for tab item. var tabEntity = TabControl.Builder .Create() .WithSize(new Vector2(0.3f, 0.2f)) .AddItem(new TabItem { Name = () => \"Tab Name\", Contents = () => this.CreateContent(), // Function to set content entity }) .Build(); In XRV we make use of this tab control builder in configuration and help windows. If you want a window which only content entity is a tab control, you can make use of TabbedWindow class. Tab items definition TabItem provides tab and content for the tab control. Properties Description Name Tab Name. Data General purpose data. Order Order to be displayed. Contents Content for the tab item."
  },
  "manual/addons/xrv/ui/windows_system.html": {
    "href": "manual/addons/xrv/ui/windows_system.html",
    "title": "Windows System | Evergine Doc",
    "keywords": "Windows System One of the main features that XRV offers is the Windows System, that helps you with built-in windows which contents can be customized by your own. It also includes built-in alert and confirmation dialogs, that you can include within your UI logic to notify or ask for confirmation to application users, and execute some logic or another based on their decisions. Windows interaction When a window is created, it will include some default buttons that allows users to modify window behavior or visibility. Window position behavior can be changed, you can choose how window is placed relative to the user. To change position mode, just use window button to toggle between following behaviors: Follow mode: window will follow the user wherever he moves if he goes away by a distance higher than 0.6 meters. In this mode, window will also change its orientation to face the user if he moves around. Pinned mode: when switched to this mode, window will stay in the position and orientation that button has when pressed. In this mode, user can also manipulate position and orientation of the window, using pinch gesture on surface area of the window, including both title bar and contents area. Press to close the window. Create and show a window programmatically To create a window, you can do it accessing Windows System that is exposed by XrvService. This window creation have a callback to configure the window, with different options that we will enumerate in section below. You can create as many windows as you want. var xrv = Application.Current.Container.Resolve<XrvService>(); var windowSystem = xrv.WindowSystem; // Setting Window var window = xrv.WindowSystem.CreateWindow(config => { config.Title = \"Window #1\"; config.Size = new Vector2(0.3, 0.2); }); // Show window (empty in this case) window.Open(); Windows include different built-in parts and layers: Title bar: this is on the top of the window, also contains action buttons on right side of the window. Back plate: it has the same material than title bar, and the unique content that it includes is an optional logo image. Front plate: it will be drawn over back plate, and is intended to be placed behind window contents. Window instance options Each window instance that is returned by Windows System offers a set of options that developers can change by their own criteria. You can also open or close any window programmatically or subscribe to events about that window being opened or closed. Properties Description AllowPin If this option is disabled, default toggle button to let user change window positioning behavior that we commented above, will not be available for a window instance. DistanceKey This lets developers to specify the distance key that will be used to place window once opened. EnableManipulation When set to false, this option will not let users to change window orientation and positioning when pinned mode is active. Methods Description Open Opens window instance. Close Closes window instance. Events Description Opened It occurs when window instance is opened. Close It occurs when window instance is closed. Window instance configuration On configuration callback, you can use following properties to change how window is displayed. Properties Description Content This property lets you to set an entity that will be placed as window contents. Here you can set whatever you want: buttons, 3D text, images, etc. We will talk later about what is the easier way to layout your windows contents and how to set up window parameters like size to fits its contents. DisplayFrontPlate Front plate can be optionally hidden using this flag. DisplayBackPlate Back plate can be optionally hidden using this flag. DisplayLogo Controls logo visibility for the window. This image is located in bottom-left corner of the back plate. FrontPlateOffsets XY front plane offset relative to window back plate. FrontPlateSize Front plate width and height in meters. LocalizedTitle Lets you to specify callback function to set a localized title for the window. LogoMaterial To change default logo image, you can set your own material here. Size You can set window width and height in meters. Title Lets you to specify a fixed title string to your window. How-to: create own window with contents If you want to create your own window, you should first know the size of its contents, but calculating this manually could be tricky. We are going to quickly explain the steps we follow in this case. Create a new scene, that will contain your window contents. Window contents will be then exported to a prefab that will be loaded as window contents. Create a new mesh with BorderlessFrontPlate material. This plane will be a help guide just to know how contents will fit in final window. Set PlaneMesh width and height with desired size. Create your window layout. Create a prefab from your contents entity. Do not include guide reference in the prefab, just Painter entity in this case. Remember that you should not save your changes after creating the prefab. Instantiate your window from code, and set its size with same values as your reference. var contentsSize = new Vector2(0.214f, 0.173f); var window = windowsSystem.CreateWindow(config => { config.Size = contentsSize; config.FrontPlateSize = contentsSize; config.Content = this.assetsService.Load<Prefab>(<Prefab GUID here>).Instantiate(); }); Built-in dialogs XRV also offers two built-in dialog types that you can use to request a user action before making a decision. An important thing here is that, unlike windows creation, there could only be a single instance of alert or confirmation dialog at the same time in the application. We limit this to avoid dialog stacks that would make users uncomfortable while using the application. Alert dialog: this may be used to alert user about anything that happens while using the application, but where the user can choose no option, just confirm the dialog. Confirmation dialog: in this case, we can use this type of dialog to ask user for confirmation about an action, for example, removing a 3D model from virtual space. Here you have an example of how use Windows System to show an alert or confirmation dialog. var dialog = windowsSystem.ShowConfirmationDialog(...); // or ShowAlertDialog dialog.Closed += Dialog_Closed; private void Dialog_Closed(object sender, EventArgs e) { if (dialog is ConfirmDialog confirm) { dialog.Closed -= this.Dialog_Closed; if (confirm.Result == confirm.AcceptOption.Key) { // Do something only if user taps on accept option } } } In the sample, you can notice that we are subscribing to Closed event. This is safe as long as you do event unsubscription in callback method, as we only allow a single dialog opened at the time. This means, if we have other part of the code that has already opened a dialog an is waiting for the response, this new dialog opening will provoke Closed event to be invoked for that previously executed code. XRV dialogs have a property named Result that you can check to know which button has been pressed by the user. Alert dialog Result value It AlertDialog.AcceptKey When user presses dialog Accept button. null When user presses dialog Close button, or other part of the code invokes ShowAlertDialog. Confirmation dialog Result value It ConfirmationDialog.AcceptKey When user presses dialog Accept button. ConfirmationDialog.CancelKey When user presses dialog Cancel button. null When user presses dialog Close button, or other part of the code invokes ShowConfirmationDialog. Other things that you can do with Windows System Windows System instance also provide two properties that may be helpful for developers: Properties Description Distances You can register or modify predefined window distances. OverrideIconMaterial This property allows to you to set a custom Material that will override default window logo material. So, you don't need to go instance by instance replacing default logo material. Window distances definition XRV provides a set of predefined window distances, but you can add as many as you want to your application. Distance key Value (in meters) Usage NearKey 0.35 This is the distance used by default for alert and confirmation dialogs. MediumKey 0.5 This is the distance used by default for rest of the windows. FarKey 1 This is not used at all by built-in elements of XRV but you can use it by your own purpose. To override or add a new distance, you can use SetDistance method. // override an existing key windowsSystem.Distances.SetDistance(Xrv.Core.UI.Windows.Distances.NearKey, 0.45f); // adding a new key windowsSystem.Distances.SetDistance(\"custom\", 0.5f); // use distance key in a window window.DistanceKey = \"custom\"; Examples Get Windows System in component public MyComponent : Component { [BindService] private XrvService xrvService = null; private windowSystem => xrvService.WindowSystem; private void ShowAlert() { var dialog = this.windowSystem.ShowAlertDialog(\"Alert Title\", \"Sample Content.\", \"OK\"); dialog.Closed += this.OnAlertClosed(); } private void ShowConfirmation() { var dialog = this.windowSystem.ShowConfirmationDialog(\"Confirmation Title\", \"Sample Content.\", \"No\", \"Yes\"); dialog.Closed += this.OnConfirmationClosed(); } private void OnAlertClosed(object sender, EventArgs e) { if (sender is AlertDialog dialog) { dialog.Closed -= this.OnAlertClosed(); } } private void OnConfirmationClosed(object sender, EventArgs e) { if (sender is ConfirmationDialog dialog) { dialog.Closed -= this.OnConfirmationClosed(); } } }"
  },
  "manual/addons/xrv/voice_commands.html": {
    "href": "manual/addons/xrv/voice_commands.html",
    "title": "Voice Commands | Evergine Doc",
    "keywords": "Voice Commands Provides a service where we can register key words, that the speech recognition service will detect and we can take actions when this happens. It is based on MRTK, so you should work with SpeechHandler if you want to create handlers for your custom controls. If you are only interested in buttons and toggle buttons, you can make use of PressableButtonSpeechHandler defined in MRTK, or ToggleButtonSpeechHandler defined in XRV, to activate a button if associated voice command is recognized. Note Current implementation supports voice commands for HoloLens 2 (UWP) only. Speech recognition service must be enabled or command recognition will never be fired. Please, note that you should add an explicit reference to Evergine.Xrv.Core NuGet package for your UWP Mixed Reality project. This is necessary in order to take correct implementation of speech service for UWP platform. User can also activate or deactivate voice command recognition in Configuration -> General section. Associate voice commands programmatically You have two options to add custom voice commands: Specify voice commands in menu button description for module definition. public MyModule : Module { private const string VoiceCommandShow = \"Show feature\"; private const string VoiceCommandHide = \"Hide feature\"; public override IEnumerable<string> VoiceCommands => new[] { VoiceCommandShow, VoiceCommandHide }; public override void Initialize(Scene scene) { this.HandMenuButton = new MenuButtonDescription() { VoiceCommandOff = VoiceCommandShow, VoiceCommandOn = VoiceCommandHide, }; } } Using Voice System API to programmatically register voice commands. Please, note that this should only be invoked on application startup. var voiceSystem = this.xrvService.VoiceSystem; voiceSystem.RegisterCommands(new [] { \"one command\", \"other command\" }); Create custom speech handler public MySpeechRecognizer : SpeechHandler { protected override void InternalOnSpeechKeywordRecognized(string keyword) { base.InternalOnSpeechKeywordRecognized(keyword); // Do something depending on matching command } }"
  },
  "manual/animation/animation_blend_tree.html": {
    "href": "manual/animation/animation_blend_tree.html",
    "title": "Animation Blend Tree | Evergine Doc",
    "keywords": "Animation Blend Tree Coming soon"
  },
  "manual/animation/animation_clip.html": {
    "href": "manual/animation/animation_clip.html",
    "title": "Animation Clip | Evergine Doc",
    "keywords": "Animation Clip Coming soon"
  },
  "manual/animation/index.html": {
    "href": "manual/animation/index.html",
    "title": "Animation | Evergine Doc",
    "keywords": "Animation Coming soon In this section Animation Clip Animation Blend Tree"
  },
  "manual/audio/audio_editor.html": {
    "href": "manual/audio/audio_editor.html",
    "title": "Audio Editor | Evergine Doc",
    "keywords": "Audio Editor Audio Editor allows editing the audio assets. Double click over an audio asset shown in Assets Details will open this editor. The editor is composed of 2 main parts: Viewport Shows the wave form of the audio asset. In the top of the viewport the original audio file properties is shown while above the toolbox the audio asset properties is shown, these parameters depend of the propeties panel. The viewport has a toolbox on the bottom side that allows to test the audio. Icon Description Plays the audio asset. Enable/Disable playing in loop. Volume of the playing audio. Distribution of audio signal to listen more through right or left speaker. Decrease/Increase the audio speed. Properties The audio properties that you can configure are: Property Available values Description ChannelFormat Mono / Stereo The number of channels of audio wave data. SampleRateMode Low / High The sample rate of audio wave data. A high sample rate improves the quality but increases the file weight. Enconding PCM8 / PCM16 Pulse Code Modulation bits."
  },
  "manual/audio/import_audio.html": {
    "href": "manual/audio/import_audio.html",
    "title": "Import Audio | Evergine Doc",
    "keywords": "Import Audio The Audio asset allows you to play sound effects and music in your project. Import a Audio asset in Evergine Studio You can create an audio asset by dragging an audio file to the Assets Details panel, as explained in this article. Audio files in content directory Audio imported in Evergine create an additional metadata .wesn file. Supported formats: Evergine supports the following audio file formats: Extension Description .wav Waveform Audio File Format is an audio file format standard, developed by IBM and Microsoft, for storing an audio bitstream on PCs. It is the main format used on Microsoft Windows systems for uncompressed audio. The usual bitstream encoding is the linear pulse-code modulation (LPCM) format. .mp3 MPEG-2 Audio Layer III is a coding format for digital audio developed largely by the Fraunhofer Society in Germany, with support from other digital scientists in the United States and elsewhere. Originally defined as the third audio format of the MPEG-1 standard, it was retained and further extended — defining additional bit-rates and support for more audio channels — as the third audio format of the subsequent MPEG-2 standard. A third version, known as MPEG 2.5 — extended to better support lower bit rates — is commonly implemented, but is not a recognized standard."
  },
  "manual/audio/index.html": {
    "href": "manual/audio/index.html",
    "title": "Audio | Evergine Doc",
    "keywords": "Audio Evergine supports creating audio sounds and playing music in your projects including 3D spatialized audio very useful in Virtual Reality or Augmented Reality. In this section Import Audio Audio Editor Using Audio from Evergine Studio Using Audio from code"
  },
  "manual/audio/using_audio_from_code.html": {
    "href": "manual/audio/using_audio_from_code.html",
    "title": "Create audio from code | Evergine Doc",
    "keywords": "Create audio from code In this section are explained in more detail the all important elements to reproduce audio in your project. Audio Device Audio Device is a class that represents an audio output device. Evergine supports to audio device implementations: Audio Device Implementation Platforms supported Description XAudioDevice Windows, Hololens A lower-level audio API for Microsoft Windows, Xbox 360 and Windows Phone 8, the successor to DirectSound on Windows and a supplement to the original XAudio on the Xbox 360. ALAudioDevice Windows, Web, Android, IOS, Linux, Mac A cross-platform audio application programming interface (API). It is designed for efficient rendering of multichannel three-dimensional positional audio. Its API style and conventions deliberately resemble those of OpenGL. OpenAL is an environmental 3D audio library, which can add realism to a game by simulating attenuation (degradation of sound over distance), the Doppler effect (change in frequency as a result of motion), and material densities. OpenAL aimed to originally be an open standard and open-source replacement for proprietary (and generally incompatible with one another) 3D audio APIs such as DirectSound and Core Audio, though in practice has largely been implemented on various platforms as a wrapper around said proprietary APIs or as a proprietary and vendor-specific fork. In the Program.cs class in Evergine project template is possible to see the implementation used as Audio Device: Creating a XAudio2 implementation // Creates XAudio device var xaudio = new global::Evergine.XAudio2.XAudioDevice(); application.Container.RegisterInstance(xaudio); Creating a OpenAL implementation // Creates OpenAL device var audioDevice = new Evergine.OpenAL.ALAudioDevice(); application.Container.RegisterInstance(audioDevice); Its methods more relevant are: Method Description CreateAudioSource Creates an audio source where you can enqueue audio buffers. CreateAudioBuffer Creates an audio buffer. Audio Buffer The audio buffer represents a sound. When you drag an audio file to the Evergine Studio and audio asset is created automatically. You can load an audioBuffer from an audio asset using the AssetService: // Create audioBuffer from audio asset. AudioBuffer audioBuffer = assetsService.Load<AudioBuffer>(EvergineContent.Audio.sound1_wav); It is possible to create an audio buffer with procedural data too: // Create buffer this.format = new WaveFormat(false); this.buffer = this.AudioDevice.CreateAudioBuffer(); // Fill with data var bufferData = new byte[100 * this.format.BlockAlign]; this.buffer.Fill(bufferData, 0, bufferData.Length, this.format); For more details see the AudioBuffer API reference Audio Source The audio source represents the audio queue where it is possible to enqueue several audio buffers and reproduce them one after another. To enqueue an audio buffer and playing it: // Load audio buffer AudioBuffer audioBuffer = assetsService.Load<AudioBuffer>(EvergineContent.Audio.sound1_wav); // Gets Audio Device instance AudioDevice audioDevice = Application.Current.Container.Resolve<AudioDevice>(); // Create Audio Source AudioSource audioSource = audioDevice.CreateAudioSource(audioBuffer.Format); // Enqueue audio buffer if (audioSource.PendingBufferCount == 0) { audioSource.EnqueueBuffer(audioBuffer); } // Play the audio source. audioSource.Play(); Its methods more relevants are: Method Description EnqueueBuffer Enqueue an audio buffer. Play Play all buffers enqueued in the audio source. Pause Pause the audio source reproduction. Stop Stop the audio source reproduction. For more details see the AudioSource API reference"
  },
  "manual/audio/using_audio_from_editor.html": {
    "href": "manual/audio/using_audio_from_editor.html",
    "title": "Using Audio from Evergine Studio | Evergine Doc",
    "keywords": "Using Audio from Evergine Studio The 3D spatialized audio allows simulate sounds in a 3D environmnet. The goal is to reproduce audio in a way that replicates the way we hear sound in the real world. Spatial sounds are very useful to simulate environments in Virtual Reality / Augmented reality because this feature added more realism to the experience. In Evergine there are two components to simulate audio/spatial audio: Component Description SoundListener3D Represents a listener. Usually, this component is used with the camera entity. SoundEmitter3D Represents an emitter. This component can be added to any Entity of your scene to emit a 3D sound. Sound Listener To add a sound listener component to the main camera, select the camera and click the button on from Entity Details panel and search the component. Properties Description DopplerFactor Changes in frequency of a wave in relation to a listener who is moving relative to the sound emitter. The value is a positive float and the default value is 1.0. Sound Emitter To add a sound emitter component to any scene entity, select the entity and click the button on from Entity Details panel and search the component. Properties Description Audio The audio asset. Note. The audio must be mono to work as spatial sound. Volume The audio volume. The value is between [0.0-1.0] and the default value is 1.0. Pitch The quality makes it possible to judge sounds as higher and lower in the sense associated with musical melodies. The value is between [0.0-1.0] and the default value is 1.0. IsMuted Indicates whether the emitter is muted or not. Distance Scale Factor This is only used to calculate the doppler effect on the sound effect. Play Automatically The emitter starts playing the sound automatically. Loop The emitter is playing in loop mode. Apply3D Indicates whether the position of the emitter with respect to the listener will be used to simulate spatial sound or not."
  },
  "manual/basics/application/container.html": {
    "href": "manual/basics/application/container.html",
    "title": "Application Container | Evergine Doc",
    "keywords": "Application Container The Application Container is a class responsible to store objects that will be accesses from the entire application. The application itself does not have functionality itself, and all logic is made using registered instances into the Container. The application Container could be accessed using the Container property in the Application class. Here you will register all Services of your application, among other instances to control your project (like the GraphicContext to expose the graphic API, or AssetsService to control the asset library of your application, and ScreenContextManager to control the scenes that will be played in your application). Tip In general terms, in the Container you will register all Services and logic that will be consumed for every Scene or Component in your Application. Note There can be only one instance per Type in the Container. Every object registerd here is treated like a Singleton. Using the Container Register Instances You can register elements inside the Container by two ways: Register a Type You can register a type in the Container. If this functionality will be needed in the future, the Container will create an instance and will offer it to everyone that require this instance. This is dome by the Container.RegisterType<T>() method: Method Description RegisterType<T>() Register the specified type T to the container. A small example: public partial class MyApplication : Application { public MyApplication() { // Previous code :) // You could register the service by type... this.Container.RegisterType<MyService>(); } ... Register an Instance In the other hand, you can register directly an instance, this useful if you want to properly initiate the service, or offer an implementation of an abstract class: Methods Description RegisterInstance(T instance) Register the instance object into the Container. It will associate to the type of the instance. RegisterInstance<T> (T instance) Register the instance object into the Container. It will associate to the type of the generic type <T>. the instance parameter type must be a subclass of <T>. A small example: public partial class MyApplication : Application { public MyApplication() { // Previous code :) // Register directly the service instance... this.Container.RegisterInstance(new MyService()); // You can register a instance but specifying a parent class to indicates // the type that you want to expose. // In that case, you are offering the DX11 implementation when someone request the GraphicsContext... this.Container.RegisterInstance<GraphicsContext>(new DX11GraphicsContext()); } ... Get instances Is easy to obtains instances from the Application Container Using [BindService] tag attribute You can use the [BindService] attribute in your Component, SceneManager or even from another Services to automatically inject the Service instance into your property. using Evergine.Framework; using System; namespace MyProject { public class MyBehavior : Behavior { // Use the BindService attribute on top of the property or attribute // in which you want to inject the Service [BindService] private MyService myService = null; protected override void Update(TimeSpan gameTime) { this.myService.DoRequest(); } } } Using Resolve() methods The Container has the following methods to obtains instances: Methods Description T Resolve<T>() Obtains the instance of the type <T> specified. object Resolve(Type type) Other way to obtains an objects, but without C# generics. using Evergine.Framework; using System; namespace MyProject { public class MyBehavior : Behavior { private MyService myService = null; protected override bool OnAttached() { // Use the Resolve<Type> method from the Application Container.... this.myService = Application.Current.Container.Resolve<MyService>(); return base.OnAttached(); } protected override void Update(TimeSpan gameTime) { this.myService.DoRequest(); } protected override bool OnDettached() { // Release the reference when a component is being dettached... this.myService = null; } } }"
  },
  "manual/basics/application/index.html": {
    "href": "manual/basics/application/index.html",
    "title": "Application | Evergine Doc",
    "keywords": "Application In Evergine, the Application class is the entry point for your application. It contains all services and expose the application loop with DrawFrame() and UpdateFrame() cycles. Additionally, it offers the Container instance, that allows developer to register all Services and global functionality to be accessed and binded through the entire application (from Components, Scenes, Managers, etc...) There can be only one instance of Application class in your Evergine project, and we treat it like a Singleton instance Static property Description Application.Current Static property to access the current Application. It's the most common way to access to your application instance from every part of your Evergine classes. In this section Application Container Using Application ScreenContext Manager"
  },
  "manual/basics/application/screen_context_manager.html": {
    "href": "manual/basics/application/screen_context_manager.html",
    "title": "ScreenContext Manager | Evergine Doc",
    "keywords": "ScreenContext Manager The ScreenContext Manager allows you to load a ScreenContext with single or multiple scenes simultaneously and transit to other ScreenContext while the application is running. Then the Screen Context manager is used to control the flow of your application through multiple scenes. ScreenContext A ScreenContext could be simple if only contains a single scene or multiple if contains several scenes. Transit to other ScreenContext The ScreenContext Manager has the To, Push and Pop methods to transit between different ScreenContexts. Method Description To The Screen ContextManager stack unloads and clears all ScreenConnect enqueued before and loads only the new ScreenContext. By default, the method dispose all ScreenContexts previously in the stack, but this behavior can be modified using an overloading method with the paramaeter doDispose Push The ScreenContextManager stack pauses the last ScreenContext enqueued and enqueues the new ScreenContext. Only the peek ScreenContext is running. . Pop The ScreenContextManager stack unloads and dequeues the ScreenContext located in the peek and the previous ScreenContext in the stack will be running. By default, the method dispose of the dequeued ScreenContext, but this behavior can be modified using an overloading method with the parameter doDispose Using ScreenContent Manager The ScreenContext Manager is only available from code so is not possible to configure it from Evergine Studio. If you inspect the MyApplication class created by the Evergine project template, you can see the method Initialize with the following code: public override void Initialize() { base.Initialize(); // Get ScreenContextManager var screenContextManager = this.Container.Resolve<ScreenContextManager>(); var assetsService = this.Container.Resolve<AssetsService>(); // Navigate to scene var scene = assetsService.Load<MyScene>(EvergineContent.Scenes.MyScene_wescene); ScreenContext screenContext = new ScreenContext(scene); screenContextManager.To(screenContext); } In the code below a simple ScreenContext is created with the MyScene scene and the ScreenContextManager transit to this ScreenContext using the To method explain before. In addition, you can assign names to the ScreenContexts using: ScreenContext screenContext = new ScreenContext(\"Menu\", scene); To find the ScreenContext in the stack you can use the following line: var menuScreenContext = screenContextManager.FindContextByName(\"Menu\"); ScreenContext behaviors When a ScreenContext has been paused the update and draw method of the scenes inside of ScreenContext they will not run. But this behavior can be modified using the parameter Behavior in the ScreenContext class. ScreenContext screenContext = new ScreenContext(\"Menu\", scene) { Behavior = ScreenContextBehaviors.UpdateInBackground | ScreenContextBehaviors.DrawInBackground }; Behavior Description UpdateInBackground The scenes inside of ScreenContext keep updating when the ScreenContext is paused. DrawInBackground The scenes inside of ScreenContext keep drawing when the ScreenContext is paused."
  },
  "manual/basics/application/using_application.html": {
    "href": "manual/basics/application/using_application.html",
    "title": "Using Application | Evergine Doc",
    "keywords": "Using Application In this documents yo will learn the common usages and tips to deal with Evergine Application class. Default Application After creating a new Evergine project (checks the Project Structure document for further details), you will find a MyApplication class that inherit from Application: public partial class MyApplication : Application { public MyApplication() { // Register services into application container... this.Container.RegisterType<Clock>(); this.Container.RegisterType<TimerFactory>(); this.Container.RegisterType<Random>(); this.Container.RegisterType<ErrorHandler>(); this.Container.RegisterType<ScreenContextManager>(); this.Container.RegisterType<GraphicsPresenter>(); this.Container.RegisterType<AssetsDirectory>(); this.Container.RegisterType<AssetsService>(); this.Container.RegisterType<ForegroundTaskSchedulerService>(); this.Container.RegisterType<WorkActionScheduler>(); } public override void Initialize() { base.Initialize(); // Get ScreenContextManager... var screenContextManager = this.Container.Resolve<ScreenContextManager>(); var assetsService = this.Container.Resolve<AssetsService>(); // Load the scene asset and instantiate into a MyScene class instance... var scene = assetsService.Load<MyScene>(EvergineContent.Scenes.MyScene_wescene); // Use ScreenContextManager service to navigate the main scene (MyScene)... ScreenContext screenContext = new ScreenContext(scene); screenContextManager.To(screenContext); } } Interesting things: It uses the MyApplication constructor to register all available services. You can register services in any part of your code, but it is a good place to register services. The Initialize() method is invoked to initialize your application. In that part the ScreenContextManager is notified what Scene is going to be played. Note You can register Services on any place of your code. For example, if you want to register the GraphicsContext, we suggest to register in the Profile Project (I recommend again the Project Structure document). For example, you will register the DX11GraphicsContext in the Windows profile project, or VulkanGraphicsContetx in the Android profile. Application lifecycle methods The application class will invoke several methods to maintain its lifecycle, and helps developers to keep things going well. Methods Description Initialize() Initialize all aspects needed to start your application. Here you will usually navigate to the Scene to be played, and setup initial functionality or services to be consumed during the execution. UpdateFrame() This method execute the Update cycle of your application. It will notify all UpdatableServices to be updated, including the ScreenContextManager, that will propagate this invocation to the Scene and finally your Components. You can override this method to customize the Update loop. DrawFrame() This method is triggered to Draw your application. All Scenes will be rendered, and the GraphicsPresenter service will present the rendering results in all displays (windows, XR headsets, off screen rendering, etc...). You can override this method to customize the Draw loop. Destroy() Use this method to dispose all resources of your application. Checks if the application is running inside Evergine Studio When you are developing components or functionality, you maybe need to check if your code is running inside Evergine Studio or outside. Property Description IsEditor Application property that indicates if the application is running in Evergine Studio. public class MyBehavior : Behavior { [BindComponent] private Transform3D transform; protected override void Start() { // Reset the position to Zero only if is not Evergine Studio... if (!Application.Current.IsEditor) { this.transform.Positon = Vector3.Zero; } } }"
  },
  "manual/basics/bindings/bind_components.html": {
    "href": "manual/basics/bindings/bind_components.html",
    "title": "Bind Components | Evergine Doc",
    "keywords": "Bind Components Using [BindComponent] attribute allows your Components to stablish dependencies with another components. // Bind with the Transform component of the owner entity... [BindComponent] private Transform3D transform; // Bind a list with all Camera3D components of the entire scene... [BindComponent(source: BindComponentSource.Scene)] private List<Camera3D> sceneCameras; Note [BindComponent] can only be used inside Components. In other case the binding cannot be resolved [BindComponent] Properties This attribute offers several ways to customize: isRequired (default true) If the value is true the dependency is required to be resolved, in the oher case, the current Component won't be attached. For insatnce, in the following example we have a custom component MyComponent, and a definition of an Entity with two components (a Transform3D and MyComponent) public class MyComponent : Component { [BindComponent(isRequired: true)] private Transform3D transform; [BindComponent(isRequired: false)] private Camera3D camera; } Entity entity = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MyComponent()); The MyComponent will be attached correctly, because all requirements has been satisfied: The required Transform3D will be injected into the transform attribute, because we previously added a Transform3D component. The camera attribute won't be resolved, and this value will be equal to null, however, the component would be attached because this dependency is not required. In the other hand, in this other Entity, now the MyComponent instance won't be attached because the Transform3D dependency cannot be resolved: Entity anotherEntity = new Entity() .AddComponent(new MyComponent()); isExactType (default true) If the value is true indicates that the Type of the component to bound must be the same as the type required (nor subclass or parent class). For insatnce, the following component MyComponent, requires a component of the exact type Camera public class MyComponent : Component { [BindComponent] // isExactType: true by default private Camera camera; } Entity entity = new Entity() .AddComponent(new Transform3D()) .AddComponent(new Camera3D()) .AddComponent(new MyComponent()); In that case, the dependency won't be injected, because the entity has no Camera component (it has a Camera3D component, which is a subclass, but isExactType is true) However, if we change the MyComponent definition and sets isExactType value to false, now the dependency will be satisfied, because the entity has a component asignable to a Camera type (the Camera3D component): public class MyComponent : Component { [BindComponent(isExactType: false)] private Camera camera; } source (default BindComponentSource.Owner) This property indicates where the component or components will be searched. There are several values: Source Description Owner (default) The Component will be searched in the owner entity. Scene The Component is searched in the entire Scene. It iterates over all entities in the scene to find if exist components. Children Search the Components in its descendant entities, including the owner entity ChildrenSkipOwner Search the Components in its descendant entities, not including the owner entity Parents Search the Components in the ascendant entities, including the owner entity ParentsSkipOwner Search the Components in its ascendant entities, not including the owner entity A brief example: public class MyComponent : Component { // Bind a the first Camera3D component in the scene... [BindComponent(source: BindComponentSource.Scene)] private Camera3D firstCamera; // Bind a list with all Camera3D components in the entire scene... [BindComponent(source: BindComponentSource.Scene)] private List<Camera3D> sceneCameras; // ... } isRecursive (default true) If set to true the search will include all descendants (or ascendants) in the hierarchy; otherwise, the search will only include the direct descendants. tag (default null) If the tag has value, it will only find components in entities that has the specified Tag. It works for filtering entities. public class MyComponent : Component { // Bind a list with all Camera3D components of all Entities tagged with \"Tag\" [BindComponent(source: BindComponentSource.Scene, tag: \"Tag\")] private List<Camera3D> sceneCameras; // ... }"
  },
  "manual/basics/bindings/bind_entities.html": {
    "href": "manual/basics/bindings/bind_entities.html",
    "title": "Bind Entities | Evergine Doc",
    "keywords": "Bind Entities Using [BindEntity] attribute allows your components to stablish dependencies with Entities in your scene. The search is made using the Entity.Tag propert: // Binding with the first Entity in the Scene tagged with \"Player\" [BindEntity(tag: \"Player\")] private Entity player; // Search all Entities with \"Item\" tag... [BindComponent(tag: \"Item\")] private List<Entity> itemList; Note [BindEntity] can only be used inside Components. In other case the binding cannot be resolved [BindEntity] Properties This attribute offers several ways to customize: source (default BindEntitySource.Scene) This property indicates where the component or components will be searched. There are several values: Source Description Scene (default) The Entity is searched in the entire Scene. It iterates over all entities in the scene. Children Search the Entity in its descendant entities, excluding the owner entity ChildrenSkipOwner Search the Entity in its descendant entities, not including the owner entity Parents Search the Entity in the ascendant entities, including the owner entity ParentsSkipOwner Search the Entity in its ascendant entities, not including the owner entity A brief example: public class MyComponent : Component { // Binding with the first Entity in the Scene tagged with \"Player\" [BindEntity(tag: \"Player\")] // source: Scene is default value private Entity player; // ... } isRequired (default true) If the value is true the dependency is required to be resolved, in other case, the current Component won't be attached. The IsRequired value has the same functionality than [BindComponent] (see [Bind Components](bind_components.md for further details)). isRecursive (default true) If set to true the search will include all descendants (or ascendants) in the hierarchy; otherwise, the search will only include the direct descendants. tag (default null) The Entities will be filtered by its tag value."
  },
  "manual/basics/bindings/bind_scenemanagers.html": {
    "href": "manual/basics/bindings/bind_scenemanagers.html",
    "title": "Bind SceneManagers | Evergine Doc",
    "keywords": "Bind SceneManagers Using [BindSceneManager] attribute allows the developer to stablish dependencies with a SceneManager registered in the Scene. // Bind the RenderManager of the scene... [BindSeneManager] private RenderManager renderManager; Note [BindSceneManager] can be used inside Components and SceneManagers. In other case the binding cannot be resolved [BindSceneManager] Properties This attribute offers several ways to customize: isRequired (default true) If the value is true the dependency is required to be resolved, in other case, the current Component won't be attached. The isRequired value has the same functionality than [BindComponent] (see [Bind Components](bind_components.md for further details)). For example, if this is all Services registered inside the Application Container: this.managers.AddManager(new EntityManager()); this.managers.AddManager(new AssetSceneManager()); this.managers.AddManager(new BehaviorManager()); this.managers.AddManager(new RenderManager()); this.managers.AddManager(new EnvironmentManager()); The following component will be attached because the EnvironmentManager has been registered: public class MyComponent : Component { [BindSceneManager] private EnvironmentManager environmentManager; // ... } However, in that case, the dependency will fail because PhyisicManager3D is not registered in the scene: public class MyComponent : Component { [BindSceneManager] private PhyisicManager3D physicManager; // ... }"
  },
  "manual/basics/bindings/bind_services.html": {
    "href": "manual/basics/bindings/bind_services.html",
    "title": "Bind Services | Evergine Doc",
    "keywords": "Bind Services Using [BindService] attribute allows the developer to stablish dependencies with a Service registered in the Application Container. // Bind the Graphics Context registered in the Application... [BindService] private GraphicsContext transform; Note [BindService] can be used inside Services, Components and SceneManagers. In other case the binding cannot be resolved [BindService] Properties This attribute offers several ways to customize: isRequired (default true) If the value is true the dependency is required to be resolved, in other case, the current Component won't be attached. The isRequired value has the same functionality than [BindComponent] (see [Bind Components](bind_components.md for further details)). For example, if this is all Services registered inside the Application Container: // Register services into application container... this.Container.RegisterType<TimerFactory>(); this.Container.RegisterType<Random>(); this.Container.RegisterType<ErrorHandler>(); this.Container.RegisterType<ScreenContextManager>(); this.Container.RegisterType<GraphicsPresenter>(); this.Container.RegisterType<AssetsDirectory>(); this.Container.RegisterType<AssetsService>(); this.Container.RegisterType<ForegroundTaskSchedulerService>(); this.Container.RegisterType<WorkActionScheduler>(); The following component will be attached because the AssetsService has been registered: public class MyComponent : Component { [BindService] private AssetsService assetService; // ... } However, in that case, the dependency will fail because the Clock service is not registered: public class MyComponent : Component { [BindService] private Clock clock; // ... }"
  },
  "manual/basics/bindings/index.html": {
    "href": "manual/basics/bindings/index.html",
    "title": "Binding Elements | Evergine Doc",
    "keywords": "Binding Elements Evergine allows to bind dependencies automatically across your components, services and sceneManagers. For example, bindings allow a custom component to require that its Owner Entity has a Transform3D component, and set it to a property during the Attach phase (see the Lifecycle documents) This binding feature gives to developers a lot of flexibility to interconnect components, services and scene managers, avoiding the tedious part to setup this dependencies in Evergine Studio or from code. In Evergine, we define bindings using property attributes ([BindComponent] for example). Note All dependencies are not available until the Attach phase, if you try to access during in the constructor, OnLoaded() or OnDestroy() methods, the property value is goint to be null In this section There are four types of binding: Bind Components Bind Services Bind Entities Bind SceneManagers Example In the following example, the component will bind a service, a component an a scene manager. public class MyComponent : Component { [BindService] private AssetsService assetsService; [BindSceneManager] private RenderManager renderManager; [BindComponent] private Transform3D transform; private Material defaultMaterial; protected override Start() { // All values of bind attributes will be injected after the Attach phase... // Bind assetService this.defaultMaterial = this.assetsService.Load<Material>(EvergineContent.Materials.DefaultMaterial); // Bind SceneManagers this.RenderManager.DebugLines = true; // Bind Components this.transform.Position = Vector3.Zero; } // ... } Bind Collections If the [Bind] attribute is set on top of List<T> property, they will attempt to search a list of elements that elements that satisfy the dependency: // Bind with the Transform component of the owner entity... [BindComponent] private Transform3D transform; // Bind a list with all Camera3D components of the entire scene... [BindComponent(source: BindComponentSource.Scene)] private List<Camera3D> sceneCameras;"
  },
  "manual/basics/component_arch/components/behaviours.html": {
    "href": "manual/basics/component_arch/components/behaviours.html",
    "title": "Behaviours | Evergine Doc",
    "keywords": "Behaviours The Behaviors are one kind of Component that allows you to do and action each Update cycle of the Application. A Behavior is associated with an Entity and all Behaviors in a scene are managed by Behavior Manager. Create a Behavior From Visual Studio you can create a c# class with the following template. using Evergine.Framework; using System; namespace MyProject { public class MyBehavior : Behavior { protected override void Update(TimeSpan gameTime) { // Your code } } } Update Order You can indicate the order of execution of every Behavior by setting its UpdateOrder property. Property Description UpdateOrder Value used to order the execution of every Behavior of the Scene, whether lower values indicate that the Behavior would be updated first. The default value is 0.5 Behavior family There are three Behavior families that you can specify in the constructor of your Behavior using the base constructor with the FamilyType parameter. DefaultBehavior: This is the default family when you don't specify anything in the constructor. The Behavior only runs in runtime but not in the Evergine Studio. PriorityBehavior: This is a special family that indicates your Behavior runs both in runtime and in Evergine Studio. PhysicBehavior: This family is specific to Physics components that need to be updated by the PhysicsManager. BehaviorManager The Behavior Manager is a SceneManager registered by default in every SCene that manages the execution of all Behaviors in each update cycle. All Behaviors are registered automatically into the BehaviorManager when it's attached, and unregistered when the Behaviour is detached. Behavior example The following example creates Behavior that allows you to rotate your entity every update cycle. using Evergine.Framework; using Evergine.Framework.Graphics; using Evergine.Mathematics; using System; namespace MyProject { public class MyBehavior : Behavior { [BindComponent] private Transform3D transform = null; public MyBehavior() : base(FamilyType.DefaultBehavior) // This base(...) constructor could be ommited. { } protected override void Update(TimeSpan gameTime) { this.transform.LocalOrientation *= Quaternion.CreateFromEuler(new Vector3(0, (float)gameTime.TotalSeconds, 0)); } } } Tip BindComponent allows binding other components, to know more about that visit the following section Add/Remove a Behavior To add/remove a Behavior to/from your entity both from code or Evergine Studio is the same that adding/removing a component because a Behavior is a kind of component. You can see how to add/remove a component here"
  },
  "manual/basics/component_arch/components/drawables.html": {
    "href": "manual/basics/component_arch/components/drawables.html",
    "title": "Drawables | Evergine Doc",
    "keywords": "Drawables The Drawables are one kind of Component that allows you to do and action each Draw/Render cycle of the Application. A Drawable is associated with an Entity and all Drawables in a scene are managed by RenderManager. Drawable3D Drawables3D are a kind of drawables that is designed to provides 3D content. They will be processed on every Camera3D render. In these components you usually create graphics elements to draw 3D features (models, billboard, background environment, etc...) Add the following property: Property Description CastShadows Boolean value indicating whether this model will cast shadows. True by default. Creating a Drawable3D From Visual Studio you can create a c# class with the following code: using Evergine.Common.Graphics; using Evergine.Framework; using Evergine.Framework.Graphics; using Evergine.Mathematics; using System; namespace MyProject { public class BBoxDrawable : Drawable3D { [BindComponent] private Transform3D transform; public Vector3 Size {get;set;} = Vector3.One; public Color Color {get; set;} = Color.Red; public override void Draw(DrawContext drawContext) { var orientedBBox = new BoundingOrientedBox( this.transform.Position, this.Size, this.transform.Orientation); // Draw an oriented boundingbox with the specified color and size... this.Managers.RenderManager.LineBatch3D.DrawBoundingOrientedBox(orientedBBox, this.Color); } } } Drawable2D This drawables will be processed on every Camera2D render. In the other hand with Drawable2D you will render 2D content (Sprites, UI...). Add the following properties: Property Description Layer Suggest the RenderLayer that will use all 2D content used by this Drawable. Creating a Drawable2D From Visual Studio you can create a c# class with the following code: using Evergine.Common.Graphics; using Evergine.Framework; using Evergine.Framework.Graphics; using Evergine.Mathematics; using System; namespace MyProject { public class CircleDrawable : Drawable2D { [BindComponent] private Transform2D transform; public float Radius {get;set;} = 0.5f; public Color Color {get; set;} = Color.Red; public override void Draw(DrawContext drawContext) { // Draw a circle... this.Managers.RenderManager.LineBatch2D.DrawCircle( this.transform.Position, this.Radius, this.Color, this.transform.DrawOrder); } } } Graphics content A Drawable will add to RenderManager objects to be drawn (sprites, meshes, etc...). Read Render Overview document for further details. Add/Remove a Drawable To add/remove a Drawable to/from your entity both from code or Evergine Studio is the same that adding/removing a component because a Drawable is a kind of component. You can see how to add/remove a component here"
  },
  "manual/basics/component_arch/components/index.html": {
    "href": "manual/basics/component_arch/components/index.html",
    "title": "Components | Evergine Doc",
    "keywords": "Components A Component allows to add functionality and logic to an Entity. The Component class is the one of every component in Evergine. There are three types of Components which are broadly used along the engine: Component: You could derive directly from the Component class to add functionality whithout an Update() or Draw() calls. You could register to events or expose some logic, the only limit is your imagination :) Behavior: It is thought to add logic to the associated Entity. It provices an Update() method which is executed on each game loop. Drawable: In charge of managing the rendering on the screen. In the same way as above one, this provides a Draw() abstract method invoked during the rendering cicle. It is suitable to register and update objects to be rendered. Component Lifecycle Please, check Lifecycle of element for details regarding the lifecycle of elements in Evergine, including Components. Using Components You can deal with Components both in Evergine Studio and directly from code. From Evergine Studio In Evergine Studio, you can add/remove Components to an Entity, and modify its properties. Add a Component In Evergine Studio, first select an Entity that you want to add the Component, and click the button in the Entity Details section: A Component selector dialog appears. Select the component type that you want to add: Remove a Component The process of removing a component is quite simple. First, select the Entity that you want to remove one Component. Later, on the Entity Details, right click in the Component name area, and click the Delete button: From Code Add Components To add Components, you just need to invoke the Entity.AddComponent() method: Entity entity = new Entity(\"MyAwesomeEntity\"); // Add a Component (Transform3D)... entity.AddComponent(new Transform3D()); // You can chain AddComponent() calls... entity.AddComponent(new CubeMesh()) .AddComponent(new MaterialComponent()) .AddComponent(new MeshRenderer()); Remove Components You have several options to specify the component or components that you want to remove. You can remove components by indicating the Component instance itself, or by giving the type of the component to remove. In every method to remove the component specifying the type, you have the optional parameter isExactType, which indicate if the component to search and remove must match the given type, or for contrary, it can be a subclass of the type. // Remove a component passing the instance: entity.RemoveComponent(component); // Remove a component with the type (MeshRenderer in that case): entity.RemoveComponent<MeshRenderer>(); // An alternative way to remove a Component using the type: entity.RemoveComponent(typeof(MeshRenderer)); // You can remove all components that match the specified type (all Drawables in that example) // keep in mind that isExactType is false... entity.RemoveAllComponentsOfType<Drawable>(isExactType: false) Create a new Component Evergine provides a good Component library, but when you are developing a custom application, you will need to create your own Components to accomplish your application requirements. Write the C# code of your Component You only need to create a class that inherit from Component class, and add to your application project: public class MyComponent : Component { // Add some properties to expose data :) public int Value { get; set; } // Override the Start method, which is called once the Entity is started: protected override void Start() { base.Start(); Trace.TraceInformation($\"The component has been started: {this.Value}\"); } } Allow multiple instances By default, an Entity can only have one Component per Type (for instance, an Entity can't have more than one Transform3D). However, in some cases it is useful that an Entity can have more instances per type (for example, if you want to add several colliders). In that case, you need to add the [AllowMultipleInstances] attribute to your class: [AllowMultipleInstances] public MyComponent : Component { // The entity can have multiple instances of this component type... } // This is valid becuse MyComponent has the [AllowMultipleInstances] attribute... Entity entity = new Entity() .AddComponent(new MyComponent())) .AddComponent(new MyComponent())) .AddComponent(new MyComponent())) .AddComponent(new MyComponent())) .AddComponent(new MyComponent()));"
  },
  "manual/basics/component_arch/entities/entity_hierarchy.html": {
    "href": "manual/basics/component_arch/entities/entity_hierarchy.html",
    "title": "Entity Hierarchy | Evergine Doc",
    "keywords": "Entity Hierarchy You can establish an Entity as a child of another Entity. Using this relationships, it is possible to define an entity tree, where root entities has several children, and those children could have another children too. Hierarchy transformations If an Entity has a Transform3D component (or Transform2D for 2D entities), the Child Entity will move, rotate and scale in the same way as its Parent does. You can think of parent / child hierarchy as being like the relationship between your arms and your body, whenever your body moves, your arms also move along with it. Child objects can also have children. Your hands can be considered as children of your arms, and your fingers are hands children. Hierarchy Properties and Methods The Entity has the following properties and methods to maintain and inspect the hierarchy: Property Type Description Parent Entity Point to the inmediate ascendant of this entity. If the entity has no parents this property is null ChildEntities IEnumerable<Entity> A collection of the inmediate descendants. To add and remove entities, an Entity instance have the following methods: Method Description Entity.AddChild(Entity) Add a child entity to the current entity. Entity.RemoveChild(...) Remove a child entity from the current entity. You can specify the entity to remove by giving the following options: The Entity instance. The child Name The child ID Entity Paths Like in a normal file system, Evergine implements a simple Entity Path system to allow identification of entities in the Scene entity tree. An Entity Path cound be accessed by the EntityPath property, and is represented by the sequence of ascendant entity names (including the current entity) separated by the . symbol. For example, in the hierarchy described above, the EntityPath of the Tire2 entity is Car.Wheel2.Tire2 Path representation These are the path representation elements: Entity separator: . Current entity: [this] Parent entity: [parent] Sample uses Using the example described above... The relative path from Wheel1 to Tire2 would be: [parent].Wheel2.Tire2 The relative path from Car to Tire1 would be: .Wheel1.Tire1 Or: [this].Wheel1.Tire1, Notice that [this] is optional. The absolute path of Wheel1 entity would be exactly the one we've been using until now: Car.Wheel1 Whe you want to get an entity that doesn't belong to the source's root entity, we just specify the target's absolute path. For example, the relative path from Tire1 to Ground is just: Ground, instead of [parent].[parent].[parent].Ground (Incorrect path) To figure out if a specific path is absolute or relative, we just have to read the first element. If it's one of the special elements (., [this] pr [parent]): Path Type [parent].Wheel1 Relative Relative .Tire2 Relative [this] Relative Car.Wheel2 Absolute Road Absolute Additional methods In order to make easier the search of entities in code, we’ve added the Find() method in the Entity class. public Entity Find (string path) This method finds an entity with the desired relative path respect the caller entity. If the relative path is not correct, it returns null. We have also added an additional parameter in the Find method in EntityManager class, allowing to set the source entity and allowing to directly search there. public Entity Find (string path, Entity sourceEntity = null) For example (following with the above-mentioned hierarchy), if we want to search the tire1 entity from car we can call one of the next: Entity tire; // We can find the entity either of these ways tire = car.Find(\".Wheel1.Tire1\"); tire = entityManager.Find(\".Wheel.Tire1\", car);"
  },
  "manual/basics/component_arch/entities/entity_manager.html": {
    "href": "manual/basics/component_arch/entities/entity_manager.html",
    "title": "EntityManager | Evergine Doc",
    "keywords": "EntityManager EntityManager is the subsystem in the Scene responsible to manage Entities. It is responsible to maintain the Entity list, and the lifecycle of each one. Useful ways to interact with EntityManager: Add and Remove Entities The most common way to interact with EntityManager is by adding and removing entities: EntityManager entityManager = scene.Managers.EntityManager; // Add an Entity to the Scene.. entityManager.Add(entity); // Remove an Entity from the Scene... entityManager.Remove(entity); Important Properties Method Description EntityGraph Return the entities at the top level on the scene (Those without parent) AllEntities Return all the entities of the scene, including those who have parent. Find Entities You can find Entities in several ways: Find Entity by EntityPath This is the most common way to find an Entity in the Scene, by using its EntityPath: // Find an entity by its Entity Path... var entity = entityManager.Find(\"Parent.Entity1.ChildEntity\"); Find Entity by ID You can find an Entity if you specify its ID (Guid) // Find an entity by its ID... Guid id = entity.Id; var entity = entityManager.Find(id); Find all Entities by Tag As we mentined before, you can set the tag Entities by setting the Tag property. So, you can obtain an Entity collection that match a given Tag: // Find all entities with a given Tag... IEnumerable<Entity> entityCollection = entityManager.FindAllByTag(\"Tag\"); Entity Tag Collection A useful feature of EntityManager is the possibility to obtain a live collection of entities that match a specified Tag. This collection is represented by the EntityTagCollection class. The main advantage compared to invoke FindAllByTag() method is that in this case the collection is dynamic during the application lifetime. So, you can subscribe to events to get notified when the collection changes: Here it is a glimpse of how to use it: // Gets the tag collection... var tagCollection = entityManager.GetEntityTagCollection(\"Tag\"); // Iterates over all matching entities... foreach (var entity in tagCollection.Entities) { System.Console.WriteLine(entity.EntityPath); } // TagCollection Events: // OnEntityAdded: Event triggered when a new Entity with the given Tag is added to the Scene... tagCollection.OnEntityAdded += (sender, entity) => { System.Console.WriteLine($\"New entity: {entity.EntityPath}\"); }; // OnEntityRemoved: Event triggered when an Entity is no longer belong to this collection, // becuase the Entity is removed or its Tag has changed... tagCollection.OnEntityRemoved += (sender, entity) => { System.Console.WriteLine($\"Entity removed: {entity.EntityPath}\"); };"
  },
  "manual/basics/component_arch/entities/index.html": {
    "href": "manual/basics/component_arch/entities/index.html",
    "title": "Entities | Evergine Doc",
    "keywords": "Entities Entities are the fundamental objects in Evergine that represent characters, lights, models and so on. An Entity without any Component has no functionality (nothing will be rendered or interacted). To give an Entity the properties it needs to become a Light, a Model, or a Camera, you need to add Components to it. Depending on what kind of objects do you want to create. Basic Properties Despite entities has no functionality itself, it provides several ways to identify Entity instances and access to information: Property Type Description ID Guid The unique identifier of an Entity. Among other objects in Evergine, entities are identified by its ID property. There can only be one Entity with the same ID. This value is usually autogenerated when a new Entity is created, or when it is deserialized when a Scene is loaded from asset. Name String The name of the Entity. Several objects can share the same name. Tag String A string used to categorize entities. It is useful to tag entities with things in common (for instance, tag all vehicle entities with the tag \"Vehicle\"). Evergine provides ways to get entity collections sharing the same Tag. Components IEnumerable<Component> A collection of registered components in this Entity Scene Scene Point to the Scene in which this Entity has been registered. This value is set after attaching the Entity (see LifeCycle document for further details) In this section Entity Manager Entity Hierarchy Using entities"
  },
  "manual/basics/component_arch/entities/using_entities.html": {
    "href": "manual/basics/component_arch/entities/using_entities.html",
    "title": "Using Entities | Evergine Doc",
    "keywords": "Using Entities Manipulate Entities is quite straightforward in Evergine. From Evergine Studio Add Entity in Evergine Studio In the Entities Hiearchy area in an oppened SceneEditor, just click the button. A popup menu will appear: This menu gives you several options to create entities. If you just want to create an Entity with only the Transform3D component, click the Empty entity button. If an Entity is selected, you can change its properties (Name, Tag, enable) in the Entity Details panel: From Code Create a new Entity from code Creating and Entity from code is a not a complex task: Entity entity = new Entity(\"EntityName\"); // Set the Entity Tag... entity.Tag = \"Tag\" // Add some components... entity.AddComponent(new Transform3D()); .AddComponent(new TeapotMesh()) .AddComponent(new MaterialComponent()) .AddComponent(new MeshRenderer()) // Add to the EntityManager this.Managers.EntityManager.Add(entity); Create a simple Hierarchy from code // Create the parent entity... Entity parentEntity = new Entity() .AddComponent(new Transform3D()); // Create the child entity... Entity childEntity = new Entity() .AddComponent(new Transform3D()); // Add the child entity to the parent... parentEntity.AddChild(childEntity); // Add only the parent entity to the EntityManager this.Managers.EntityManager.Add(parentEntity);"
  },
  "manual/basics/component_arch/index.html": {
    "href": "manual/basics/component_arch/index.html",
    "title": "Component Based Architecture | Evergine Doc",
    "keywords": "Component Based Architecture Evergine is based on entity-oriented programming. An Entity is built of Components, where each component does just an specific piece of work, and are reusable on other entities. Overview When you are making applications in Evergine, you need to create objects to represent the entities in your scene, like a car, a player, a sunlight, and so on. Traditional deep class hierarchies When you first get started ,you might think the most logical thing is to create a base class called AppObject or similar, that contains the common code. Then you can create subclasses like Vehicle with all common functionality between all vehicles, and maybe subclases for specific types (Car, Motorbike, Train...). For simple scenarios, this works quite well and is quite easy to program. However, has your application get larger and more complex, this architecture begins to cause some problems in practice: Root classes tend to be heavier and is difficult to split the functionality in separate subsystems. Class inheritance may introduce weird scenarios. For instance, if we have created a GroundVehicle and WaterVehicle base classes, Which class a AmphibianVehicle should be derived from? If it is derived from GroundVehicle, all water functionality would be missing, and the same if it is derived from WaterVehicle. Entities as an aggregation of Components To solve this issues, we have chosen the aggregation of components approach. In ths approach, the functionality is separated into individual components that are mostly independent of one another. The old object hierarchy is replaced by an object (Entity) with a collection of independent components (derived from Component). Each object now only has the functionality that it needs. Any distinct new functionality is implemented by adding a new component. Entity-Component relationship Entities are the fundamental objects in Evergine that represent characters, lights, models and so on. An Entity without any Component has no functionality (nothing will be rendered or interacted). To give an Entity the properties it needs to become a Light, a Model, or a Camera, you need to add Components to it. Depending on what kind of objects do you want to create. SceneManagers and Components In Evergine, a Scene has several subsystems (called SceneManagers). Every SceneManager control different aspects of the scene (A few examples: RenderManager to render and draw the scene, BehaviorManager to udpate all Behaviors, PhysicManager3D to perform Physics simulations, and so on) With the Component architecture, every component is responsible to registers itself into the associated SceneManager, allowing in these SceneManagers to have the scope of Components which they are interested in and ignore the rest of them. Note For instance: Every physics related components (RigidBody3D, BoxCollider3D, etc...) are internally registered into the PhysicManager3D when an Entity is spawned into the scene. This allows PhysicManager3D to gather and control all the physic information in the scene. Summary Diagram A Scene is composed for several Entities. Each one has a collection of Components that give it the required functionality or data. Every component may be registered into the different SceneManagers of the Scene. The following diagram gives an overview concerning this In this section Entities Components Prefabs"
  },
  "manual/basics/component_arch/prefabs/index.html": {
    "href": "manual/basics/component_arch/prefabs/index.html",
    "title": "Prefabs | Evergine Doc",
    "keywords": "Prefabs Sometimes you need to reuse entities including their components and their descendants. Instead of use copy and paste or entity duplication for doing this, that will create maintenance problems, you can use the concept of prefab. A prefab is a type of asset which helps developers with this basic function: reusing. Creating a prefab To create a prefab, first you need to build your entities hierarchy and assign components. For example, we can create an entity with a teapot and a plane, and a Spinner component to the top of this hierarchy. To create a prefab, right click on the top-most entity that you wan to include in it, and select Create prefab option. This will create a new asset with weprefab extension the same folder that you have selected in Project Explorer panel. If you created prefab in the wrong folder, don't worry as they are like any other Evergine asset, you can move it wherever you want. If you save changes and reload the scene, you will realize that entities that were part of created prefab, are marked now with a \"(Prefab)\" suffix. Using prefab creation option is not reversible once you save the scene. You can also drag and drop asset element from Asset Details panel to your scene, to create new prefab instances. Prefabs edition If you change values on those entities, this will no affect to new and existing prefab instances. In future releases, we plan to provide an specific prefabs editor. For now, a good option is to create your prefab in a separated scene. At the time of using Create prefab option, a new prefab file will be created, but do not save the scene. Doing this, you could modify your entities in the future. Just replace previous prefab file version with the new one. We don't officially support prefab nesting. This is, create a separated scene for your prefab, and use another prefab in that scene. Prefab instantiation To create a prefab instance programmatically, you can use following code snippet: var prefab = this.assetsService.Load<Prefab>(EvergineContent.Scenes.Entity_weprefab); var entity = prefab.Instantiate();"
  },
  "manual/basics/index.html": {
    "href": "manual/basics/index.html",
    "title": "Basics | Evergine Doc",
    "keywords": "Basics This chapter explains the basic concepts you need to know before starting to develop Evergine applications. Evergine and .NET Evergine is a powerful component based engine developed on .NET and designed to be completely multiplatform. Because of this, when you develop an Evergine application, all code are written in C#. You can edit your source code in Visual Studio or your preferred IDE that supports .NET. At last, an Evergine application is a .NET application, so you can integrate every library or service available in this ecosystem. NuGet Packages Evergine libraries are distributed using NuGet package system. As a result, is straightforward to update your application and integrate with tons of libraries available in the large .Net community ❤. Caution To update the version of your Evergine application, use Evergine Launcher evergine_launcher/index.md) instead of NuGet Package Manager in Visual Studio. In this section Project structure Component Based Architecture Transform Bindings Lifecycle of elements Scenes Services Application"
  },
  "manual/basics/lifecycle_elements.html": {
    "href": "manual/basics/lifecycle_elements.html",
    "title": "Lifecycle of elements | Evergine Doc",
    "keywords": "Lifecycle of elements Running Evergine components, entities or other objects executes a number of functions in a predetermined order. This document describes those functions and explains how they fit into the execution sequence. The following elements has the same lifecycle methods, and all information described in this document can be applied on each one: Components Entities Services SceneManagers Lifecycle overview The diagram below summarizes how Evergine orders and repeats function invocations over the element's lifetime: Lifecycle properties All elements that implements the default lifecycle shares the same properties. These properties and methods are exposed in the AttachableObject class. Because of this, the Component, Entity, Service or SceneServices classes extend AttachableObject class. Property Description IsEnabled (getter and setter) Allows to enable or disable an element. A disabled Behavior is not updated, or a disabled Drawable doesn't draw anything. State (getter) Gets the current state of this object. We cover this area later in this document To easy checking states, Evergine offers several properties (getter only) to know in which state are the element: Property Description IsLoaded Indicates if this object has been loaded (the OnLoaded() method has been invoked). IsAttached Indicates if this object has been attached (the OnAttached() method has been invoked). When an element is detached this property is set to false IsActivated Indicates if this object has been activated (the OnActivated() method has been invoked). IsStarted Indicates if this object has been started (the Start() method has been invoked). IsDestroyed Indicates if this object has been destroyed (the OnDestroy() method has been invoked). Initialization These methdos get usually called when the Application starts. OnLoaded() This method is called after the element is deserialized (during the scene loading) or it is created from code: This method is invoked once during the object lifetime. OnLoaded() is usually used to initialize all variables and functionality that does not depends on other external elements. Note On this step, all Bindings are not yet resolved. OnAttached() This method is invoked when an element is attached into Evergine. All bindings are resolved prior execution of this method. This method is used to stablish dependencies with external elements. This method returns a boolean value. Return true if the execution has been succeded, otherwise the component is not suscessfully attached. Note On this step, all binding elements (Components for example) may not have been attached yet. OnActivated() This method is invoked when an element is activated, this can happend after the OnAttached() execution or we change the IsEnabled property. This method only is invoked if IsEnabled == true, if a Component or its Entity has been disabled, this method is not executed. During this method, we setup the functionality once we have previously stablished all dependencies. We usually put here all code that can be easily undone when the component will be deactivated (subscribe to events for example.) Note On this step, all dependencies (Components for example) have being previously attached, but some elements may not have been activated yet. Start() This method is called before the first frame update only if the element has been previously activated. This method is called only once per attachment. If we enable or disable a Component, the Start method is not called again. However, if we detach a Component and reattach again to an Entity, the Start() method will be called again (after the OnAttached() and OnActivated() invocations) In that method we usually put all initialization functionality that depends on other elements and that we only want to execute once. Note On this step, all dependencies have being activated, but some elements may not have been started yet. Per frame loop During the appliction Update/Draw loop, each Frame the following methods are invoked: Update() This method is only available on Behaviors, UpdatableServices or UpdatableSceneManagers. This method can only be executed if the element has been started (attached and initialized). In general terms, the Update() method is called once per frame. We put here execution code to update the application logic or state (player movement, camera input controller, etc...) Draw() This method is only available on Drawables This method can only be executed if the element has been started (attached and initialized). The Draw() method is called once per drawing camera, during the rendering phase. We put here all code to update RenderObjects before the camera will process them. Deinitialization Evergine follows the following steps to properly destroy or detach an element OnDeactivated() This method is called when an activated element becomes disabled or inactive. This method is the opposite of OnActivated() method, and it's a good practice to undo all functionalities done in the OnActivated() method (unsubscribe events for example). OnDetach() OnDetach is called when an element is detached This method is the opposite of OnAttached() method, and it's a good practice to undo all functionalities done in the OnAttached() method. OnDestroy() This method is invoke when the element is defintively removed and we want to destroy or dispose objects. A destroyed element cannot be attached again, and it is finally disposed. A good practice is to remove all internal data of this component (tables, collections, etc...)"
  },
  "manual/basics/project_structure.html": {
    "href": "manual/basics/project_structure.html",
    "title": "Project Structure | Evergine Doc",
    "keywords": "Project Structure Evergine projects need to be inter-connected so we can deal with different platforms or rendering system, for example. That means that by default an Evergine project has a specific structure. Folders / Projects When we create our Evergine application, it defines by default one Visual Studio Solution for every Project profile we creates. It will also define the following folders: A description of this project structure: Folder Example Element description [ProjectName].weproj NewProject.weproj This is the Evergine Project file. This file contains metafile information about the project profiles, packages among other things. If you double click this file, the project will be opened in Evergine Studio Content/ Content/ Contains all the Evergine assets of the project. Every texture, model, scene of the project are saved in this folder. [ProjectName]/ NewProject/NewProject.csproj It contains [ProjectName].csproj, the base project where usually the main Evergine code are defined (scenes, components, services, etc.). All code written in this project will be shared between all profile projects. [ProjectName].[Profile]/ NewProject.Windows/NewProject.Windows.csproj NewProject.Windows.sln There will be a folder with every application profile. It contains [ProjectName].[Profile].csproj, the Launcher application for that specific profile carrying all its specific classes and logic. For example, the project for UWP.MixedReality will be a UWP Mixed Reality application. Additionally, a Visual Studio Solution is created per each profile. In that projects we reconmend to put all specific code for that profile. [ProjectName].Editor/ NewProject.Editor/NewProject.Editor.csproj Contains [ProjectName].Editor.csproj project that contains the Evergine Editor customizations. For example, for showing a custom panel for a specific component is created here. Visual Studio Solutions As we have mentioned before, per each different Profile (Windows, UWP, Android, etc...) Evergine will create a Visual Studio solution. This solution will launch the Evergine application in the specified target. So for example, a Windows profile will produces a .Net desktop application, however an Android profile solution will create a Xamarin.Android project to deploy in devices. Note Every profile solution may require differents Visual Studio features (\"Mobile development with .Net\" for mobile devices, or \"UWP development\" for UWP profiles...). Follow Visual Studio indications to install the missing features. Custom structue Evergine supports custom structures in your application. It only requires that every specific profile solution can build properly."
  },
  "manual/basics/scenes/create_scenes.html": {
    "href": "manual/basics/scenes/create_scenes.html",
    "title": "Create scenes | Evergine Doc",
    "keywords": "Create scenes When you create a project, it contains a initial Scene (MyScene.wescene) containing some base entities. In Evergine scenes are created like another Asset. Create scene from Evergine Studio There are two ways of creating a Scene from Evergine Studio. In the Assets Details panel, right click and select Create scene menú item. Alternatively, in the main menu, select Assets > Create scene Note When creating a Scene, the new asset will be placed in the current selected Asset Folder, the one selected in Project Explorer panel and the one showed in Assets Details panel. Create a scene by code To generate a new Scene by code we just need to create a new class extending Scene: Create a new class in your Evergine project (MyScene.cs for example). Make that class inherit from Scene class. You can override the following important methods: Method Description CreateScene() Method where all the scene entities are created and added to the Scene. RegisterManagers() Method where the SceneManagers are loaded. base.RegisterManagers() will register the default SceneManagers. Start() Called just before the Scene start updating and drawing. End() Ends the scene Pause() Called when the scene is paused. It can be due the app being suspended or either manually paused through the ScrenContextManager. Resume() Resume the Scene when it's dead.. Here is a code example: public class MyScene : Scene { public override void RegisterManagers() { base.RegisterManagers(); this.Managers.AddManager(new Evergine.Bullet.BulletPhysicManager3D()); } protected override void CreateScene() { // Add scene entities. // Create a camera var cameraEntity = new Entity(\"camera\") .AddComponent(new Transform3D()) .AddComponent(new Camera3D()) .AddComponent(new FreeCamera3D()); this.Managers.EntityManager.Add(cameraEntity); } }"
  },
  "manual/basics/scenes/index.html": {
    "href": "manual/basics/scenes/index.html",
    "title": "Scenes | Evergine Doc",
    "keywords": "Scenes A Scene is a basic scenario that are composed by Entities. They usually represents one screen, although there are ways for showing more scenes at the same time. The above image shows a scene with a robot, light, camera and environment entities. Scenes are a type of asset. In this section Create Scenes Using Scenes Scene Managers Scene Editor"
  },
  "manual/basics/scenes/scene_editor.html": {
    "href": "manual/basics/scenes/scene_editor.html",
    "title": "Scene Editor | Evergine Doc",
    "keywords": "Scene Editor Evergine supports a complete Scene Editor. It allows creating entities, modifying them, adding / removing components. It contains the following sections: Scene Toolbar. Scene Viewport. Entities Hierarchy. Entity Details. Scene Toolbar It contains useful controls for adjusting the scene edition. Control Description Camera selection. Allow the Scene Viewport to visualize one of the scene cameras. Perspective is the default value, representing Evergine Studio camera. Set the transform manipulation in Translation Mode. Set the transform manipulation in Rotation Mode. Set the transform manipulation in Scale Mode. / Toggles the transform manipulation from local axis to global axis. When enabled, translation manipulation is done by steps of a custom value (0.5, 1, 5, 10, 50, 100) When enabled, rotation manipulation is done by steps of a custom value (5, 10, 15, 30, 45, 60 and 90 degrees) When enabled, scale manipulation is done by steps of a custom value (0.001, 0.01, 0.1, 0.5, 1, 5, 10) Open a dialog with the properties of the editor scene camera. (More details below) Toggles the visibility of the grid in the viewport. Editor Camera properties This dialog sets the properties of the Editor camera. This camera is the default camera when editing your scene. The properties shown in this panel are the same that appear when editing a Camera3D component. More information in this article Scene Viewport This area allows navigating the camera through the scene and also selecting, transforming and manipulate all the en entities of the scene (Cameras, Lights and other types). On top of the viewport there is the Scene toolbar where the user can adjust how the viewport behaves. Controls Action Description Left Mouse Select entity. Right Mouse Rotate camera. Right Mouse + Arrows / WASD Move camera. Right Mouse + Mouse Wheel Change camera speed up / down. Right Mouse + Shift Speed up camera by 2. Middle Mouse Pan camera Mouse Wheel Zoom in/out camera. Ctrl + D Duplicate selected entity. G Toggle Grid visibility W Set Translating manipulation mode. E Set Rotating manipulation mode. R Set Scaling manipulation mode. Basic Manipulation When selecting an entity, a Bounding Selection Box will appear and also a manipulator for adjusting the entity Transform3D. There are 3 different transform manipulation, selected by the above keys (W, E and R), or throug the Toolbar: Translation Moves the entity through the scene. It allows to translate the entity: 3 main axis (X, Y an Z) as an one dimension translatio. 3 main surfaces (XY, XZ and YZ) as a two dimensional translation. Rotation Rotates the entity through one rotation axis. Those axis are: X axis. Y axis. Z axis. Screen Axis, the entity will rotate around the camera (using the camera forward as axis). Scale Scales the entity in one or more axis: X axis. Y axis. Z axis. Uniform, scales proportionally so the entity proportions remain the same. Note Scaling manipulations always use local axis. Entities Hierarchy This panel shows the entity tree of the scene. Every node represents an entity so it reflects entity hierarchy. When a node has some children mean that entity has some entity children. Operations Entities can be rearranged. This will cause the entities to be relocated to another parent. When this operation is made the overal world transform (scale, rotation and translation) tries to remain constant during the process. Entities can be removed. Pressing the Supr button will delete the selected entity and all their children. Double click in an entity will focus it in the Scene Viewport. Clicking in the button will show the Add Entity dialog. More details in the Using Entities article. In the botton bar shows the total number of entities in the scene (137 in above image) Entity Details This panel shows all the properties of a selected entity. It shows all the entity parameters like name, tag and enable status, and also shows an accordion panel of their components. Here are some specific controls: Controls Control Description Toggles the entity as static entity. Collapses the visibilisation of all the components Expand the visibilisation of all the components More details in the Using Entities article."
  },
  "manual/basics/scenes/scenemanagers.html": {
    "href": "manual/basics/scenes/scenemanagers.html",
    "title": "Scene Managers | Evergine Doc",
    "keywords": "Scene Managers In Evergine the SceneManagers are elements that control some important aspects of the scene. They are not based as entities because they behave as a global element of the scene and cannot be attached into a specific entity. All SceneManagers in a Scene can be accessed using the Scene.Managers property. This allow to register and access SceneManagers. Default SceneManagers. Evergine scenes are created with some SceneManagers by default. These are: SceneManager Access Property Description EntityManager this.Managers.EntityManager Controls the entities of the scene. More information in the EntityManager article. AssetsSceneManager this.Managers.AssetsSceneManager Controls the asset of the scene. More information below. BehaviorManager this.Managers.BehaviorManager Manages the behavior update execution of the scene and their priority order. RenderManager this.Managers.RenderManager Handles the Rendering of the scene. More information in the Rendering article. EnvironmentManager this.Managers.EnvironmentManager Controls the environmental information of the scene. Their Reflection Probes, their environmental radiance and irrandiance maps, etc. More information in the Environment article. PhysicManager3D this.Managers.PhysicManager3D Manages the physic simulation. All the Rigid or Static bodies are registered into this manager. More information in the Physic Manager and Bullet article. Note The PhysicsManager3D scene manager is not registered by default although in the project template it's loaded in the RegisterManagers method of the template scene class. Assets Scene Manager The AssetsSceneManager controls all the asset loaded in the scene. When the scene is disposed AssetsSceneManager automatically unloads all the assets of the scene, releasing all the GPU memory in the process. More information in the Use Assets article. Create custom SceneManager In the previous section we've covered all the default SceneManagers of the scene. However you can create your own SceneManager. For doing that: Create a new class in the Evergine project in the Visual Studio Solution. Set that the new class inherit from SceneManager class. The new created scene manager can use all the lifecycle methods like OnAttached, OnDetach, etc. More information in Life cycle article. UpdatableSceneManager In addition, instead on creating a class that inherit SceneManager we can choose to inherit from UpdatableSceneManager. This class has an Update method that will be called in every frame. This is useful for managers that need to update some components in real time. For example, PhysicManager3D controls the physic simulation using this method. Using SceneManagers Register a Scene Manager We can register a new SceneManager from code like this: // We can register a scene manager directly using the actual instance. this.Managers.AddManager(new MyManager()); // Alternatively we can register by type. Useful when dealing with abstract classes. this.Managers.AddManager<PhysicsManager3D>(new BulletPhysicsManager3D()); Unregister a Scene Manager. We can also unregister calling the following function: // We can remove the manager using its instance. this.Managers.RemoveManager(myManagerInstance); // Or the type. For example, next line will remove BulletPhysicsManager3D manager that we registered in the previous section. this.Managers.RemoveManager<PhysicsManager3D>(); Obtaining a SceneManager We can obtain a registered SceneManager in several ways. Using [BindSceneManager] attribute With the [BindSceneManager] attribute in your Component or another custom SceneManager (See the Binding article) public class MyComponent : Component { [BindSceneManager] public RenderManager RenderManager; protected override void Start() { // Once the component is attached, the RenderManager property has being bound... this.RenderManager.DebugLines = true; } } Searching the SceneManager directly Using the SceneManager.FindManager () you can find your desired manager MyManager manager = this.Managers.FindManager<MyManager>(); // This for example will return the BulletPhysicsManager3D registered before. PhysicsManager3D physics = this.Managers.FindManager<PhysicsManager3D>(); // For default SceneManagers you can access using quick properties :) RenderManager renderManager = this.Managers.RenderManager; Scene managers in Evergine Studio Until Evergine 2022.2.16, default scene template included an entity named SceneManagers to configure some of built-in managers. From now, this entity is no longer included when you create a new scene, as scene managers are included in a separate panel. In that panel, you count with a user interface very similar to Entity Details panel, where you can manage components for an entity. In this case, you can manage scene managers for your scene. Behavior is the same: to add a new manager, just make use of add button and search for the element you want. By default, managers search window will scan all available managers in your project and Evergine core libraries. If you don't want that one of your custom managers to be selectable, mark your class with [Discoverable(false)] attribute. [Discoverable(false)] public class MyManager : SceneManager { } To remove a manager from a scene, just right click over the element and select remove option. There are a few managers that are required for the engine to correctly work, so you should be careful about removing them. Anyways, a confirmation message will be displayed if you try to remove one of those managers. Migrating older scenes For scenes created with Until Evergine 2022.2.16 or below, Evergine Studio will ask you for asset update once you open the scene file. It will automatically copy your custom values for shadows and environment managers located in obsolete SceneManagers entity to scene managers saved within your scene. It is recommendable to confirm scene update when requested, but this process can't be undone if, for some reason, you decide to go back to Evergine 2022.2.16. Once scene asset is updated, SceneManagers entity will be removed and replaced by entries in managers file section."
  },
  "manual/basics/scenes/using_scenes.html": {
    "href": "manual/basics/scenes/using_scenes.html",
    "title": "Using Scenes | Evergine Doc",
    "keywords": "Using Scenes For loading and launching a Scene from code we have to use ScreenContextManager and understand the concept of ScreenContexts. ScreenContext A ScreenContext represents a list of Scenes that can be simultanously loaded in the application at the same time. Note For example, this the diagram above depicts a ScreenContext containing two scenes: MainScene for the application logic, and UIScene another one just the UI. ScreenContextManager ScreenContextManager is a Service that manages the Scene navigation between scenes or, more accurately, ScreenContext. Its main methods are: Method Description To Navigating to a new ScreenContext (passed by parameter), replacing the previous ScreenContext. Push Navigtes to a new ScreenContext but keeps the previous one in a stack so we can restore later. Pop Removes the current ScreenContext and restores the previous stacked one. FindContextByName For searching among the ScreenContext list. So loading and navigating to a Scene by code would be like this (this code is placed in the Application class): // Loads the scenes. will create MyScene and UIScene objects which should be existing classes that intherit Scene class. // These scenes are populated with all entities defined in its respectives assets (MainScene.wescene and UIScene.wescene) var baseScene = assetsService.Load<MyScene>(EvergineContent.Scenes.MainScene_wescene); var uiScene = assetsService.Load<UIScene>(EvergineContent.Scenes.UIScene_wescene); // Creates a context and navigate to it. ScreenContext screenContext = new ScreenContext(scene, uiScene); screenContextManager.To(screenContext); For more details read the ScreenContextManager section."
  },
  "manual/basics/services.html": {
    "href": "manual/basics/services.html",
    "title": "Services | Evergine Doc",
    "keywords": "Services In Evergine, Services are elements that allow you to manage global features. A Service functionality could be accessed from every Scene, Component or Behavior in your application. The services could be bond from any component even other services using the aplication container. Developing custom Evergine services are useful to integrate your application with external external services or APIs. There are two kind of services: Basic Services: This kind of service is very useful to expose functionality or to execute global tasks. Updatable Services: Is a Service subclass with an Update() method that allows running an action every application update cycle. Creating a Service To create a basic Service, add a class from Visual Studio and extend the Service class: using Evergine.Framework.Services; namespace MyProject { public struct MyServiceData { public string name; public int requests; } public class MyService : Service { private MyServiceData data; public MyServiceData Data { get => this.data; private set => this.data = value; } public MyService() { this.data.name = \"myService\"; } public void DoRequest() { this.data.requests++; } } } Create an UpdatableService On the other hand, to create a updatable service add a class from Visual Studio and extend the `UpdatableService' class. using Evergine.Framework.Services; public class MyUpdatableService : UpdatableService { public override void Update(TimeSpan gameTime) { // Called on every application update cycle... } } Register a new Service in your Application Before using a service is necessary to register it in the application container where you can register the type or an instance. public partial class MyApplication : Application { public MyApplication() { // Previous code :) // You could register the service by type... this.Container.RegisterType<MyService>(); // Or register the Service using an instance... this.Container.RegisterInstance(new MyService()); } ... Using Services Accessing registered services could be done by two ways: Using [BindService] attribute You can use the [BindService] attribute in your Component, SceneManager or even from another Services to automatically inject the Service instance into your property. using Evergine.Framework; using System; namespace MyProject { public class MyBehavior : Behavior { // Use the BindService attribute on top of the property or attribute in which you want to inject the Service [BindService] private MyService myService = null; protected override void Update(TimeSpan gameTime) { this.myService.DoRequest(); } } } Using Application Container In the other hand, you could obtains the Service instance directly from the Application Container: using Evergine.Framework; using System; namespace MyProject { public class MyBehavior : Behavior { private MyService myService = null; protected override bool OnAttached() { // Use the Resolve<Type> method from the Application Container.... this.myService = Application.Current.Container.Resolve<MyService>(); return base.OnAttached(); } protected override void Update(TimeSpan gameTime) { this.myService.DoRequest(); } protected override bool OnDettached() { // Release the reference when a component is being dettached... this.myService = null; } } }"
  },
  "manual/basics/transform.html": {
    "href": "manual/basics/transform.html",
    "title": "Transform3D | Evergine Doc",
    "keywords": "Transform3D Transform3D is maybe the most important Component in Evergine. The Transform3D components are used to store the Entity positiom, rotation and scale. Parent / Child hierarchy when an Entity is a Parent of another Entity, the Child Entity will move, rotate and scale in the same way as its Parent does. You can think of parent / child hierarchy as being like the relationship between your arms and your body; whenever your body moves, your arms also move along with it. Child objects can also have children. Your hands can be considered as children of your arms, and your fingers are hands children. Local vs World space 3D and 2D position and transformations exist within a coordinate systems called spaces: World Space World Space is the coordinate system for the entire scene. Its origin is at the center of the scene: Position [0, 0, 0] Scale [1, 1, 1] Rotation [0, 0, 0] In an entity hierarchy this space is used to get the global position of your body hand (following the example explained above). Local Space Local Space is the coordinate system where position, rotate and scale is used taking the entity parent as reference. Transform3D Properties When you set a local property, the equivalent world property is set automatically, and vice versa. So, you can place a child object in a global position, and the Transform3D component translates it to local space. Local Space properties The following properties are used to modify Transform3D in local space: Property Type Default Description LocalPosition Vector3 Vector3.Zero This is the position of the entity in local space LocalOrientation Quaternion Quaternion.Identity This is the orientation of the entity in local space LocalRotation Vector3 Vector3.Zero This is the rotation of the entity in local space expressed in Euler angles. LocalScale Vector3 Vector3.One This is the scale of the entity in local space LocalTransform Matrix4x4 The transform matrix relative to the parent transform. Is obtained by combining the LocalPosition, LocalOrientation and LocalScale (TRS matrix). Note As you can see, we use the terminology Orientation to express quaternion properties, and Rotation to express Euler angles. Tip Internally the orientation/rotation are stored in a Quaternion value, so, for performance reasons, we reconmend to use Quaternion properties (apart from the great benefits that Quaternion offers itself). World Space properties The following properties are used to modify Transform3D in local space: Property Type Description Position Vector3 This is the position of the entity in world space Orientation Quaternion This is the orientation of the entity in world space Rotation Vector3 This is the rotation of the entity in world space expressed in Euler angles. Scale Vector3 This is the scale of the entity in world space WorldTransform Matrix4x4 The matrix that transforms from local space to world space. Is obtained by combining the Position, Orientation and Scale (TRS matrix). Note The initial Position, Orientation and Scale values depends on parent Transforms, because parent translation, scaling or orientation chages are propagated. Events You can subscribe to transform events to be notified when one of its properties changes: Event Description LocalPositionChanged Triggered when the LocalPosition is changed. LocalOrientationChanged Triggered when the LocalOrientation is changed. LocalScaleChanged Triggered when the LocalScale is changed. LocalTransformChanged Triggered when the LocalTransform is changed. PositionChanged Triggered when the Position is changed. OrientationChanged Triggered when the Orientation is changed. ScaleChanged Triggered when the Scale is changed. TransformChanged Triggered when the WorldTransform is changed. Hierarchy properties Property Description ParentTransform Indicates the transform of the parent entity. null if the entity has no parents. ChildrenTransform The list of children Transforms. LocalScaleChanged Triggered when the LocalScale is changed. LocalTransformChanged Triggered when the LocalTransform is changed. PositionChanged Triggered when the Position is changed. OrientationChanged Triggered when the Orientation is changed. ScaleChanged Triggered when the Scale is changed. TransformChanged Triggered when the WorldTransform is changed. Useful methods Method Description LookAt(targetPosition, up) Rotates the entity to point to the target position and up vector. All calculations are made in world space. LocalLookAt(targetPosition, up) Rotates the entity to point to the target position and up vector. All calculations are made in local space. SetLocalTransform (localPosition, localOrientation, localScale) Compute the local transform and update all local space properties. Tip We reconmend to invoke the SetLocalTransform() method instead of setting LocalPosition, LocalOrientation and LocalScale by separate."
  },
  "manual/evergine_launcher/create_project.html": {
    "href": "manual/evergine_launcher/create_project.html",
    "title": "Create a new project | Evergine Doc",
    "keywords": "Create a new project To create a new project from Evergine launcher, go to the My Projects section and click on the Add new project button. The project configuration windows will be opened. Then you can choose the Project Name, the disc location of your project, and the Evergine version to use. Also, you can choose a single or multiples profiles for your project. Tip New profiles can be added later As number of templates has been growing in latest Evergine releases, we have decided to add some filters that may be useful to find desired template. You have a free text input and a platform dropdown selector that you can combine to limit template results. Open your new Evergine project Finally, click on the Create button on the bottom right side of the configuration panel to create and open your project in Evergine Studio. Next steps Larn how to open your Evergine project in Visual Studio"
  },
  "manual/evergine_launcher/manage_versions.html": {
    "href": "manual/evergine_launcher/manage_versions.html",
    "title": "Manage Evergine versions | Evergine Doc",
    "keywords": "Manage Evergine versions In the Versions section of the Evergine Launcher you can find all available Evergine versions. From this section, you can install/uninstall, search for a specific version in the Evergine repositories. Repositories Evergine Studio has different repositories with version collections that you can install. The official repositories are: Repository Description URL Stable This collection has the stable Evergine versions. https://everginestudio.blob.core.windows.net/stable Preview This collection has the preview Evergine versions. https://everginestudio.blob.core.windows.net/preview Nightly This collection has the nightly Evergine versions. https://everginestudio.blob.core.windows.net/nightly Manage repositories You can add or manage your repositories collection click on the Repositories button on the bottom right side of the Versions section. Group by Repository The Versions section allows filter the available Evergine version by repository or group by them to help you find a specific version. New version indication When a new Evergine version is published on some of the available repositories a New Version label will be shown close to the Versions section."
  },
  "manual/evergine_launcher/samples_learning_support.html": {
    "href": "manual/evergine_launcher/samples_learning_support.html",
    "title": "Samples, Learning and Support | Evergine Doc",
    "keywords": "Samples, Learning and Support In Evergine launcher you can have access to learning and samples sections to learn more about how to use Evergine and the Support section in the case that you have doubts or need help with something. Samples In this section, you can find an official Evergine sample collection. These samples are stored in GitHub so you can navigate through the collection and click on the Web button to go to sample code in Github. Learning Here you can find the the Official Manual, the API Documentation, and the official Evergine Youtube Channel with useful resources. Support In this section you can find the official Evergine support that includes: Comunity support where you can create a ticket and vote for existing tickets. Priority support Portal where you can create priority tickets that will be resolve in less to 48 hours for an Evergine team member (only available if you contract Priorty Support service). Access to Evergine Source code (only available if you contract Source Code Access service)."
  },
  "manual/evergine_studio/assets/create.html": {
    "href": "manual/evergine_studio/assets/create.html",
    "title": "Create assets | Evergine Doc",
    "keywords": "Create assets In Evergine there are two ways of creating an asset depending on its type : Importing a resource file to the Assets Details panel, either with drag and drop or selecting the Import asset menu item. This works with images, 3d models and sound files. Creating an asset directly from Evergine Studio. Only for non resource and Evergine specifics assets. Metafile generation When creating an asset, a Metafile will be generated with the same name of the resource file. That file will contains all the properties and profile specifications of the asset. Every asset type has a different extension, described below. Importing assets Drag and drop the resource file You can drag any file into the Evergine Studio to import that asset file into the project and create an asset with it. Depending of the file extension Evergine Studio will decide what kind of asset will be created in the panel folder: Asset type Metafile extension Supported file extensions Texture .wetx .jpg , .jpeg , .png , .bmp , .tga , .ktx , .dds , .hdr Model .wemd .gltf , .glb , .fbx , .3ds , .obj , .dxf , .dae Audio .wesn .wav , .mp3 , .ogg File .wefile Any other file. Font .weft .ttf, .otf Import an asset by dragging and dropping a resource file from your File Explorer to the Assets Detail panel. Use the import asset menu item You can also import a resource file selecting the Import Asset menu item located in: In the Assets main menu. In the button on the Assets Details panel. The Asset Details panel contextual menu. Create assets without resource file. Evergine uses a variety of assets that don't require external resource files. Those assets can be created directly from the Evergine Studio. Like in the previous section, you can access the Assets menu items in three places: In the Assets item in the main menu. In the button on the Assets Details panel. In the Assets Details contextual menu. This way we can create the following assets: Asset type Metafile extension Additional files Scene .wesc .wescene file with the scene elements. A folder with the scene name that contains Scene subassets like the environment probe. Effect (graphic effect and compute shader) .wefx Sources folder, containing a Shader.fx file with the shader description Sampler .wesp Material .wemt Render Layer .werl Post-Processing Graph .wepp Note If there is already an asset with the same name than the importing resource file, the new asset will be renamed adding a number suffix in the name. For example, texture.jpg would be renamed to texture(1).jpg and texture(2).jpg if another file with the same name is imported."
  },
  "manual/evergine_studio/assets/edit.html": {
    "href": "manual/evergine_studio/assets/edit.html",
    "title": "Edit assets | Evergine Doc",
    "keywords": "Edit assets Evergine Studio offers the possibility of editing plenty of properties of every asset to fully customize the use of that asset into your app. Also, every Evergine project also defines a set of profiles, so you can also set different properties per profile. Note For example, we can halve the resolution of a texture for the Android or iOS profiles. Or sets a different PixelFormat. Contextual Actions We can apply some actions to the assets regardless its type. Right click to an asset in the Assets Details panel to show a contextual menu with the following actions: Rename the asset. Cut the asset (For cut/paste operation). Copy the asset (For copy/paste operation). Duplicate will create an exact copy of the asset but with different Id. Delete asset. Set to export as raw copies to the Export Content Folder the resource file instead the compiled exported version. Open folder location will open the File explorer in the same location of the asset. Copy path to clipboard. Copies the path of the asset metafile. Copy id to clipboard. Assets Editor To edit an asset: Click on it in the Assets Details panel. A new panel will be opened, specific for that asset. Every Asset editor panel is different, but some of the most common areas are the next ones: Asset viewer area. Global properties panel. Profile properties panel(s). Viewer area Usually the viewer area of the asset will show a visual representation of the asset. It's also common to have some sort of controls to change the visualization settings. Some examples are: Enabling / disabling Texture channels. Selecting the Level of Mipmap of a Texture. Play/Pause a Model animation. Set the geometry where a Material is applied. Global properties Usually the global properties are the one that applies for all the Project profiles. Next image shows the Render Layer parameters: Profile properties Present in many assets there is a Profile parameters area where users can customize properties per profile. Usually this area is similar to this: This area shows: Profile tabs: It shows the default profile tab and one tab per Project profile. Allows to switch to a specific profile property panel. Default profile: Sets the profile properties by default when no other profile is specified. Override Default Property: When enabled, it enables the custom properties of this profile. If disabled, the default profile will be used. Exclude Asset: Property that allows an asset not to be exported in a specific profile, hence making it only available in some of the profiles."
  },
  "manual/evergine_studio/assets/export.html": {
    "href": "manual/evergine_studio/assets/export.html",
    "title": "Exporting assets | Evergine Doc",
    "keywords": "Exporting assets Evergine usually doesn't load the resource files (.jpg, .fbx, etc.) directly at runtime. However, it processes the resource files and compile or export them into actual binary files that can be properly and efficiently loaded into Evergine. For example, the Model asset exported file contains areas that can be directly copied into buffers and uploaded into the graphic card. Note Evergine actually can load direct resource files at runtime. However it relies on third-party libraries that would make the asset workflow much slower and memory consuming. Also you wouldn't be able to edit their properties. However for many application that's not only a good option, but a desirable one. Specially if it needs to load images dynamically loaded from internet, for example Exported asset file extensions. Every asset type defines the exported file asset extension (in the same way it defines its metafile file extension). Below you can find them: Asset type Exported extension Texture .weptx Model .wepmd Sound .wepsn Scene .wepsc Effect .wepfx Sampler .wepsp Material .wepmt Render Layer .weprl Post Processing Graph .weppp Export process Evergine asset export process is actually a pipeline with the next steps: Gather information about the resource file (if any). Get the application profile name. Check if the asset metafile contains specific parameters for the app profile. If there is a specific profile definition use those parameters. Otherwise the asset will use its default profile parameters. Convey all the parameters of the asset and generate the exported binary file."
  },
  "manual/evergine_studio/assets/index.html": {
    "href": "manual/evergine_studio/assets/index.html",
    "title": "Assets | Evergine Doc",
    "keywords": "Assets An asset is an item that represents an element in Evergine Studio that can be used in your project. It can represent visual or graphic elements like 3D models, textures, or either more abstract engine elements like sampler states, materials and render layers. Type of assets Here it is a summary of different asset types. Assets with resource file Some assets are created using external applications, like 3ds Max, Blender or Photoshop. These assets are resource files that can be imported in Evergine Studio and used in your project, either by the Studio or directly by code. Here are the asset with resource files that Evergine currently supports. Asset Description Supported file extensions Texture Image file used as texture graphic resource. .jpg, .jpeg, .png, .bmp, .tga, .ktx, .dds, .hdr Model 3d model with geometry, animation and material information. .gltf, .glb, .fbx, .3ds, .obj, .dae Sound Audio file used for music and sound effects .wav, .mp3, .ogg File Any file that does not satisfy the previous formats Any other file. Font Font file used as font graphics resource .ttf, .otf Assets created only by Evergine However, some assets represents just abstract elements that can only be created by Evergine Studio, some of them even by code. They don't have any external associated resource file. Some of these assets have complex folder structure, while others are more basic. Asset Description Scene Main Evergine asset. It defines an entity graph that populates a scene, and also define their components. Effect Contains a HLSL shader. It automatically translates to other shading languages like GLSL. They are also flavored with custom attributes and annotations for a better integration. Post Processing Graph Defines a visual post processing graph node, using compute shaders for effects like anti-aliasing, tone mapping, SSAO, and many others. Material Represents how a geometry is rendered. It references an Effect asset and describes its parameters like textures and values. Prefab Contains an entity hierarchy that can be instanced in any scene. Render Layer Contains Rasterizer, Blending, Depth and Stencil information. Every material needs a Render Layer asset. Sampler State Element that represents a texture sampler state description, like filtering, clamping or wrapping information. In this section Create Assets Edit Assets Export Assets Use Assets"
  },
  "manual/evergine_studio/assets/scenes.html": {
    "href": "manual/evergine_studio/assets/scenes.html",
    "title": "Scenes | Evergine Doc",
    "keywords": "Scenes Scenes are a 3D space where you can create your experiences/projects. The evergine scene is composed of entities and each entity will be an element of your scene. Scenes are a type of asset and have a dedicated Editor Scene Editor. You can find the scene assets in the Assets Details panel when you select a folder in the Project Explorer. The scene file has the wscene extension and always come together a file .wesc descriptor and a folder with the same name of the scene. Create a new scene asset You can create a scene click button on from Assets Details panel to deploy a create menu options and click on the option \"Create scene\""
  },
  "manual/evergine_studio/assets/use.html": {
    "href": "manual/evergine_studio/assets/use.html",
    "title": "Use assets | Evergine Doc",
    "keywords": "Use assets We can use an asset in our project in these ways: Reference it in an entity Component. Reference it from another asset. Load it from code. Reference asset from components. Lots of components can use assets. For example, MeshComponent uses Model assets and Sprite uses Texture assets. When a component uses an asset, it will show a Asset Selection Control in its section in the Entity Details panel. When a Scene is loaded in Evergine, all referenced assets by components will be loaded automatically. To add an asset to that component, we need to click on it and a Asset Picking Dialog will appear, allowing is to select a desired asset. The user can also fill the Asset filter textbox to filter all the assets, making it easier in big projects. Clicking the lens icon will select the asset in the Asset Details panel. Useful to locate and edit a specific asset used in your scene. Clicking in an asset of the list will select it and set it as the property value of the component. To clear the asset reference, just select the No Asset element on the list (it's the first one). Note The dialog will only shows assets of the same type of the defined by the component property or field. Reference asset by other assets In the same way as the components, assets can reference other assets. For example, a Material can reference a Texture and a Texture references a SamplerState asset. You can reference those assets in the same way you add them to components (see above). Reference assets by code. An asset can be loaded an accessed at runtime using in two ways depending of the asset scope: AssetsService: For loading global assets, used in more than one Scene. AssetsSceneManager: For loading assets in a Scene. AssetsService loading AssetsService is a Service that manages all the assets in the application. When loading an asset using this service we are also responsible of unloading it when it's no longer needed. var assetsService = Application.Current.Container.Resolve<AssetsService>(); Texture textureAsset; // Asset loading. // Load asset by id (using EvergineContent). textureAsset = assetsService.Load<Texture>(EvergineContent.Textures.SampleTexture_png); // Load asset by path. textureAsset = assetsService.Load<Texture>(\"SampleTexture.wetx\"); // Load asset by stream (we need to provide an asset name anyways). textureAsset = assetsService.Load<Texture>(\"SampleTexture.wetx\", stream); // Asset unloading. // Unloads asset by id. assetsService.Unload(EvergineContent.Textures.SampleTexture_png); // Unload asset by path. assetsService.Unload(\"SampleTexture.wetx\"); AssetsSceneManager loading AssetsSceneManager is a SceneManager that controls all the assets in a specific Scene. All the assets loaded through this SceneManager will be unloaded when the Scene is disposed (When navigating to other scenes, for example). Its methods are pretty similar to the AssetsService. var assetSceneManager = this.Managers.AssetSceneManager. Texture textureAsset; // Asset loading. // Load asset by id (using EvergineContent). textureAsset = assetSceneManager.Load<Texture>(EvergineContent.Textures.SampleTexture_png); // Load asset by path. textureAsset = assetSceneManager.Load<Texture>(\"SampleTexture.wetx\"); // Load asset by stream (we need to provide an asset name anyways). textureAsset = assetSceneManager.Load<Texture>(\"SampleTexture.wetx\", stream); // Asset unloading. // Unloads asset by id. assetSceneManager.Unload(EvergineContent.Textures.SampleTexture_png); // Unload asset by path. assetSceneManager.Unload(\"SampleTexture.wetx\"); Force new instance when loading By default when an asset is loaded either in the AssetsService or the AssetsSceneManager only one instance of the asset is generated. This saves GPU memory and time. However, in some certain occasions we want to load a different instance* of an already loaded asset. For example, we want to load and use a Material and changing it but we don't want to change the rest of instances, only this one. In this case we can do using the forceNewInstance in the Load method. // Forces a new instance to load. Texture textureAsset = assetSceneManager.Load<Texture>(EvergineContent.Textures.SampleTexture_png, true); Texture textureAsset = assetsService.Load<Texture>(EvergineContent.Textures.SampleTexture_png, true);"
  },
  "manual/evergine_studio/index.html": {
    "href": "manual/evergine_studio/index.html",
    "title": "Evergine Studio | Evergine Doc",
    "keywords": "Evergine Studio Evergine studio is the most important tool for creating experiences in Evergine. This documentation section covers the principal topics of how to use this powerful tool. Click on the following links to know more about this tool. In this section Interface Assets Project Properties & Profiles Profile with RenderDoc"
  },
  "manual/evergine_studio/interface.html": {
    "href": "manual/evergine_studio/interface.html",
    "title": "Interface | Evergine Doc",
    "keywords": "Interface Evergine Studio's interface uses dockable panels to organize different section so you can create your own layout with the panel location that you prefer. The above image shows the default layout and his main panels are: Project Explorer: Is used to manage the assets and packages of the current project. Assets Details: Shows the assets of the current folder selected on Project Explorer. Allow you open, rename or create new assets. Asset Editor: Is used to edit assets (scenes, materials, textures, etc..). Some asset types, such as scenes, have dedicated editors where you can edit it. To open a dedicated editor (when available), double-click the asset in the Asset Details region. Asset Editors Evergine studio has the following editors that allow you manage the associate asset type: Editors Description Scene Editor Double-clicking a Scene asset opens a dedicated editor. Effect Editor Double-clicking an Effect asset opens a dedicated editor. Material Editor Double-clicking a Material asset opens a dedicated editor. Model Editor Double-clicking a Model asset opens a dedicated editor. Render Layer Editor Double clicking a RenderLayer asset opens a dedicated editor. Sampler Editor Double clicking a Sampler asset opens a dedicated editor. Sound Editor Double clicking a Sound asset opens a dedicated editor. Post-Processing Graph Editor Double clicking a Post-Processing graph asset opens a dedicated editor. Evergine Studio Layout As we have mentioned earlier, Evergine Studio layout is customizable. By clicking in the Window menu, you can manage the Evergine Studio layout:"
  },
  "manual/evergine_studio/project_profiles.html": {
    "href": "manual/evergine_studio/project_profiles.html",
    "title": "Project settings & Manage profiles | Evergine Doc",
    "keywords": "Project settings & Manage profiles As cross-platform engine, Evergine provides a way to create different launcher projects for supported platforms. For each platform, a C# project will be added to your filesystem. You can create more than a project for a given platform, but each one of those projects will be considered as a different profile. If you didn't add all platforms you want in project creation window, you can do it from Evergine Studio once your project is loaded. Just go to Settings > Project Settings menu to open a dialog to manage your project profiles. By default, an Evergine project includes Windows as platform. This platform is required for Evergine Studio to work, so you can't remove it. To add a new platform profile, use the Add button. A new window will be open, where you can look for platform you want to add. By default, a profile name will be proposed, but you can change it if you want. As described above, you can even have more than one profile for the same platform. This will create separated C# projects for each one of those platforms. In the same way as launcher creation form, we have added here some filters to make it easier to find a template. Just make use of free text input and platform dropdown selector to limit template results. Profile management For each profile, a different set of settings is applied. Once you add a new platform profile, a default set of values will be added for that platform. For example, in platforms like Windows shaders are compiled on runtime by default, but other platforms like Web or Android require shaders to be precompiled due to performance and platform limitations. Profile settings whenever can be customized at any time you want. Changes will be saved once you press Update button. List of options that you can configure are described below. Textures Controls texture compression to be used in a given profile. This is important to optimize textures to be loaded in graphics hardware, that may be limited in platforms like mobile devices. You can specify both alpha and non-alpha compression format. List of available formats are provided by PixelFormat enum. Shaders You can enable shaders to be precompiled and included in platform package. This may increase build time, but reduce loading time while running the application. Also, you can configure your own directive specifications that fits better to your project. For each directive you can choose on enabling or disabling it. Once enabled, you can also add each directive to a directive combination. At the bottom, there is a read-only text area that presents configured directive combinations. Open platform projects For each profile registered within your project, you will find a new entry in File > Open C# editor menu. When clicking on one of provided options, platform specific Visual Studio solution."
  },
  "manual/evergine_studio/renderdoc.html": {
    "href": "manual/evergine_studio/renderdoc.html",
    "title": "Profile with RenderDoc | Evergine Doc",
    "keywords": "Profile with RenderDoc RenderDoc is a graphics debugger currently available for Vulkan, DirectX 11/12 and OpenGL development on Windows, Linux and Android. It is integrated on Evergine Studio to make it easy to debug your application during the development process. To install the latest RenderDoc version visit the project website Loading RenderDoc First you need to load the RenderDoc assembly to allow the graphical commands that are sent to the GPU to be captured. In the setting menu of the Editor you will find and option called \"Enable RenderDoc\". This will reload the graphics device so you must save any changes, and afterwards RenderDoc will be ready to capture the scene. Capturing a frame with RenderDoc Once RenderDoc is enabled into the Evergine Studio, a new button will appear on the right side of the toolbar on the scene view. Pressing this button will trigger a capture of the next frame of rendering for the view. And a new RenderDoc instance will be launched to show the capture. From there you can open the capture and debug using the tool. Naming objects The Evergine low level API allow you to name all the different object types available, this include samplers, buffers, pipelines and much more. These names can then be displayed on RenderDoc to help to debugging the application. To set an object name, in a buffer for example, just set it as a parameter in the factory constructor or set the property Name. this.graphicsContext.Factory.CreateBuffer(ref Description, \"Buffer_Name\"); or buffer.Name = \"Buffer_Name\"; Debug markers and regions In addition to naming the Evergine low level API also adds the ability to place debug markers inside command buffers. These can be used to mark points of interest and highlight specific areas inside of the command buffer. Tip Note that contrary to naming objects, debug markers (and regions) have to placed inside of an active command buffer. commandBuffer.BeginDebugMarker(\"Region_Name\"); // Stuff commandBuffer.EndDebugMarker(); Including shader debug information By default to optimize the size of DirectX shaders, debugging information is stripped out. This mean that constants and resources will have no names, and the shader source will not be available. To include this debugging information in your shader you need to set the debug mode inside the pass shader code by adding [Mode Debug] line: [Begin_Pass:Default] [Mode Debug] [Profile 10_0] [Entrypoints VS=VS PS=PS] // ... Alternative graphics debugging technique If you build a desktop Windows application using DirectX, you can capture a frame and debug it using Visual Studio graphics debugger, NVidia Nsight Graphics or PIX on Windows"
  },
  "manual/extensions/armobile/index.html": {
    "href": "manual/extensions/armobile/index.html",
    "title": "ARMobile (Experimental) | Evergine Doc",
    "keywords": "ARMobile (Experimental) ARMobile is an extension to work with augmented reality experiences, so you will need compatible devices to run this kind of applications. By now, only supported platform is Android (ARCore) but we also plan to support iOS (ARKit) in the future. AR Mobile is based on XR Platform. Note Please, note that Evergine.ARMobile extension is in experimental state, so it has some limitations and known issues described at Known issues. Create a project using ARMobile To make it easier to create ARMobile projects, we have created a new project template for Android ARCore. It works with OpenGL as graphics backend, due to limitations related with ARCore, that does not support Vulkan. You also have to open Evergine Studio add-ons manager and add Evergine.ARMobile add-on. Once Evergine Studio loads default scene, you need to do some changes to make it work properly for ARMobile. For scene 3D camera, set background color to #00000000 color and remove FreeCameraBehavior component. Disable SkyAtmosphere entity. Go to Scene Managers tab and add ARMobileManager scene manager. Built-in AR.Mobile add-on prefabs AR.Mobile add-on provides a set of prefabs that you can use for common AR scenarios. Expand Dependencies > Evergine.ARMobile > Prefabs. You can find following prefabs here: Cursor, Face, FeaturePoints, Image and Planes. Cursor prefab This prefab draws a cursor which orientation changes depending on surface detection. Drag and drop Cursor prefab to your scene and set its position to (0, 0, 0). If you select prefab entity, will realize that apart from plane mesh components, there is a ARMobileHitTestBehavior that lets you choose about how plane collision is detected: every frame or just when user taps on the screen. Face prefab Face prefab allows you to use face tracking capabilities from AR platform. Drag and drop Face prefab to your scene and set its position to (0, 0, 0). Then, go to Scene Managers and mark FaceTrackingEnabled as checked. When you use face tracking detection, rear camera is no longer used and front camera is used instead. Because of this, any other AR mode like image tracking or plane detection will not work. If you select prefab entity, you can use attached MaterialComponent to change face material. You also can realize that there is a component called XRFaceTrackingMesh that creates face mesh from native buffers. Feature Points prefab This prefab uses LineBatch3D to render feature points in green, orange or red color, from more confident to less confident results. Confidence is saved in W field of Points property in XRFeaturePoints. Drag and drop FeaturePoints prefab to your scene and set its position to (0, 0, 0) and enable feature points detection in ARMobileManager scene manager. Image prefab With image prefab you can easily track images, and place 3D objects when those images are detected and their real world position changes. It includes a MaterialComponent that you can change to use a custom material for plane that appears once target image has been detected. Drag and drop Image prefab to your scene and set its position to (0, 0, 0) and enable image detection in ARMobileManager scene manager. Under Dependencies > Evergine.ARMobile > AR you will find default images dataset. It includes a single image, named arimage.jpg. Dataset file is in CSV format: Each line represents a single image from dataset. First column is the relative path to the image file. Second column is expected image size in real world, measured in meters. So, for default image, we expect it to be 20 centimeters wide. For each one of the images, we use file name (without extension) as image name, that we internally register in dataset database. This is why default ImageName value for XRImageTracking component in Image prefab is arimage, as this is the file name for default image in Evergine.ARMobile add-on. If you want a custom set of images, just create your own dataset within your project folders. Then, you have to change ImageDataSetPath value in ARMobileManager scene manager to meet your dataset path. To ensure your images are optimal for tracking, please follow platform recommendations: Android: use arcoreimg tool to evaluate your images quality. Planes prefab We provide this prefab to visualize detected planes. For this, drag and drop Planes prefab to your scene and set its position to (0, 0, 0). Then, go to Scene Managers and select a plane detection mode. Prefab counts with a XRPlaneRenderer that lets you change the material used to draw the planes. Known issues As ARMobile extension is in experimental state, you could find some issues while running applications. This is a list of known issues: Application crashes when returning back from background. You will find, in Debug configuration, some Google.AR.Core.Exceptions.DeadlineExceededException exceptions that are written to output console, depending on ARMobileManager configuration. When dragging AR prefabs to the scene, you need to ensure that their Transform3D location is set to (0, 0, 0) position. Any other position could provoke an OpenGL rendering issue that results in a full screen green texture."
  },
  "manual/extensions/imgui/features.html": {
    "href": "manual/extensions/imgui/features.html",
    "title": "Features | Evergine Doc",
    "keywords": "Features There are a list of different controls that you can create with just a few lines of codes, here is the complete list: Window Windows are drag-able containers of Controls. They can receive and lose focus when clicked. Because of this, they are implemented slightly differently from the other Controls. Each Window has an id number, and its contents are declared inside a separate function that is called when the Window has focus. ImguiNative.igSetNextWindowPos(new Vector2(420, 120), ImGuiCond.Appearing, new Vector2(1, 0.5f)); ImguiNative.igSetNextWindowSize(new Vector2(400, 100), ImGuiCond.Appearing); ImguiNative.igBegin(\"MyWindow\", this.imguiDemoOpen.Pointer(), ImGuiWindowFlags.None); // Controls ImguiNative.igEnd(); LabelText The LabelText is non-interactive. It is for display only. It cannot be clicked or otherwise moved. It is best for displaying information only. ImguiNative.igBegin(\"MyWindow\", this.imguiDemoOpen.Pointer(), ImGuiWindowFlags.None); ImguiNative.igLabelText(\"This is a label\", \"Value1\"); ImguiNative.igLabelText(\"This is other label\", \"Value2\"); ImguiNative.igEnd(); Button The Button is a typical interactive button. It will respond a single time when clicked, no matter how long the mouse remains depressed. The response occurs as soon as the mouse button is released. ImguiNative.igBegin(\"MyWindow\", this.imguiDemoOpen.Pointer(), ImGuiWindowFlags.None); if (ImguiNative.igButton(\"Press me\", new Vector2(100, 25))) { clicked++; } if (clicked > 0) { ImguiNative.igText(\"Thanks for clicking me!\"); } ImguiNative.igEnd(); Text The Text Control is an interactive, editable single-line field containing a text string. ImguiNative.igBegin(\"MyWindow\", this.imguiDemoOpen.Pointer(), ImGuiWindowFlags.None); ImguiNative.igText(\"This is a text control\"); ImguiNative.igEnd(); InputText The InputText Control is an interactive, editable area containing a text string. Toggle The Toggle Control creates a checkbox with a persistent on/off state. The user can change the state by clicking on it. Toolbar The Toolbar Control is essentially a row of Buttons. Only one of the Buttons on the Toolbar can be active at a time, and it will remain active until a different Button is clicked. This behavior emulates the behavior of a typical Toolbar. You can define an arbitrary number of Buttons on the Toolbar. SelectionGrid The SelectionGrid Control is a multi-row Toolbar. You can determine the number of columns and rows in the grid. Only one Button can be active at time. HorizontalSlider The HorizontalSlider Control is a typical horizontal sliding knob that can be dragged to change a value between predetermined min and max values. VerticalSlider The VerticalSlider Control is a typical vertical sliding knob that can be dragged to change a value between predetermined min and max values. HorizontalScrollbar The HorizontalScrollbar Control is similar to a Slider Control, but visually similar to Scrolling elements for web browsers or word processors. This control is used to navigate the ScrollView Control. VerticalScrollbar The VerticalScrollbar Control is similar to a Slider Control, but visually similar to Scrolling elements for web browsers or word processors. This control is used to navigate the ScrollView Control. ScrollView ScrollViews are Controls that display a viewable area of a much larger set of Controls. Images The images control allow to display the content of texture, this is a non-interactive control."
  },
  "manual/extensions/imgui/imguizmo.html": {
    "href": "manual/extensions/imgui/imguizmo.html",
    "title": "ImGuizmo | Evergine Doc",
    "keywords": "ImGuizmo This extension also allow you to use the ImGuizmo library, this is a collection of 3D widgets on top o ImGUI library, this project and source code is available on github at: https://github.com/CedricGuillemet/ImGuizmo. Note The C# wrapper is generated on top of a c-api wrapper version of this library, so some samples may not work directly but all the features are supported. Features This library comes with many widget in 3D useful to create an editor tool, here you have more info about the ImViewGizmo widget This widget display a 3D cube in a corner of the screen that represent the view transformation, it is used to select between different camera views preset like top, left, right and bottom. ImGuizmo widget This widget allow you to draw 3D helpers useful to move, scale and transform 3D geometry, this widgets compute the result 4x4 Matrix to make it easy to apply it to the geometry. ImSequencer widget A WIP little sequencer used to edit frame start/end for different events in a timeline. Graph editor widget This widget is useful to create a custom graph editor based on nodes and links between them. It is possible to change how nodes are rendered to customize them."
  },
  "manual/extensions/imgui/imnodes.html": {
    "href": "manual/extensions/imgui/imnodes.html",
    "title": "ImNodes | Evergine Doc",
    "keywords": "ImNodes This extension also allow you to use the ImNodes library, this is a simple, immediate-mode interface for creating a node editor within an ImGui window, this project and source code is available on github at: https://github.com/Nelarius/imnodes. Note The C# wrapper is generated on top of a c-api wrapper version of this library, so some samples may not work directly but all the features are supported. Imnodes aims to provide a simple, immediate-mode interface for creating a node editor within an ImGui window. Imnodes provides simple, customizable building blocks that a user needs to build their node editor. Features Create nodes, links, and pins in an immediate-mode style. The user controls all the state. Usage private bool imnodesDemoOpen = true; ... ImguiNative.igSetNextWindowSize(new Vector2(500, 500), ImGuiCond.Appearing); ImguiNative.igBegin(\"ImNodes Demo\", this.imnodesDemoOpen.Pointer(), ImGuiWindowFlags.None); string[] nodes = new string[] { \"Node1\", \"Node2\", \"Node3\" }; ImnodesNative.imnodes_BeginNodeEditor(); int id = 0; for (int i = 0; i < nodes.Length; i++) { var node = nodes[i]; ImnodesNative.imnodes_BeginNode(id++); ImnodesNative.imnodes_BeginNodeTitleBar(); ImguiNative.igText(node); ImnodesNative.imnodes_EndNodeTitleBar(); ImnodesNative.imnodes_BeginInputAttribute(id++, ImNodesPinShape.Circle); ImguiNative.igText(\"input\"); ImnodesNative.imnodes_EndInputAttribute(); ImnodesNative.imnodes_BeginOutputAttribute(id++, ImNodesPinShape.Circle); ImguiNative.igIndent(40); ImguiNative.igText(\"output\"); ImnodesNative.imnodes_EndOutputAttribute(); ImnodesNative.imnodes_EndNode(); } ImnodesNative.imnodes_MiniMap(0.25f, ImNodesMiniMapLocation.BottomRight, IntPtr.Zero, IntPtr.Zero); ImnodesNative.imnodes_EndNodeEditor(); ImguiNative.igEnd();"
  },
  "manual/extensions/imgui/implot.html": {
    "href": "manual/extensions/imgui/implot.html",
    "title": "ImPlot | Evergine Doc",
    "keywords": "ImPlot This extension also allow you to use the ImPlot library, this is an immediate mode, GPU accelerated plotting library for ImGUI, this project and source code is available on github at: https://github.com/epezent/implot. Note The C# wrapper is generated on top of a c-api wrapper version of this library, so some samples may not work directly but all the features are supported. Features The list of supported plot are: Line Plots A line plot or chart plot is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments. It is a basic type of chart common in many fields. It is similar to a scatter plot except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. A line plot is often used to visualize a trend in data over intervals of time. Filled Line Plots A Filled Line Plot or area chart displays graphically quantitative data. It is based on the line chart. The area between axis and line are commonly emphasized with colors. Commonly one compares two or more quantities with an area chart. Shaded Plots A Shaded Plot is a type of chart which display graphically the difference between to series based on lines. The area between two lines of the same graph is colored to highlight when they are further apart. Scatter Plots A scatter plot is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. Realtime Plots A Realtime Plot or Realtime chart is a type of chart which display information as a series with data capture in realtime. It is chart common to display fps (Frames per seconds) or other type of measurements in a graphic application. Stairstep Plots A Stairstep plot is a type of chart which represent digital signals that advancing discreetly by jumps or steps in an stairs. Bar Plots A bar plot or bar chart is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent. The bars can be plotted vertically or horizontally. Bar Groups A bar groups or clustered chart is similar to a bar chart but in this case the bars in the same category are drawn together. Bar Stacks A bar stack is based on bar plot but in this case it stacks bars on top of each other so that the height of the resulting stack shows the combined result. Stacked bar charts are not suited to data sets having both positive and negative values. Error Bars A Error bar chart is a type of chart used to visualize series of data and the distance to errors in those data. Stem Plots A Stem plot is a type of chart used to compare two or more stem series in the same chart. It is using vertical lines to highlight the area between the stem series. Infinite Lines A Infinite line chart is used to show a grid and the values are represented by horizontal lines as a constant values. Pie Charts A pie chart (or a circle chart) is a circular statistical graphic, which is divided into slices to illustrate numerical proportion. In a pie chart, the arc length of each slice (and consequently its central angle and area) is proportional to the quantity it represents. While it is named for its resemblance to a pie which has been sliced, there are variations on the way it can be presented. Heatmaps A heat map (or heatmap) is a data visualization technique that shows magnitude of a phenomenon as color in two dimensions. The variation in color may be by intensity, giving obvious visual cues to the reader about how the phenomenon is clustered or varies over space. Histogram A histogram is an approximate representation of the distribution of numerical data. To construct a histogram, the first step is the range of values that divide the entire range of values into a series of intervals and then count how many values fall into each interval. Histogram 2D A Histogram 2D chart is similar to the histogram chart but in this case information from a legend is also displayed that allow you understand the colors displays. Digital Plots A Digital Plot is a type of chart which display digital or analog signal over time. This is useful to show electronic signal inside an application."
  },
  "manual/extensions/imgui/index.html": {
    "href": "manual/extensions/imgui/index.html",
    "title": "ImGUI | Evergine Doc",
    "keywords": "ImGUI Dear ImGui is a bloat-free graphical user interface library for C++. It outputs optimized vertex buffers that you can render anytime in your 3D-pipeline enabled application. It is fast, portable, renderer agnostic and self-contained (no external dependencies). Dear ImGui is designed to enable fast iterations and to empower programmers to create content creation tools and visualization / debug tools (as opposed to UI for the average end-user). It favors simplicity and productivity toward this goal, and lacks certain features normally found in more high-level libraries. In this section Getting started Features ImPlot ImNodes ImGuizmo"
  },
  "manual/extensions/imgui/setup.html": {
    "href": "manual/extensions/imgui/setup.html",
    "title": "Getting Started | Evergine Doc",
    "keywords": "Getting Started Install Evergine.ImGUI extension To start using this extension, just add Evergine.ImGui package to your project using NuGet package manager from Visual Studio. <PackageReference Include=\"Evergine.ImGui\" Version=\"2022.9.28.1\" /> Register the ImGuiManager on your scene: public class ImGuiSceneTest : Scene { public override void RegisterManagers() { base.RegisterManagers(); this.Managers.AddManager(new global::Evergine.Bullet.BulletPhysicManager3D()); this.Managers.AddManager(new ImGuiManager() { ImGuizmoEnabled = true, ImPlotEnabled = true, ImNodesEnabled = true, }); } ... } And add the namespace ImGuiNET: using ImGuiNET; Now you can start calling the ImGui API from everywhere using the static reference: private bool imguiDemoOpen; ... ImguiNative.igShowDemoWindow(this.imguiDemoOpen.Pointer()); Usage The UI is generated every frame and all the controls between Begin and End will be drawn. Here you have a simple example: bool open = true; ImguiNative.igBegin(\"Debug\", open.Pointer(), ImGuiWindowFlags.None); ImguiNative.igText(\"Hello, world 123\"); if (ImguiNative.igButton(\"Save\", Vector2.Zero)) { // MySaveFunction(); } float f = 0.5f; ImguiNative.igSliderFloat(\"float\", &f, 0.0f, 1.0f, null, ImGuiSliderFlags.None); ImguiNative.igEnd(); And this is the result: And now will see a more advanced example: bool open = false; ImguiNative.igBegin(\"Path Tracing\", open.Pointer(), ImGuiWindowFlags.None); float x = this.worldInfo.LightPosition.X; float y = this.worldInfo.LightPosition.Y; float z = this.worldInfo.LightPosition.Z; ImguiNative.igSliderFloat(\"Camera Pos X\", &x, -10.0f, 10.0f, null, ImGuiSliderFlags.None); ImguiNative.igSliderFloat(\"Camera Pos Y\", &y, -10.0f, 10.0f, null, ImGuiSliderFlags.None); ImguiNative.igSliderFloat(\"Camera Pos Z\", &z, -10.0f, 10.0f, null, ImGuiSliderFlags.None); this.worldInfo.LightPosition.X = x; this.worldInfo.LightPosition.Y = y; this.worldInfo.LightPosition.Z = z; float lightRadius = this.worldInfo.LightRadius; ImguiNative.igSliderFloat(\"Light Radius\", &lightRadius, 0.0f, 0.2f, null, ImGuiSliderFlags.None); this.worldInfo.LightRadius = lightRadius; int numRays = this.worldInfo.NumRays; ImguiNative.igSliderInt(\"AO Num Rays\", &numRays, 0, 32, null, ImGuiSliderFlags.None); this.worldInfo.NumRays = numRays; float aoRadius = this.worldInfo.AORadius; ImguiNative.igSliderFloat(\"AO Radius\", &aoRadius, 0.0f, 2.0f, null, ImGuiSliderFlags.None); this.worldInfo.AORadius = aoRadius; int numBounces = this.worldInfo.NumBounces; ImguiNative.igSliderInt(\"GI Num Bounces\", &numBounces, 0, 3, null, ImGuiSliderFlags.None); this.worldInfo.NumBounces = numBounces; float reflectanceCoef = this.worldInfo.ReflectanceCoef; ImguiNative.igSliderFloat(\"Reflectance Coef\", &reflectanceCoef, 0, 1, null, ImGuiSliderFlags.None); this.worldInfo.ReflectanceCoef = reflectanceCoef; float roughness = this.worldInfo.Roughness; ImguiNative.igSliderFloat(\"Roughness\", &roughness, 0,1, null, ImGuiSliderFlags.None); this.worldInfo.Roughness = roughness; ImguiNative.igSpacing(); ImguiNative.igSeparator(); ImguiNative.igSpacing(); int numSamples = this.pathTracerNumSamples; ImguiNative.igSliderInt(\"Num Samples\", &numSamples, 0, 1024, null, ImGuiSliderFlags.None); this.pathTracerNumSamples = numSamples; ImguiNative.igProgressBar((float)this.pathTracerSampleIndex / (float)this.pathTracerNumSamples, Vector2.Zero, null); ImguiNative.igEnd(); This is the result: Take a look this in our path tracer demo. Note: This project need to create a ImGuiRenderer because is using the Evergine low level api, but this is not necessary in a default project created from the Evergine Studio."
  },
  "manual/extensions/index.html": {
    "href": "manual/extensions/index.html",
    "title": "Extensions | Evergine Doc",
    "keywords": "Extensions Evergine core packages comes with a lot of useful features, including a lot of Services, Managers, Components in the Evergine.Framework and Evergine.Components packages. Apart from this, Evergine gives several Extensions that provide integration and functionality with different technologies or libraries, but are too specific to be part of the main Evergine packages. In this section ARMobile Networking ImGUI"
  },
  "manual/extensions/networking.html": {
    "href": "manual/extensions/networking.html",
    "title": "Networking | Evergine Doc",
    "keywords": "Networking With networking extension you will be able to establish a client-server communication channel along any processes running Evergine. Those processes could live in the same computer or in different devices connected to a local network. This extension relies in a fork of Lidgren.Network library, that will be in charge of in-process communication using sockets, with server-client and client-client messages delivering. Install Evergine.Networking extension To start using this extension, just add Evergine.Networking package to your project using NuGet package manager from Visual Studio. <PackageReference Include=\"Evergine.Networking\" Version=\"2021.11.17.2-preview\" /> Server configuration To create a networking server, you should make use of built-in service named MatchmakingServerService. This service is part of the extension and manages server rooms, players and provides different events that you can use to be notified when a player (client) connects or disconnects from the server, joins or leaves a room, etc. For a complete list of events, take a look to MatchmakingServerService API documentation. Just register MatchmakingServerService in Evergine dependencies container to start using it. this.Container.RegisterInstance(new MatchmakingServerService()); MatchmakingServerService has a set of properties that you should use to configure your server settings. Some considerations here: It's recommended to not use big values for PingInterval and ConnectionTimeout values. Also, ConnectionTimeout may be a multiple of PingInterval. Values of 4 and 8 seconds, respectively, would be fine, depending on your requirements. Set a value for ApplicationIdentifier that you could easily identify. For example, your application name. Set a version value for ClientApplicationVersion. this.matchmakingServerService.PingInterval = 4; this.matchmakingServerService.ConnectionTimeout = 8; this.matchmakingServerService.ApplicationIdentifier = \"MyApp\"; this.matchmakingServerService.ClientApplicationVersion = \"1.0.0\"; this.matchmakingServerService.ServerName = \"MyServer\"; Note An exception for setting bigger values for ConnectionTimeout would be debugging. When a break point is hit, time will be still running and you could find clients getting disconnected. To avoid problems while developing, you can set ConnectionTimeout to a temporary value of TimeSpan.FromHours(1), for example, to have enough time while debugging. Start a server Once you have your server configured it's time to start it, to make it able to receive incoming client connections. int port = 12345; await this.matchmakingServerService.StartAsync(port); Our recommendation is to create a custom Service or component to contain all server initialization code, and also make use of MatchmakingServerService to log events, very useful for debugging purposes. this.matchmakingServerService.PlayerConnected += this.MatchmakingServerService_PlayerConnected; this.matchmakingServerService.PlayerDisconnected += this.MatchmakingServerService_PlayerDisconnected; this.matchmakingServerService.PlayerJoining += this.MatchmakingServerService_PlayerJoining; this.matchmakingServerService.PlayerJoined += this.MatchmakingServerService_PlayerJoined; this.matchmakingServerService.PlayerLeaving += this.MatchmakingServerService_PlayerLeaving; this.matchmakingServerService.PlayerLeft += this.MatchmakingServerService_PlayerLeft; this.matchmakingServerService.RoomCreated += this.MatchmakingServerService_RoomCreated; this.matchmakingServerService.RoomDestroyed += this.MatchmakingServerService_RoomDestroyed; // ... private async void MatchmakingServerService_PlayerJoined(object sender, ServerPlayer e) { var numberOfClients = this.matchmakingServerService.AllConnectedPlayers.Count(); Debug.WriteLine($\"A client just joined: {e.Nickname}. There are {numberOfClients} clients\"); } Clients configuration In case of clients, built-in service is MatchmakingClientService. As we did for server side, you should register this service in dependencies container. For client settings, check that you apply same values as those provided for server side. this.Container.RegisterInstance(new MatchmakingClientService()); // ... this.matchmakingClientService.PingInterval = 4; this.matchmakingClientService.ConnectionTimeout = 8; this.matchmakingClientService.ApplicationIdentifier = \"MyApp\"; this.matchmakingClientService.ClientApplicationVersion = \"1.0.0\"; Note When creating a server, you would probably also want register your own process as client of that server. This is, use MatchmakingClientService to connect to the server that you have already created, as a common scenario where one of the clients acts as server at the same time. Server discovery Networking extension provides a way of automatic discovery of servers, always that local network infraestructure has broadcast traffic available. To discover a server, make use of following line, using the same port that you configured for your server. Once a server is discovered, ServerDiscovered event will be raised. this.matchmakingClientService.ServerDiscovered += this.MatchmakingClientService_ServerDiscovered; // ... int port = 12345; this.matchmakingClientService.DiscoverServers(port); // ... private async void MatchmakingClientService_ServerDiscovered(object sender, HostDiscoveredEventArgs e) { var connected = await this.matchmakingClientService.ConnectAsync(e.Host); Debug.WriteLine($\"Connected to server {e.ServerName}.\"); } Note You must ensure your server can be reached by clients in the network. Some platforms, like UWP would require extra configuration at application level for security reasons. In case of this platform, capabilities and, depending on your scenario, even review the application isolation. Joining to a server room After connecting to a server, the next step is joining a room. Clients in the same room can share information to update their states, depending on your application requirements. var roomOptions = new RoomOptions() { RoomName = \"MyRoom\", }; var joinResult = await this.matchmakingClientService.JoinOrCreateRoomAsync(roomOptions); Once you have joined to a room, MatchmakingClientService will update its CurrentRoom property with room information, including the list of connected clients (players). You can use this information, for example, to display list of room partipants in your client application. Messages delivery With an existing client-server connection, you can now send information in both directions, or even to other client applications within the network. There are two mechanisms to do this. With first one, messages, you are free to create a network message by your own, writing scalar fields in a buffer. The second one are the network properties, and they are very useful as their values will be automatically synchronized for all connected clients. Messages Both MatchmakingServerService and MatchmakingClientService provide some methods to send messages to different peers in the network. For MatchmakingServerService you have: SendToClient: sends a message to a destination client. For MatchmakingClientService you have: SendToCurrentRoom: sends a message to all clients connected to the room. SendToPlayer: sends a message to a single player in the room. SendToServer: sends a message from a client to the server. For example, we can send a \"hello\" message from a client to another with something line the following. // client A sends hello message var message = this.matchmakingClientService.CreateMessage(); message.Write(\"hello!\"); this.matchmakingClientService.SendToPlayer(message, player, DeliveryMethod.ReliableOrdered); // client B listens for incoming messages this.matchmakingClientService.MessageReceivedFromPlayer += this.Client_MessageReceivedFromPlayer; private void Client_MessageReceivedFromPlayer(object sender, MessageFromPlayerEventArgs e) { var message = e.ReceivedMessage.ReadString(); Debug.WriteLine($\"Received '{message}'\"); } Network properties Network properties are a set of components provided by Evergine.Networking extension. There are two ways properties can be stored and shared for members inside a room: room properties, that will be shared at room level; and player (client) properties, that will be attached to clients connected to a room. As they are components, you can add it to any of the entities of your scene. Network properties require the existence of a properties provider for component owner or its ascendants. Depending of the type of property you want to use, you need to ensure that a component of type NetworkRoomProvider or NetworkPlayerProvider is placed somewhere in entity hierarchical path. This can be done manually using Evergine Studio and adding the component manually, or doing it programatically. Network properties maintain an internal table of key-values to store information. There is also a size limitation for this tables: table key type is byte, so you can't have more than 256 properties for a room or for each single client (player). In Evergine.Networking.Components namespace you can find a set of built-in components to work with network properties of more common scalar data types and structs like numbers, strings, vectors, etc. Properties synchronization For example, imagine that we want to have an object in our scene that can be manipulated by one of the clients (move, scale and rotate it). We also want all the clients connected to the room to see those transformations. In this case, the best option is to use a network property to synchronize entity transform. Below you will find a block of code to see how it works, but lets see a few notes before: It's recommended to have a centralized enum to have an easier control of what room properties are already in use, and avoid using same key for different synchronization properties. We have used a NetworkMatrix4x4PropertySync component because we want to synchronize transform information, but there are many other built-in properties. In this case, we are using a room provider, but remember that you could also have specific properties for clients connected to the room. In that case, you should use NetworkPropertyProviderFilter.Player. Method OnPropertyReadyToSet will be invoked once internal key-value table is ready to be synchronized. This will also change IsReady property to a value of true. You should always check this property before trying to set a network property value. Method OnPropertyRemoved will be invoked if property is removed from shared table. Method OnPropertyAddedOrChanged will be invoked once property is ready or someone within the room has changed its value. To update a property value, just need to set PropertyValue value, as stated in UpdatePropertyValue method. public enum RoomProperties : byte { MyObjectTransform = 0x00, } public class SyncLocalTransform : NetworkMatrix4x4PropertySync<RoomProperties> { [BindComponent(source: BindComponentSource.Owner)] private Transform3D transform3d = null; public SyncLocalTransform() { this.ProviderFilter = NetworkPropertyProviderFilter.Room; this.PropertyKey = RoomProperties.MyObjectTransform; } // We should determine somehow if current client can manipulate the object or // not, as only one at the same time should do it. public bool CanManipulate { get; set; } protected override bool OnAttached() { this.transform3d = this.Owner.FindComponent<Transform3D>(); this.transform3d.LocalTransformChanged += this.Transform3D_LocalTransformChanged; return base.OnAttached(); } protected override void OnDetach() { this.transform3d.LocalTransformChanged -= this.Transform3D_LocalTransformChanged; base.OnDetach(); } protected override void OnPropertyAddedOrChanged() { if (!this.CanManipulate) { this.transform3d.LocalPosition = this.PropertyValue.Translation; this.transform3d.LocalScale = this.PropertyValue.Scale; this.transform3d.LocalRotation = this.PropertyValue.Rotation; } } protected override void OnPropertyRemoved() { } protected override void OnPropertyReadyToSet() { base.OnPropertyReadyToSet(); this.UpdatePropertyValue(); } private void Transform3D_LocalTransformChanged(object sender, EventArgs e) => this.UpdatePropertyValue(); private void UpdatePropertyValue() { if (this.IsReady && this.CanManipulate) { this.PropertyValue = this.transform3d.LocalTransform; } } } Synchronization of complex properties Evergine.Networking extension provides properties for more common scalars and structs but, what if we want to synchronize data of custom class. We can do it, but always keep in mind that is not recommendable to have big objects saved in internal key-value tables, as this will affect to memory consumption and network traffic bandwidth. In any case, we should implement some methods of INetworkSerializable to achieve this. For this example, imagine that our application has a map and we want to control the center and zoom level of that map. INetworkSerializable requires you to implement two methods, one for data serialization and other for data deserialization. Only thing that you need to remember is that you should read data in the same order as you wrote it. public class MapInfo : INetworkSerializable { public Coordinates Center { get; set; } public short ZoomLevel { get; set; } public void Write(NetBuffer buffer) { buffer.Write(this.Center.Latitude); buffer.Write(this.Center.Longitude); buffer.Write(this.ZoomLevel); } public void Read(NetBuffer buffer) { var lat = buffer.ReadDouble(); var lng = buffer.ReadDouble(); this.Center = new Coordinates(lat, lng); this.ZoomLevel = buffer.ReadInt16(); } public struct Coordinates { public Coordinates(double latitude, double longitude) { this.Latitude = latitude; this.Longitude = longitude; } public double Latitude { get; } public double Longitude { get; } } } public class SyncMapInfo : NetworkSerializablePropertySync<RoomProperties, MapInfo> { public SyncMapInfo() { this.ProviderFilter = NetworkPropertyProviderFilter.Room; this.PropertyKey = RoomProperties.MapInfo; } // ... protected override void OnPropertyAddedOrChanged() { if (this.IsReady) { MapInfo info = this.PropertyValue; // Do something with this, like updating the map } } }"
  },
  "manual/extensions/openxr.html": {
    "href": "manual/extensions/openxr.html",
    "title": "OpenXR | Evergine Doc",
    "keywords": "OpenXR Coming soon"
  },
  "manual/get_started/index.html": {
    "href": "manual/get_started/index.html",
    "title": "Get started | Evergine Doc",
    "keywords": "Get started Welcome to the Official Evergine documentation website. Here you will find everything you need to start creating 3D application from scratch. In this section Install Evergine Create an Evergine Project Open your project in Visual Studio Manage Evergine Versions Samples, Learning and Support"
  },
  "manual/get_started/install.html": {
    "href": "manual/get_started/install.html",
    "title": "Install Evergine | Evergine Doc",
    "keywords": "Install Evergine To get Evergine, please, take the following steps: Download the Evergine installer (EvergineSetup.exe) from the Evergine download page. After downloading the Evergine installer, start the installer and follow the installation wizard steps. In order to install Evergine you will be asked to accept the EULA that defines the engine licensing terms: The installer will install some dependencies required to execute Evergine Studio, like DirectX tools. Note Please, note that some dependencies require admin rights and this will be required during installation process. Once installation process is completed, you will be ready to launch Evergine with the Evergine Launcher. Evergine Launcher After the installer closes, the Evergine Launcher opens. Evergine Launcher is a standalone windows app that allows you to install Evergine versions, create and manage your Evergine Projects and find useful resources like samples, access to learning and support Evergine section. Next steps First thing you can do is creating a project with Evergine Launcher."
  },
  "manual/get_started/open_in_vs.html": {
    "href": "manual/get_started/open_in_vs.html",
    "title": "Open in Visual Studio | Evergine Doc",
    "keywords": "Open in Visual Studio Once your project is created to open it on Visual Studio you just have to go File -> Open C# Editor. A new Visual Studio instance will be launched to show the project solution, from here you can build and debug the application. You will find a solution file ready to open on Visual Studio for each configured platform template. By default the solution will contains 3 project: The application source code: This contains the application source code common cross-platform. The editor extensions code: This contains the component specific editor extensions. The platform specific launcher code: This contains the code needed to launch the application in an specific platform like Windows using a specific API like DirectX 11. You can read the Project Structure for further details."
  },
  "manual/graphics/billboard/create_billboard.html": {
    "href": "manual/graphics/billboard/create_billboard.html",
    "title": "Create Billboard | Evergine Doc",
    "keywords": "Create Billboard Billboard allows simulating far objects like bushes or trees reducing the amount of geometry needed to render your scene. Create a Billboard in Evergine Studio You can create a billboard click button on from Entity Hierarchy panel to deploy a create menu options and click on the option \"Billboard\" A billboard entity will be added to your scene In the billboard component of your billboard entity you will find the following properties: Property Description Texture The billboard texture. TintColor Each pixel of the Billboard will be multiplied by such color during the drawing. By default, it is white. Origin Gets or sets the Billboard Origin. The origin (also known as pivot) from where the entity scales, rotates and translates. Its values are included in [0, 1] where (0, 0) indicates the top left corner. Such values are percentages where 1 means the 100% of the rectangle's width/height. Rotation The Billboard rotation. BillboardType The billboard type. The available types are Point orientation or Axial orientation. FlipMode Allows flipping the texture coords in Horizontal or Vertical. Create a Billboard from code The following code shows the list of components necessary to convert an entity into a billboard entity. public class MyScene : Scene { protected override void CreateScene() { var assetsService = Application.Current.Container.Resolve<AssetsService>(); // Load default texture Texture treeTexture = assetsService.Load<Texture>(EvergineContent.Textures.BillboardTree_png); // Load default sampler SamplerState linearClampSampler = assetsService.Load<SamplerState>(EvergineContent.Samplers.LinearClampSampler); // Load a Render Layer description... RenderLayerDescription layer = assetsService.Load<RenderLayerDescription>(EvergineContent.RenderLayers.Alpha); var billboard = new Entity() .AddComponent(new Transform3D()) .AddComponent(new Billboard() { Texture = treeTexture, Sampler = linearClampSampler, BillboardType = BillboardType.Axial_Orientation, }) .AddComponent(new BillboardRenderer() { Layer = layer, }); this.Managers.EntityManager.Add(billboard); } } The result:"
  },
  "manual/graphics/billboard/index.html": {
    "href": "manual/graphics/billboard/index.html",
    "title": "Billboard | Evergine Doc",
    "keywords": "Billboard Billboard is a quad orientated to face the camera. While the active camera in your scene is moving the billboard is orientated to face the camera. The billboards are useful to create indicators or making an impostor of your far mesh to reduce the geometry that the render needs to draw. A common use is to simulate far bushes or trees. Types of billboards Point Orientation The billboard is oriented about his origin to always face the camera. With this type of billboarding, the object will always appear the same to the camera, however, it will be affected by perspective. Axial orientation The billboard is rotated about an axis to face towards the camera. In this section The following sections show how to create and use billboards in your scene. Create Billboard"
  },
  "manual/graphics/cameras.html": {
    "href": "manual/graphics/cameras.html",
    "title": "Cameras | Evergine Doc",
    "keywords": "Cameras Cameras are responsible to capture your scene and display it to the user. By customizing and manipulating cameras, you can make the visual composition of your scene truly appealing. You can create an unlimited number of cameras in a Scene. They can be set to render in any order, at any place on the screen, and choose the render target destination of this camera. Create a Camera3D from code The following sample code can be used to instantiate a new camera entity in a scene. protected override void CreateScene() { // Create a new camera entity. Entity cameraEntity = new Entity() .AddComponent(new Transform3D()) .AddComponent(new Camera3D() { BackgroundColor = Color.CornflowerBlue, }); // Add the camera entity to the entity manager. this.Managers.EntityManager.Add(cameraEntity); } Create a Camera3D in Evergine Studio In the Entities Hierarchy panel of your Scene Editor, click the \"Add Entity and select Camera3D, then choose the kind of camera you want to create: Fixed Camera: This camera does not have any built-in behaviour, it is static. View Camera: This camera can be moved using the mouse, touch or keyboard while respecting the look-at point. Free Camera: This camera can be moved using the mouse, touch or keyboard. Camera3D properties Basic Camera3D properties Property Description Field of View The Camera’s view angle, measured in degrees along the axis specified in the Field of View Axis drop-down. Field of View Axis Field of view axis: Vertical: The camera uses a vertical field of view axis. Horizontal: The camera uses a horizontal field of view axis. Near Plane The nearest distance the camera can see. Far Plane The furthest distance the camera can see. Background Color The color applied to the background. Clear Flags This flags indicates wich part of the framebuffer will be cleared before rendering: Target: Clear the color buffer attachments. Depth: Clear the depth buffer attachment. Stencil: Clear the stencil buffer attachment. All: Clear all attachments described before. HDR Enabled Render the camera output in a HDR format Camera Order Specify the order in which the camera will be rendered. Lower values produces that the camera will be rendered first. Frustum The camera frustum is the region of the space that will be appear on the screen. Is defined by near, far planes and field of view properties. The near and far planes determine where the camera's view begins and ends. The near plane is the closest point the camera can see. The default value is 0.1. Objects before this point aren't drawn. The far plane, also known as the draw distance, is the furthest point the camera can see. Objects beyond this point aren't drawn. The default setting is 1000. Photometric properties By default, the camera uses basic properties to specify camera views (field of view and exposure). However, is possible to specify these values using physical values used in real cameras. To enable physical parameters: Property Description Enable Physical Parameters Boolean to indicate if the camera will use the physical parameters to define its field of view. Focal Length and sensor size Property Description Focal Length (milimeters) The Focal length is a common terms in photography to describe the field of view. Sensor Size (milimeters) The Sensor size describes the size in milimeters of the camera sensor. It has several implications in combination with other properties. For example, Sensor Size and Focal length defines the camera field of view. Exposure The Exposure property specifies the overal factor that will be applied to the render output. In combination with HDR render output and environments will produce realistic results: Exposure = 0.2 Exposure = 1.0 Exposure = 3.0 The exposure can be specified using the Exposure property, but if you use photometric camera properties you could reproduce physical behavior concerning the amount of light gathered by the camera: Property Description Aperture (f-stops) The Aperture, expressed in f-stops, controls how open or closed the camera system's aperture is. In addition to the exposition, the aperture setting controls the depth of field. Shutter speed (Seconds) The Shutter speed, expressed in seconds, controls how long the aperture remains opened. In addition to the exposition, the shutter speed controls motion blur. Sensitivity (ISO) The Focal length, expressed in ISO, controls how the light reaching the sensor is quantized. In addition to the exposition, the sensitivity setting controls the amount of noise. Compensation (EV units) The Compensation, Exposure Compensation or EC is expressed in EV units. Applying an exposure compensation EC is a simple as adding an offset to the final exposure. Tip Exposure of 1 could be achieved using Aperture 1 f-stop, Shutter Speed of 1.2 seconds and Sensitivity of 100 ISO Camera render output By default, the camera render output will be targeted to the default Display registered in the GraphicPresenter service. This behavior could be modified using two properties: Property Description DisplayTag It controls wich Display will be used to output the render. Each display is registered into the GraphicPresenter using a DisplayTag. Setting this property will be used to specify the camera output to the framebuffer defined in this display. Framebuffer However, you can override this behavior by setting a Framebuffer instance. If you are doing that, the camera output will be targeted to this framebuffer instance, even if you have previously specified a DisplayTag."
  },
  "manual/graphics/compute_tasks/create_computetasks.html": {
    "href": "manual/graphics/compute_tasks/create_computetasks.html",
    "title": "Create Compute Tasks | Evergine Doc",
    "keywords": "Create Compute Tasks Compute Tasks allows run tasks on GPU. Compute tasks are associated with a compute effect. It is very useful to improve the performance of a hard task that runs slow on the CPU. Compute Effect Before creating a Compute task you need to create a compute effect from the Assets Details panel and coding the task on HLSL language. Example This is an example of computing task. In that case the compute applies a grayscale filter to the input texture and stores the result into an output texture. In Create Effects you will find the structure of this code. [Begin_ResourceLayout] Texture2D Input : register(t0); RWTexture2D<float4> Output : register(u0); [End_ResourceLayout] [Begin_Pass:Grayscale] [Profile 11_0] [Entrypoints CS = CS] [numthreads(8, 8, 1)] void CS(uint3 threadID : SV_DispatchThreadID) { float4 color = Input.Load(float3(threadID.xy, 0)); color.rgb = color.r * 0.3 + color.g * 0.59 + color.b * 0.11; Output[threadID.xy] = color; } [End_Pass] ComputeTask Decorator To use a compute task from code you need a compute effect and the compute task decorator associated. You can generate the compute task decorator from Effect Editor. Create a new ComputeTask from code The following sample code can be used to create a new computeTask and run it into your scene. The example assumes that you have a compute effect GPUFilter and its compute task decorator created. protected override void CreateScene() { var graphicsContext = Application.Current.Container.Resolve<GraphicsContext>(); var assetsService = Application.Current.Container.Resolve<AssetsService>(); // Load input texture Texture inputTexture = assetsService.Load<Texture>(EvergineContent.Textures.lena_png); uint width = inputTexture.Description.Width; uint height = inputTexture.Description.Height; // Create output texture var outputTextureDesc = new TextureDescription() { Type = TextureType.Texture2D, Usage = ResourceUsage.Default, Flags = TextureFlags.UnorderedAccess | TextureFlags.ShaderResource, Format = PixelFormat.R8G8B8A8_UNorm, Width = width, Height = height, Depth = 1, MipLevels = 1, ArraySize = 1, Faces = 1, CpuAccess = ResourceCpuAccess.None, SampleCount = TextureSampleCount.None, }; Texture outputTexture = graphicsContext.Factory.CreateTexture(ref outputTextureDesc); // Load compute effect Effect computeEffect = assetsService.Load<Effect>(EvergineContent.Effects.GPUFilter); // Create compute task decorator GPUFilter task = new GPUFilter(computeEffect); task.Input = inputTexture; task.Output = outputTexture; task.Run2D(width, height, pass: \"Grayscale\"); // Load the Material and apply output texture. Material material = assetsService.Load<Material>(EvergineContent.Materials.DefaultMaterial); StandardMaterial standardMaterial = new StandardMaterial(material); standardMaterial.BaseColorTexture = outputTexture; // Apply to an entity Entity primitive = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new CubeMesh()) .AddComponent(new Spinner() { AxisIncrease = new Vector3(0.1f, 0.2f, 0.3f) }) .AddComponent(new MeshRenderer()); this.Managers.EntityManager.Add(primitive); } The result of the above code is:"
  },
  "manual/graphics/compute_tasks/index.html": {
    "href": "manual/graphics/compute_tasks/index.html",
    "title": "Compute Tasks | Evergine Doc",
    "keywords": "Compute Tasks Compute Tasks are GPU programs that don't use classical GPU pipeline such as vertex transformations, fragment shading or geometry programs. he are fully configurable code that can run on a GPU in async. They can be used for massively parallel computational algorithms or to accelerate parts of application rendering. Compute tasks need to be associated to a Compute Effect. The following sections show how to create and use compute tasks in your projects. In this section Create Compute Task Using Compute Task"
  },
  "manual/graphics/compute_tasks/using_computetasks.html": {
    "href": "manual/graphics/compute_tasks/using_computetasks.html",
    "title": "Using Compute Tasks | Evergine Doc",
    "keywords": "Using Compute Tasks Compute tasks are very similar to Materials, they are an associated effect too and you also can generate a class decorator to using from code. The main difference is that the compute tasks only can be used from code. Compute Task decorator are generated from Effect Editor like Material decorators. The compute task decorator helps you set the compute effect resource layout. As input resources, you can use ConstantBuffers, StructureBuffer, Textures, and Samplers. This is an example of a compute task decorator with an input texture and output texture define in its resource layout block: //------------------------------------------------------------------------------ // <auto-generated> // This code was generated by a tool. // // Changes to this file may cause incorrect behavior and will be lost if // the code is regenerated. // </auto-generated> //------------------------------------------------------------------------------ namespace DocumentationWorkBench.Effects { using Evergine.Common.Graphics; using Evergine.Framework.Graphics; using Evergine.Framework.Graphics.Effects; using Evergine.Mathematics; [Evergine.Framework.Graphics.MaterialDecoratorAttribute(\"6ba492de-d165-4491-b5b0-ae72dc202577\")] public partial class GPUFilter : Evergine.Framework.Graphics.ComputeTaskDecorator { public GPUFilter(Evergine.Framework.Graphics.Effects.Effect effect) : base(new ComputeTask(effect)) { } public GPUFilter(Evergine.Framework.Graphics.ComputeTask computeTask) : base(computeTask) { } public Evergine.Common.Graphics.Texture Input { get { return this.material.TextureSlots[0].Texture; } set { this.material.SetTexture(value, 0); } } public Evergine.Common.Graphics.Texture Output { get { return this.material.UABuffers[0].UATexture; } set { this.material.SetUATexture(value, 0); } } } } To run a compute task you must call to Run methods and exists the following flavors: Method Description Run Defines the groupcount X,Y and Z and the pass selected. Run1D Helper method to run 1D tasks, only need to pass threadCount X the groupSizes are defined as (64,1,1). Run2D Helper method to run 2D tasks, only need to pass ThreadCount X and Y, the groupSizes are defined as (8,8,1). Run3D Helper method to run 3D tasks, is similar to run but you can define ThreadCounts X,Y,Z and GroupSizes X,Y,Z."
  },
  "manual/graphics/effects/builtin_effects.html": {
    "href": "manual/graphics/effects/builtin_effects.html",
    "title": "Built-in Effects | Evergine Doc",
    "keywords": "Built-in Effects In this section are described the effects included in Evergine.Core package Standard Effect Property Description Lighting enabled Indicates that the material interacts with the lights of the scene. In the following image the left material has disabled ligth and the right material has it enabled IBL enabled Indicates that the material reflects the enviroment of the scene. In the following image the left material has disabled it while the right material has it enabled Base Color Indicates the surface color. Alpha Indicates the transparent of the surface. Note. This parameter requires set the LayerDescription property to Alpha or a layer with blend mode enable Vertex Color Indicates the surface color will be read from vertex data of the mesh. Base Color Texture Albedo texture to define the color of the surface. Base Color Sampler Albedo texture sampler used by Base Color Texture. UVOffsets 0 Texcoord UV offset added to the vertex texcoord data of the mesh. Note. You can create interesting material effect only animating this parameter. Metallic Indicates how \"metal-like\" the surface is. Its value is between [0-1]. In the following image you can see the result with different values. Roughness Defines the smoothness or roughness of the surface. The value is between 0.0 and 1.0 Reflectance Fresnel reflectance at normal incidence for dieletrice surface. This replaces an explicit index of refraction. Reference Alpha Threshold alpha value. Alpha Cutout Discard pixels when alpha value is less than the specified value. AllowInstancing Indicates whether this material allows instancing draw. OrderBias This value is used to modify the rendering order of the meshes LayerDescription Defines the RenderLayer used by the material. Metal Roughness Property Description MetalRoughness Texture This texture allows to define multiples roughness and metallic values to the surface area instead a constant roughness/metallic value for all surface. Note. the Metallic value is (blue channel) and Roughness is (green channel). MetalRoughness Sampler Sampler used by the MetalRoughness texture. Normal Property Description Normal Texture This texture allows to simulate more details without using more polygons. Normal Smapler Sampler used by the Normal texture. Ambient occlusion Property Description Occlusion Texture Defines how much of the ambient light is accessible to a surface point. It is a per-pixel shadowing factor between 0.0 and 1.0. Occlusion Sampler Sampler used by the Occlusion Texture. Emissive Property Description EmissiveColor Additional diffuse color to simulate emissive surfaces (such as neons, etc.) This parameter is mostly useful in an HDR pipeline with a bloom pass. Emissive Compensation The exposure compensation value of the emissive property can be used to force the emissive color to be brighter (positive values) or darker (negative values) than the current exposure. Emissive Texture Additional diffuse albedo to simulate emissive surfaces (such as neons, etc.) This parameter is mostly useful in an HDR pipeline with a bloom postprocessing effect. Emissive Sampler Sampler used by the Emissive texture. Clear Coat Property Description ClearCoat Strength of the clear coat layer. Scalar between 0 and 1 ClearCoat Roughness Perceived smoothness or roughness of the clear coat layer. Scalar between 0 and 1. ClearCoat Normal Texture Normal map texture that affects to the clear coat layer. ClearCoat Normal Sampler Sampler used by the Clear Coat Normal texture. Dual Texture Property Description Dual Texture Blend Blend mode between Base Color and 2nd Base Color. Availables values: Lightmap, Multiplicative, Additive, Mask. UV Offsets 1 UV Coord offset apply to the 2nd UV coords on the vertex format. 2nd Base Color Texture Texture used with the 2nd UV coords set on the vertex format. 2nd Base Color Sampler Sampler used by the 2nd Base Color texture Distortion Effect This effect requires to use Distortion effect inside of Default postprocessing effect) Property Description RenderLayer Defines the RenderLayer used by the material. Intensity Define the intensity of the distortion. Distortion Texture The texture used with the distortion Sampler Sampler used by the Distortion texture."
  },
  "manual/graphics/effects/create_effects.html": {
    "href": "manual/graphics/effects/create_effects.html",
    "title": "Create Effects | Evergine Doc",
    "keywords": "Create Effects An effect is a uber-shader so be able to represent a single shader or a large group of shaders. There are two type of effect in Evergine: Effect type Description Graphics Effect Defines a rasterization pipeline Vertex Shader, Geometry Shader, Hull Shader, Domain shader, Pixel Shader and are useful to create materials. Compute Effect Defines a compute pipeline with Compute Shader and are useful to create compute task and post-processing graph nodes. Create a Effect asset in Evergine Studio You can create an effect click button on from Assets Details panel to deploy a create menu options and click on the option \"Create effect->Graphics Effect or Compute Effect\" Inspect effects in Asset Details You can find the effect assets in the Assets Details panel when you select a folder in the Project Explorer. Effects files in content directory The effect file has the .wefx extension and always comes together with a folder with the same name of the effect. This folder contains the source code: Effect source code example While Effects in Evergine uses HLSL as shading language, it is empowered by using differents Metatags to automatize some tasks and help users. A tipical effect code looks like this: [Begin_ResourceLayout] [Directives:UseTexture TEX_OFF TEX] cbuffer PerDrawCall : register(b0) { float4x4 WorldViewProj : packoffset(c0); [WorldViewProjection] }; cbuffer Parameters : register(b1) { float3 Color : packoffset(c0); [Default(1, 1, 1)] }; Texture2D ColorTexture : register(t0); SamplerState ColorSampler : register(s0); [End_ResourceLayout] [Begin_Pass:Default] [Profile 10_0] [Entrypoints VS=VertexShaderCode PS=PixelShaderCode] struct VS_IN { float4 Position : POSITION; #if TEX float2 TexCoord : TEXCOORD; #endif }; struct PS_IN { float4 Pos : SV_POSITION; #if TEX float2 Tex : TEXCOORD; #endif }; PS_IN VertexShaderCode(VS_IN input) { PS_IN output = (PS_IN)0; output.Pos = mul(input.Position, WorldViewProj); #if TEX output.Tex = input.TexCoord; #endif return output; } float4 PixelShaderCode(PS_IN input) : SV_Target { float4 color = float4(Color,1); #if TEX color *= ColorTexture.Sample(ColorSampler, input.Tex); #endif return color; } [End_Pass] An effect file in Evergine is divided into the following sections: Resource Layout definition List of Passes Resource Layout definition This block of code defines all resources (Constant Buffers, Structured Buffers, Textures and Samplers) that will be used into your shaders. This section is enclosed between [Begin_ResourceLayout] and [End_ResourceLayout] tags. [Begin_ResourceLayout] [Directives:UseTexture TEX_OFF TEX] cbuffer PerDrawCall : register(b0) { float4x4 WorldViewProj : packoffset(c0); [WorldViewProjection] }; cbuffer Parameters : register(b1) { float3 Color : packoffset(c0); [Default(1, 1, 1)] }; Texture2D ColorTexture : register(t0); SamplerState ColorSampler : register(s0); [End_ResourceLayout] In this example, : [Directives:UseTexture TEX_OFF TEX]: This section contains a Directive (Called UseTexture in this example), which allows the users to enable different features in your effect. This directive specifies two macros (TEX_OFF and TEX) which indicates if this shader will use a color texture or not. In your effect code you are free to define any number of directives as you can. The tradeof is that the number of possible effect combinations rise exponentially in proportion of the number of directives. You can enable or disable features using macros into your shader with the #if #else and #endif preprocessor directives. The definition of two constant buffers, a Texture2D and a SamplerState: cbuffer PerDrawCall : register(b0) { ... }: A constant buffer. cbuffer Parameters : register(b1) { ... }: A second constant buffer. Texture2D ColorTexture : register(t0);: A texture 2D. SamplerState ColorSampler : register(s0);: A Sampler state. You would notice that you can add metatags to your constant buffers attributes to specify default values or to inject useful engine parameters. In the example we are using: The [WorldViewProjection] to inject the object world view projection matrix. The [Default(1, 1, 1)], which indicate the default value of the Color attribute (white color in this example). The majority of topics mentioned here is detailed in Effect Metatags document. List of Passes After the Resource Layout block, your code will specify a list of Passes, each Pass is defined using the [Begin_Pass] and [End_Pass] tags. Each pass requires a name, which will be used by the render path. As a naming convention, all render paths in Evergine must support the Default pass name. In the previous effect example, a Default pass is defined: [Begin_Pass:Default] [Profile 10_0] [Entrypoints VS=VertexShaderCode PS=PixelShaderCode] struct VS_IN { float4 Position : POSITION; #if TEX float2 TexCoord : TEXCOORD; #endif }; struct PS_IN { float4 Pos : SV_POSITION; #if TEX float2 Tex : TEXCOORD; #endif }; PS_IN VertexShaderCode(VS_IN input) { PS_IN output = (PS_IN)0; output.Pos = mul(input.Position, WorldViewProj); #if TEX output.Tex = input.TexCoord; #endif return output; } float4 PixelShaderCode(PS_IN input) : SV_Target { float4 color = float4(Color,1); #if TEX color *= ColorTexture.Sample(ColorSampler, input.Tex); #endif return color; } [End_Pass] In this pass, you will find: The [Entrypoints VS=... PS=...] tag, wich defines the entry point per each render pipeline stage. In the example, you are indicating that this pass will use the following entry points: VS=VertexShaderCode: During the Vertex Shader stage, the VertexShaderCode function will be executed. PS=PixelShaderCode: During the Pixel Shader stage, the PixelShaderCode function will be executed. Hereafter, a typical HLSL shader code is written. You are free to define structures, functions and uses all resources defined inside Resource Layout section. As we mentioned earlier, visit Effect Metatags for more information. Create a new Effect from code The following sample code can be used to create a new effect and its associated material to apply to an entity in your scene. protected override void CreateScene() { protected override void CreateScene() { var graphicsContext = Application.Current.Container.Resolve<GraphicsContext>(); var assetsService = Application.Current.Container.Resolve<AssetsService>(); string shaderSource = @\" [Begin_ResourceLayout] cbuffer PerDrawCall : register(b0) { float4x4 WorldViewProj : packoffset(c0); [WorldViewProjection] }; cbuffer Parameters : register(b1) { float3 Color : packoffset(c0); [Default(1.0, 0.0, 0.0)] }; [End_ResourceLayout] [Begin_Pass:Default] [Profile 10_0] [Entrypoints VS=VS PS=PS] struct VS_IN { float4 Position : POSITION; float3 Normal : NORMAL; float2 TexCoord : TEXCOORD; }; struct PS_IN { float4 pos : SV_POSITION; float3 Nor : NORMAL; float2 Tex : TEXCOORD; }; PS_IN VS(VS_IN input) { PS_IN output = (PS_IN)0; output.pos = mul(input.Position, WorldViewProj); output.Nor = input.Normal; output.Tex = input.TexCoord; return output; } float4 PS(PS_IN input) : SV_Target { return float4(Color,1); } [End_Pass] \"; // Create effect Effect myEffect = new EffectFromCode(graphicsContext, shaderSource); // Create material asociated Material myMaterial = new Material(myEffect) { LayerDescription = assetsService.Load<RenderLayerDescription>(EvergineContent.RenderLayers.Opaque), }; // Apply to an entity Entity primitive = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent() { Material = myMaterial }) .AddComponent(new SphereMesh()) .AddComponent(new MeshRenderer()); this.Managers.EntityManager.Add(primitive); } }"
  },
  "manual/graphics/effects/effect_editor.html": {
    "href": "manual/graphics/effects/effect_editor.html",
    "title": "Effect Editor | Evergine Doc",
    "keywords": "Effect Editor Effect Editor allows editing the effect assets. Double click over a effect asset shown in Assets Details will open this editor. The editor is composed of 2 main parts Shader Text Editor and Viewport: Shader Text Editor The shader text editor allows writing your effects in HLSL language with metatags. This editor has the common code editor feature included error marks, syntax highlighting and code completion (Ctrl+Space) that help you to create your custom effects. Actions Description Ctrl+Space Code completion. Ctrl+F Search word toolbox Alt+Left mouse button Edit multiple code lines. The effect is compiled automatically while you are writing the shader and on the editor bottom side, you can see the compilation process result. When the compilation results with errors, you can click on the error text and de editor will mark the error line and scroll view to it. Toolbox The shader text editor has a toolbox that helps you with some important task as enable/disable directives, generate material decorator associated or configure effect asset properties. The complete option list is: Icon Description Toggle button to indicate if the automatic compilation mode is enabled or manual compilation mode (Key F5) is enabled. Combo box to enable the current effect pass. Shows the effect directive list and allows enable/disable combinations. Allow to add common snippet codes into your effects. Generates the Material Decorator class in your project. Compiles all directive combinations and shows the combinations with errors. Shows the automatically translation from original HLSL to SPIRV(Vulkan), GLSL(OpenGL/OpenGLES) or MSL(Metal) when it will be used in other backends. Allows configure the asset profile as: exclude an effect on a single platform or pre-compile effect for a single platform. Translation Panel The translation panel is shown below the shader text editor after aclick on it button and shows the result to translate the HLSL current pass and directive combinations to other languages. The panel includes two combo boxes to select the translation languages: GLSL, ESSL, WebGL1, WebGL2, MSL or SPIRV and another to select the stage to translate: Vertex, Geometry, Hull, Domain or Compute. Profile Panel This panel allows configuring the effect asset properties by platform. The first tab is the default or global configuration but you can modify the default configuration using the platform tabs. The effect asset properties are: Property Values Description ExludeAsset True, false If it is enabled the effect asset will be excluded in the building project for the platform. If it is enabled in the Default configuration tab the asset never will be included in the build project process. GraphicsBackend ByPlatform, DirectX11, DirectX12, OpenGL, OpenGLES, Metal, Vulkan, WebGL1, WebGL2 or WebGPU. Defines the backend and languages that the effect will be translated and compiled. ByPlatform value indicates that the data will get from project settings (weproj file) Compile ByPlatform, Yes or No Defines if the effect will be pre-compiled or no. ByPlatform value indicates that the data will get from project settings (weproj file). Viewport Shows the result of the current effect in real-time. The viewport has text on the bottom side that indicates if the primitive mesh vertex input is compatible with vertex input shader because it otherwise will not possibly display the result. Actions Description Left mouse button To rotate the camera around the primitive. Right mouse button To rotate two lights around the primitive. Mouse wheel To make zoom in/out camera. Toolbox The viewport toolbox is on the top side of the Viewport and has the following options: Icon Description Defines the current RenderPipeline Path used by the viewport Defines the current pass used by the viewport Allows change the primitive used by the viewport. Allows change the background color of the viewport. Input Resources and properties The property list panel below the viewport allows to test the effect with different values for constant buffer properties, textures, or samplers, and the changes will update the viewport automatically. The resources and properties are updated automatically with any change in the effect resource layout block. The values are not stored so only are useful for testing proposes."
  },
  "manual/graphics/effects/effect_metatags.html": {
    "href": "manual/graphics/effects/effect_metatags.html",
    "title": "Effect Metatags | Evergine Doc",
    "keywords": "Effect Metatags In evergine the effect are written in HLSL languages, but to automatize some tasks evergine includes additional tags that you can add to the HLSL code. Block Metatags Effect codes are organized into two important kinds of blocks: Block Tags Description Resource Layout [Begin_ResourceLayout] [End_ResourceLayout] This block of code defines all resources (Constant Buffers, Structured Buffers, Textures and Samplers) using all effect passes. Pass [Begin_Pass:PassName] [End_Pass] This block of code defines a RenderPipeline pass. The DefaultRenderPipeline defines 3 passes that any effect can define: ZPrePass, Distortion, Default Directives Metatags Inside of resource layout block you can define the directive set that your custom effect will have. The directives are useful to enable different features of your effect. A directive can be defined as two values On/Off feature or can define a feature with multiple values: [Directive:Name A_OFF A] [Directive:Name A_OFF B C D ...] Example: [Directive:NormalMapping Normal_OFF, Normal] [Directive:ShadowFilter Shadow_OFF, ShadowFilter3 ShadowFilter5 ShadowFilter7] An effect is a set of the shader (known as Uber-shader) and the directive help you to define this set of the shader. The directives generate automatically multiple shaders with the effects is compiled. Example: [Directive:Name A_OFF A] will generate a shader with A enabled and another shader with A disabled. [Directive:Name A_OFF B C D ...] will generate an A, B, C, D ... shaders. Additionally, if you define several directives, it will multiply the combinations. In that case, if you define two directives: [Directive:FeatureA A_OFF A] [Directive:FeatureB B_OFF C D] It will generate the following shader combinations: A_OFF-B_OFF, A-B_OFF, A_OFF-C, A-C , A_OFF-D, A-D The number of combination are multiplied by the number of effect passes so a complex effect would have hundreds or thousands of combinations. The effects can compile his combination on-demand in runtime or pre-compiled combination before and use it later in runtime without compile. So you generate a bundle with compiled shader combinations. To know more details go to this section You can shape your effect code with the #if #else and #endif preprocessor directives: #if TEX // This code is compiled only if TEX directive is used... finalColor = ColorTexture.Sample(ColorSampler, input.Tex); #else // If TEX directive is not present, reach this code... finalColor = ColorAttribute; #endif Or use any directive combinations: #if TEX || NORMAL // This code is compiled only if TEX and NORMAL directives are used... output.texCoord = input.TexCoord; #endif Default Values Metatag Evergine allows to inject default values in constant buffer attributes automatically using tags. Default values can be injected directly using the [Default(value)] tag: cbuffer Parameters : register(b0) { float SpeedFactor : packoffset(c0.x); [Default(1.5)] float3 Position : packoffset(c0.y); [Default(2.3, 3.3, 5.6)] } Default value tag supports the following types: int, float, bool, float, float2, float3, float4. Inject Engine parameters Evergine allows injecting engine data to resource layout resources (Constant Buffers attributes and Textures) automatically using tags. For example, in the following code, the [WorldViewProjection] metatag is used to inject the object world view projection matrix: cbuffer PerDrawCall : register(b0) { float4x4 WorldViewProj : packoffset(c0); [WorldViewProjection] }; List of Parameter Tag Here you can find a complete list of available parameter tag that you can use into your effects: Parameters Tag Type Update Policy Description [FrameID] long PerFrame Gets Frame ID. [DrawContextID] int PerView Gets drawcontext ID. [DrawContextViewIndex] int PerView Gets the view index of this draw context. A draw context can contains several views (cascade shadow, point light shadows, reflection probe, etc...). [World] Matrix4x4 PerDrawCall Gets the world value of the current render mesh. [View] Matrix4x4 PerView Gets the view value of the current camera. [ViewInverse] Matrix4x4 PerView Gets the view inverse value of the current camera. [Projection] Matrix4x4 PerView Gets the projection value of the current camera. [UnjitteredProjection] Matrix4x4 PerView Gets the unjittered projection value of the current camera. [ProjectionInverse] Matrix4x4 PerView Gets the projection inverse value of the current camera. [ViewProjection] Matrix4x4 PerView Gets the view projection value of the current camera. [UnjitteredViewProjection] Matrix4x4 PerView Gets the unjittered view projection value of the current camera. [PreviousViewProjection] Matrix4x4 PerView Gets the view projection value of the current camera in the previous frame. [WorldViewProjection] Matrix4x4 PerDrawCall Gets the world view projection value of the current camera and mesh. [UnjitteredWorldViewProjection] Matrix4x4 PerDrawCall Gets the unjittered (TAA) world view projection value of the current camera and mesh. [WorldInverse] Matrix4x4 PerDrawCall Gets the inverse world value of the current render mesh. [WorldInverseTranspose] Matrix4x4 PerDrawCall Gets the world inverse transpose of the current mesh. [Time] float PerFrame Gets the time value since the game has started. [CameraPosition] Vector3 PerView Gets the position value of the current camera. [CameraJitter] Vector2 PerView Gets the current frame camera jittering. [CameraPreviousJitter] Vector2 PerView Gets the previous frame camera jittering. [CameraRight] Vector3 PerView Gets the right component of the camera orientation. [CameraUp] Vector3 PerView Gets the up component of the camera orientation. [CameraForward] Vector3 PerView Gets the forward component of the camera orientation. [CameraFocalDistance] float PerView Gets the camera focal distance (used with DoF). [CameraFocalLength] float PerView Gets the camera focal length. [CameraAperture] float PerView Gets the camera aperture. [CameraExposure] float PerView Gets the camera exposure. [CameraFarPlane] float PerView Gets the far plane of the camera. [CameraNearPlane] float PerView Gets the near plane of the camera. [ViewProjectionInverse] Matrix4x4 PerView Gets the inverse of the view projection value of the current camera. [MultiviewCount] int PerView Gets the number of eyes to be rendered. [MultiviewProjection] Matrix4x4 PerView Gets the stereo camera projection. [MultiviewView] Matrix4x4 PerView Gets the stereo camera view. [MultiviewViewProjection] Matrix4x4 PerView Gets the stereo camera view projection. [MultiviewViewProjectionInverse] Matrix4x4 PerView Gets the stereo camera inverse view projection. [MultiviewPosition] Vector4 PerView Gets the stereo camera view. [ForwardLightMask] ulong PerDrawCall Gets the lighting mask, used in Forward passes. [LightCount] uint PerView Gets the number of lights. [LightBuffer] IntPtr PerView Gets the light buffer ptr. [LightBufferSize] uint PerView Gets the light buffer size. [ShadowViewProjectionBuffer] IntPtr PerView Gets the shadow view projection buffer pointer. [ShadowViewProjectionBufferSize] uint PerView Gets the shadow view projection buffer size. [IBLMipMapLevel] uint PerFrame Gets the IBL texture mipmap level. [IBLLuminance] float PerFrame Gets the IBL luminance. [IrradianceSH] IntPtr PerFrame Gets the irradiance spherical harmonics buffer ptr. [IrradianceSHBufferSize] uint PerFrame Gets the irradiance spherical harmonics buffer size. [EV100] float PerView Gets the Exposition Value at ISO 100. [Exposure] float PerView Gets the camera exposure. [SunDirection] Vector3 PerFrame Gets the sun direction. [SunColor] Vector3 PerFrame Gets the sun color. [SunIntensity] float PerFrame Gets the sun intensity. [SkyboxTransform] Matrix4x4 PerFrame Gets the skybox transform. Texture Tag Description [Framebuffer] Framebuffer texture. [DepthBuffer] Depthbuffer texture. [GBuffer] GBuffer texture. [Lighting] Lighting texture. [DFGLut] Lookup table for DFG precalculated texture. [IBLRadiance] IBL Prefiltered Mipmapped radiance environment texture. [ZPrePass] ZPrePass in forward rendering (Normal + Roughness). [DistortionPass] Distortion pass in forward rendering. [IBLIrradiance] IBL diffuse irradiance map. [TemporalHistory] Temporal AA history texture. [DirectionalShadowMap] Shadow map array texture. [SpotShadowMap] Shadow map array texture. [PunctualShadowMap] Shadow map array cube texture. [Custom0..N] Custom renderpipeline texture. Pass Settings Metatags These tags are used inside of a pass block code and are useful to configure which settings do you want to compile this pass. Tag Description [Profile API_Level] Defines HLSL language version and capabilities. The API level values could be: 9_1: DirectX9.1 HLSL 3.0. 9_2: DirectX 9.2 HLSL 3.0 9_3: DirectX 9.3 HLSL 3.0 10_0: DirectX 10 HLSL 4.0 10_1: DirectX 10.1 HLSL 4.1 11_0: DirectX 11 HLSL 5.0 11_1: DirectX 11 HLSL 5.0 12_0: DirectX 12 HLSL 6.0 12_1: DirectX 12 HLSL 6.1 12_3: DirectX 12 HLSL 6.3 (Raytracing) [Entirypoints Stage=MethodName] Defines the entrypoint stage methods of the pass. The valid stages values are: VS: Vertex Shader. HS: Hull Shader. DS: Domain Shader. GS: Geometry Shader. PS: Pixel Shader. CS: Compute Shader. [Mode value] Defines the compilation mode of the pass. Available mode list: None: Default compilation mode. Debug: Debug mode includes depuration symbols to analyze with shader tools like RenderDoc, PIX or NVidia Nsight Graphics. See Profile with Renderdoc for more useful information. Release: Optimize compilation mode. [RequiredWidth Directive] Defines the directive list required by the pass. Example: [RequiredWith VCOLOR] the renderpipeline run this pass only when VCOLOR directive is enabled. Override Render Layer Metatags These tags allow the pass to modify the render layer properties when the render pipeline runs this pass. To know more details about the RenderLayer properties read this section: Rasterization Process Tag Description [FillMode Value] Determines the fill mode to use when rendering. Available values: WireFrame or Solid [CullMode Value] Indicates triangles facing the specified direction are not drawn. Available values: None, Front or Back [FrontCounterClockwise bool] Determines if a triangle is front- or back-facing. If this parameter is true, a triangle will be considered front-facing if its vertices are counter-clockwise on the render target and considered back-facing if they are clockwise. Available values: True or false [DepthBias int] Depth value added to a given pixel. The value is an integer. [DepthBiasClamp float] Maximum depth bias of a pixel. The value is a float [0-1]. [SlopeScaledDepthBias float] Scalar on a given pixel's slope. The value is a float. [DepthClipEnable bool] Enable clipping based on distance. Available values: True or False [ScissorEnable bool] Enable scissor-rectangle culling. All pixels outside an active scissor rectangle are culled. Available values: True or False [AntialiasedLineEnable bool] Specifies whether to enable line antialiasing; only applies if doing line drawing and MultisampleEnable is false. Available values: True or _False. Blend State Tag Description [AlphaToCoverageEnable bool] Specifies whether to use alpha-to-coverage as a multisampling technique when setting a pixel to a render target. Available values: True or _False. [IndependentBlendEnable bool] Specifies whether to enable independent blending in simultaneous render targets. Set to true to enable independent blending. If set to false, only the RenderTarget[0] members are used; RenderTarget[1..7] are ignored. Available values: True or _False. [RT0BlendEnable bool] Enable (or disable) blending for RenderTarget 0. Available values: True or _False. [RT0SourceBlendColor Value] This blend option specifies the operation to perform on the RGB value that the pixel shader outputs. The BlendOp member defines how to combine the SrcBlend and DestBlend operations. Availables values: Zero, One SourceColor, InverseSourceColor, SourceAlpha, InverseSourceAlpha, DestinationAlpha, InverseDesinationAlpha, DestinationColor, InverseDestinatinoColor, SourceAlphaSaturate, BlendFactor, InverseBlendFactor, SecondarySourceColor, InverseSecondarySourceColor, SecondarySourceAlpha_ or InverseSecondarySourceAlpha. [RT0DestinationBlendColor Value] This blend option specifies the operation to perform on the current RGB value in the render target. The BlendOp member defines how to combine the SrcBlend and DestBlend operations. Availables values: Zero, One SourceColor, InverseSourceColor, SourceAlpha, InverseSourceAlpha, DestinationAlpha, InverseDesinationAlpha, DestinationColor, InverseDestinatinoColor, SourceAlphaSaturate, BlendFactor, InverseBlendFactor, SecondarySourceColor, InverseSecondarySourceColor, SecondarySourceAlpha_ or InverseSecondarySourceAlpha. [RT0BlendOperationColor Value] This blend operation defines how to combine the SrcBlend and DestBlend operations. Available values: Add, Substract, ReverseSubstract, Min or Max. [RT0SourceBlendAlpha Value] This blend option specifies the operation to perform on the alpha value that the pixel shader outputs. Blend options that end in COLOR are not allowed. The BlendOpAlpha member defines how to combine the SrcBlendAlpha and DestBlendAlpha operations. Availables values: Zero, One SourceColor, InverseSourceColor, SourceAlpha, InverseSourceAlpha, DestinationAlpha, InverseDesinationAlpha, DestinationColor, InverseDestinatinoColor, SourceAlphaSaturate, BlendFactor, InverseBlendFactor, SecondarySourceColor, InverseSecondarySourceColor, SecondarySourceAlpha or InverseSecondarySourceAlpha. [RT0DestinationBlendAlpha Value] This blend option specifies the operation to perform on the current alpha value in the render target. Blend options that end in COLOR are not allowed. The BlendOpAlpha member defines how to combine the SrcBlendAlpha and DestBlendAlpha operations. Availables values: Zero, One SourceColor, InverseSourceColor, SourceAlpha, InverseSourceAlpha, DestinationAlpha, InverseDesinationAlpha, DestinationColor, InverseDestinatinoColor, SourceAlphaSaturate, BlendFactor, InverseBlendFactor, SecondarySourceColor, InverseSecondarySourceColor, SecondarySourceAlpha or InverseSecondarySourceAlpha. [RT0BlendOperationAlpha Value] This blend operation defines how to combine the SrcBlendAlpha and DestBlendAlpha operations for RenderTarget 0. Available values: Add, Substract, ReverseSubstract, Min or Max. [RT0ColorWriteChannels Value] A write mask for Render target 0. Availables values: None, Red, Green, Blue, Alpha or All. Depth Stencil Tag Description [DepthEnable bool] Enable depth testing. Availables values: True or False. [DepthWriteMask bool] Identify a portion of the depth-stencil buffer that can be modified by depth data. Available values: True or False. [DepthFunction Value] A function that compares depth data against existing depth data. Availables values: Never, Less, Equal, LessEqual, Greater, NotEqual, GreaterEqual or Always. [StencilEnable bool] Enable stencil testing. Availables values: True or False. [StencilReadMask byte] Identify a portion of the depth-stencil buffer for reading stencil data. The value is a byte. [StencilWriteMask byte] Identify a portion of the depth-stencil buffer for writing stencil data. The value is a byte. [FrontFaceStencilFailOperation Value] The stencil operation to perform when stencil testing fails in FrontFace Availables values: Keep, Zero, Replace, IncrementSaturation, DescrementSaturation, Invert, Increment, Decrement. [FrontFaceStencilDepthFailOperation Value] The stencil operation to perform when stencil testing passes and depth testing fails in FrontFace. Availables values: Keep, Zero, Replace, IncrementSaturation, DescrementSaturation, Invert, Increment, Decrement. [FrontFaceStencilPassOperation Value] The stencil operation to perform when stencil testing and depth testing both pass in FrontFace. Availables values: Never, Less, Equal, LessEqual, Greater, NotEqual, GreaterEqual or Always. [FrontFaceStencilFunction Value] A function that compares stencil data against existing stencil data in FrontFace. Availables values: Never, Less, Equal, LessEqual, Greater, NotEqual, GreaterEqual or Always. [BackFaceStencilFailOperation Value] The stencil operation to perform when stencil testing fails in BackFace Availables values: Keep, Zero, Replace, IncrementSaturation, DescrementSaturation, Invert, Increment, Decrement. [BackFaceStencilDepthFailOperation Value] The stencil operation to perform when stencil testing passes and depth testing fails in BackFace. Availables values: Keep, Zero, Replace, IncrementSaturation, DescrementSaturation, Invert, Increment, Decrement. [BackFaceStencilPassOperation Value] The stencil operation to perform when stencil testing and depth testing both pass in BackFace. Availables values: Never, Less, Equal, LessEqual, Greater, NotEqual, GreaterEqual or Always. [BackFaceStencilFunction Value] A function that compares stencil data against existing stencil data in BackFace. Availables values: Never, Less, Equal, LessEqual, Greater, NotEqual, GreaterEqual or Always. [StencilReference int] The reference value to use when doing a stencil test. The value is a integer."
  },
  "manual/graphics/effects/index.html": {
    "href": "manual/graphics/effects/index.html",
    "title": "Effects | Evergine Doc",
    "keywords": "Effects An effect is a uber-shader so be able to represent a single shader or a large group of shaders. A shader is a GPU program that can be run on the GPU and are able to perform rendering calculation using textures, vertoces and other resources. The effects in Evergine are written using HLSL. While HLSL is only supported by DirectX backends (DX11 and DX12), The effects are automatically translated to another language when it is necessary (to GLSL when using OpenGL, or Spir-V in Vulkan). Standard Effect Default evergine project template imports the Evergine.Core package package and this package includes several effects like the Standard Effect used by Default Material asset. Effects are a type of asset and have a dedicated Editor Effect Editor. In this section Create Effects Effect metatags Using Effects Effect Editor Built-in Effect"
  },
  "manual/graphics/effects/using_effects.html": {
    "href": "manual/graphics/effects/using_effects.html",
    "title": "Using Effects | Evergine Doc",
    "keywords": "Using Effects In this document, you will learn how to load and use Effects in your applications. How to apply an effect to a material from Evergine Studio. From Material Editor you can select the effect that the material will use on the top side of the properties. Load Effect from code The following sample code can be used to instantiate an existing effect asset, created an associated material, and apply an entity in your scene. protected override void CreateScene() { var assetsService = Application.Current.Container.Resolve<AssetsService>(); // Load effect Effect standardEffect = assetsService.Load<Effect>(EvergineContent.Effects.StandardEffect); // Create material asociated Material defaultMaterial = new Material(standardEffect) { LayerDescription = assetsService.Load<RenderLayerDescription>(EvergineContent.RenderLayers.Opaque), }; // Apply to an entity Entity primitive = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent() { Material = defaultMaterial }) .AddComponent(new SphereMesh()) .AddComponent(new MeshRenderer()); this.Managers.EntityManager.Add(primitive); }"
  },
  "manual/graphics/environment/environment_manager.html": {
    "href": "manual/graphics/environment/environment_manager.html",
    "title": "Environment Manager | Evergine Doc",
    "keywords": "Environment Manager The EnvironmentManager is a SceneManager responsible to control and provide the Environmental lighting of the scene. EnvironmentManager Properties Default Description IntensityMultiplier 1.0 This value modify the overall intensity of the environmental lighting. Useful to increase or reduce the IBL intensity. This property doesn't affect regular Lights (DirectionalLights, PointLights, etc...). IBLReflectionProbe scene probe This is the ReflectionProbe instance used in the EnvironmentManager. This class contains the IBL textures and information. Strategy Automatically This property indicate to Evergine Studio how often the Environment will be generated. Automatically: Evergine Studio updates automatically the scene IBL every time that detects that it need to be updated (Sun direction changes, skybox material changes, etc...) OnDemand: Only updates the scene IBL on demand, when the user want it. When this option is selected, a Generate button appears. Clicking this button force Evergine Studio to recreate the scene IBL. \"Skybox\" entity tag By default, Evergine Studio creates automatically a environmental light for each scene. To do this, they create a cubemap from the (0,0,0) position, and will include all entities with the \"Skybox\" as Tag property. When you creates a new scene in Evergine Studio, it will create by default a Sphere Dome entity called \"SkyAtmosphere\", which render a sky environment controlled by a DirectionalLight marked as Sun."
  },
  "manual/graphics/environment/environment_textures.html": {
    "href": "manual/graphics/environment/environment_textures.html",
    "title": "Environment Textures | Evergine Doc",
    "keywords": "Environment Textures This is the best way to transfer the illumination of a equirectangular HDR image to your scene. Equirectangular textures A equirectangular image is created to convert a 360º view into a 2D texture. It usually has an aspect ratio of 2:1. If you use HDR equirectangular images (generally .hdr format) will produce more realistic illuminations. In Poly Haven site you can find a lot of environment textures grouped by categories (outdoor, skies, indoor, night, etc...) SkyboxEffect En Evergine, to create an environment lighting from a equirectangular image, you need to create a environment sphere dome with a SkyboxEffect material, that will use the HDR environment texture. The SkyboxEffect only has 3 useful properties: Property Description Texture The equirectangular image to use as environment. TextureSampler The SamplerState used to sample the texture. Parameters_Intensity A factor that modify the intensity of the texture. This is used to increase or reduce the brightness of the texture. Warning If you want to use this effect with a common SphereMesh, use the Skybox layer in the LayerDescription property. Using Skybox material Steps to create an Environmental lighting using a equirectangular image: Import into your Evergine project an equirectangular HDR image. Create a new SkyboxEffect material Edit the new SkyboxEffect material, and set the Texture property with the imported equirectangular image. Assign the Skybox LayerDescription tho the new SkyboxEffect material. Now you have two options: Option A (reuse the SkyAtmosphere): Select the SkyAtmosphere entity, in the MaterialComponent, set the new SkyboxEffect material. This will override the AtmosphereController appearance. Option B (create a new sky dome): Create a new Sphere primitive in your scene, and assign the new SkyboxEffect material. In the new entity, set the Tag property to \"Skybox\" Delete the SkyAtmosphere entity."
  },
  "manual/graphics/environment/index.html": {
    "href": "manual/graphics/environment/index.html",
    "title": "Environment | Evergine Doc",
    "keywords": "Environment In this section we will cover how you can control in Evergine the environmental light. Image Based Lighting (IBL) Image Based lighting (IBL) is a rendering technique which involves capturing an omnidirectional representation of real-world light information as an image, typically using a 360° camera. This image is then projected onto a dome or sphere analogously to environment mapping, and this is used to simulate the lighting for the objects in the scene. This allows highly detailed real-world lighting to be used to light a scene, instead of trying to accurately model illumination using an existing rendering technique. Image-based lighting often uses high-dynamic-range (HDR) imaging for greater realism. IBL involves the creation of two lighting components: Irradiance map (Diffuse): For the diffuse illumination, we need what is called an Irradiance Map. This usually involves a cubemap (or Spherical Harmonics) that stores the amount of light coming from each direction. Radiance map (Specular): Now, when we get to specular illumination, we need a texture called Pre-filtered Mip-Mapped Radiance Environment Map (PMREM). This is another cubemap that pre-calculate the reflected environment. An as an addition, it store in its MipMap levels different reflections for roughness values. Credits LearnOpenGL Evergine will use Image Based Lighting to create environmental illumination. In this section Environment Manager Sky Atmosphere Environment Textures"
  },
  "manual/graphics/environment/sky_atmosphere.html": {
    "href": "manual/graphics/environment/sky_atmosphere.html",
    "title": "Sky Atmosphere | Evergine Doc",
    "keywords": "Sky Atmosphere This is the default way to create environment lighting in Evergine. When you create a new Scene in Evergine Studio, it will use this method to provide the environmental lighting. They create a HDR sky texture that simulates physically the atmosphere properties. Additionally it will use a Directional Light to control the atmosphere light dispersion and light intensity and color. SunComponent This component is used to mark a DirectionalLight as the light source that will use the atmosphere controller. When you create a Evergine Scene, it will create a DirectionalLight with a SunComponent. Changing the DirectionalLight rotation allows you to change the visual atmospheric, simulating effects like sunset, sunrise or daylight: This component has no properties, and the only purpose is to mark a DirectionalLight as the Sun source. Note Only the first light with SunComponent will be used. AtmosphereController component This component is responsible to control the set the physical properties of the atmosphere and render the sky dome texture. Property Default Description RayleighScattering 5.5, 13, 22.4 Rayleigh scattering coefficient per each color channel wavelength (Red, Green and Blue). RayleighScaleHeight 8000 Scale height for Rayleigh scattering measured in meters. PlanetRadiusInKm 6371 Radius of the planet in Kilometers AtmosphereRadiusInKm 6471 Radius of the atmosphere in Kilometers MieScatteringCoefficient 21 Mie scattering coefficient. MieScaleHeight 12000 Mie scattering scale height in meters MiePreferredScattering 0.758 Mie preferred scattering direction. ModifySunColor true This will force to SunIntensity 22.0 The Sun intensity. SunDiskEnabled true Indicates if the environment will create a sun disk. SunDiskSize 0.02 The size of the sun disk in degrees These properties allows you to simulate another atmospheres of other worlds, like Mars (atmosphere obtained from here): Or you can simulate any exotic atmosphere: Using Sky Atmosphere In Evergine Studio You can add two entities from Evergine Studio under Environment menu entry: Sun Light: This will create a Photometric Directional Light marked as a Sun light with the SunComponent Sky Atmospheric dome: Will create a Sphere mesh with the AtmosphereController component to create the atmosphere. This entity also has the \"Skybox\" tag. From code This is a small code snippet to create both entities needed to create a sky atmosphere: protected override void CreateScene() { // Create the sun light var sun = new Entity() .AddComponent(new Transform3D() { LocalRotation = new Vector3(-2, 0, 0) }) // Add some rotation to the light... .AddComponent(new PhotometricDirectionalLight()) .AddComponent(new SunComponent()); this.Managers.EntityManager.Add(sun); // Create the sphere sky dome var skyDome = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent()) .AddComponent(new SphereMesh()) .AddComponent(new MeshRenderer()) .AddComponent(new AtmosphereController()); this.Managers.EntityManager.Add(skyDome); }"
  },
  "manual/graphics/fonts/create_text3D.html": {
    "href": "manual/graphics/fonts/create_text3D.html",
    "title": "Create Text3D | Evergine Doc",
    "keywords": "Create Text3D Text3D is a component that allows to render a paragrah in 3D space. It is possible to render a simple header text or a large paragram and configure the limit of the text, wrapping and ellipsis. Create a Text3D in Evergine Studio You can create a Text3D click button on from Entity Hierarchy panel to deploy a create menu options and click on the option \"Text3D\" A Text3D entity will be added to your scene In the Text3DMesh component of your Text3D entity you will find the following properties: Property Description Font The font asset used. (Font family) Layer RenderLayer used to render the text. Text The text will be drawn. It is possible to use /n to line break. Color The text color. Size The canvas size or area. Enable DebugMode property in the Text3DRenderer component to show this area (blue rectangle) ScaleFactor The text scale factor. Wrapping Word wrapping. If this option is enabled line breaking will be created automatically when the current line hasn't enough space to add more words. Ellipsis If this option is enabled it will show a three ellipsis at the end of the text when hasn't enough space in the canvas to add more letters. HorizontalAlignment Allows align the text horizontally. The available values are: Left, Center, and Right VerticalAlignment Allows align the text vertically. The available values are: Top, Center, and Bottom Origin Allows to configure the origin of the Text3D entity. The value is a vector2 with values between [0-1]. LineSpacing Allows to configure the space between text lines. Softness Allows to configure anti-aliasing effect. The value is a float between [0-2] Create a Text3D from code The following code shows the list of components necessary to convert an entity into a billboard entity. public class MyScene : Scene { protected override void CreateScene() { var assetsService = Application.Current.Container.Resolve<AssetsService>(); var defaultFont = assetsService.Load<Font>(DefaultResourcesIDs.DefaultFontID); var text = new Entity() .AddComponent(new Transform3D()) .AddComponent(new Text3DMesh() { Font = defaultFont, Text = \"Hello World!\", }) .AddComponent(new Text3DRenderer()); this.Managers.EntityManager.Add(text); } } The result: Enable debug mode Sometimes will be useful to enable DebugMode property in Text3DRenderer component to draw the debug information of our Text3D entity. The canvas space will be shown as a blue rectangle. The text space will be shown as a yellow rectangle and a red point will be render on each character origin."
  },
  "manual/graphics/fonts/font_editor.html": {
    "href": "manual/graphics/fonts/font_editor.html",
    "title": "Font Editor | Evergine Doc",
    "keywords": "Font Editor Font Editor allows editing the font assets. Double click over a font asset shown in Assets Details will open this editor. The editor is composed of 3 main parts: Viewport Shows the result of the current font configuration. The viewport has a toolbox on the top side that allows change the font color and background color. Input Text In this area you can input different texts to test the font configuration in the viewport. By default appear the paragrah \"The quick brown fox jumps over the lazy dog. 1234567890.:,;'\"(!?)+-*/=\" that test all letters, numbers and punctuation marks. But if you modify the charset to include additional characters you can test including them in this input text. Properties The font properties that you can configure are: Property Default value Description MinGlyphSize 32 The minimum glyph size used to render a font glyph in the atlas. When a font is so thin or has thin parts as ligatures, it will be necessary to increase this size for a correct render. Increasing the size produces more large atlas and a more weight font asset. Charset ['','~'] Sets the character set. The charset is a text with UTF-8 or ASCII encondig. The following syntax are correct: Single character: 'A' (UTF-8 enconded), 65 (decimal Unicode), 0x41 (hexadecimal Unicode) Range of characters: ['A', 'Z'],[65, 90], [0x41, 0x5a] String of characters: \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" (UTF-8 enconded) The entries should be separated by commas or whitespace. In between quotation marks, backslash is used as the escape character (e.g. ''', '\\', \"!\"#\"). The order in which characters appear is not taken into consideration."
  },
  "manual/graphics/fonts/import_fonts.html": {
    "href": "manual/graphics/fonts/import_fonts.html",
    "title": "Import Fonts | Evergine Doc",
    "keywords": "Import Fonts The font asset allows you render text in your project using differents font families and styles. Import a Font asset in Evergine Studio You can create a font asset by dragging a font file to the Assets Details panel, as explained in this article. Font files in content directory Fonts imported in Evergine create an aditional metadata .weft file. Supported formats: Evergine supports the following font file formats: Extension Description .ttf TrueType is an outline font standard developed by Apple in the late 1980s as a competitor to Adobe's Type 1 fonts used in PostScript. It has become the most common format for fonts on the classic Mac OS, macOS, and Microsoft Windows operating systems. The primary strength of TrueType was originally that it offered font developers a high degree of control over precisely how their fonts are displayed, right down to particular pixels, at various font sizes. With widely varying rendering technologies in use today, pixel-level control is no longer certain in a TrueType font. .otf OpenType is a format for scalable computer fonts. It was built on its predecessor TrueType, retaining TrueType's basic structure and adding many intricate data structures for prescribing typographic behavior. OpenType is a registered trademark of Microsoft Corporation. The specification germinated at Microsoft, with Adobe Systems also contributing by the time of the public announcement in 1996. Because of wide availability and typographic flexibility, including provisions for handling the diverse behaviors of all the world's writing systems, OpenType fonts are used commonly on major computer platforms."
  },
  "manual/graphics/fonts/index.html": {
    "href": "manual/graphics/fonts/index.html",
    "title": "Fonts and Texts | Evergine Doc",
    "keywords": "Fonts and Texts Fonts is a asset that represents a TrueType font that could be using to draw Text in your project. The available font file formats supported are .ttf and .otf . Font atlas Evergine uses the Multi-channel signed distance field technique to generate a prerender sprite font atlas from the original font file. A font atlas is typically stored in texture memory and used to draw text in real-time rendering contexts, the main issue with the traditional sprite-atlas is that the glyphs are prerender in a specified resolution, so if you use this sprite-atlas to render a big text or the camera is closed to the text, is possible detect loss of resolution and aliasing on the text. Multi-channel signed distance field allows to render font glyphs minimizing loss of resolution with diferent font size and even when the camera is closed to the text. Follow this link to read more details about this technique. Default Font Default Evergine project template imports the Evergine.Core package and this package includes the Arial Font. Text3D component uses Arial font as default you can create your own font asset and change it in the Text3D component. Fonts are a type of asset and have a dedicated Editor Font Editor. In this section Import Font Font Editor Create Text3D"
  },
  "manual/graphics/index.html": {
    "href": "manual/graphics/index.html",
    "title": "Graphics | Evergine Doc",
    "keywords": "Graphics Evergine's graphics features let you control the appearance of your applications and are highly-customizable. You can use Evergine's graphics features to create beautiful, optimized graphics across a range of platforms, from mobile, portable XR headset to desktop. This section explains how to use Evergine for graphics and rendering. In this section Supported graphics backends Cameras Lights Materials Textures Models Primitives Post Processing Graph Particles Effects Environment Line Batch Sprites Billboards Fonts and Texts Low-level API"
  },
  "manual/graphics/lights.html": {
    "href": "manual/graphics/lights.html",
    "title": "Lights | Evergine Doc",
    "keywords": "Lights Evergine uses an advanced lighting mode to simulate how light affect geometries. It also supports multiple lights in the scene, allowing a wide range of environment and possibilities. Every type of light is modelled in Evergine using a Light component. Create a Light in Evergine Studio In the Entities Hierarchy panel of your Scene Editor, click the \"Add Entity and select Light, then choose the kind of light you want to create: Point Light Directional Light Sphere Area Light Spot Light Disc Area Light Rectangle Area Light Rube Area Light We discuss the light types later in this article. Additionally we can create photometric lights from the same panel with theese options: Photometric Point Light Photometric Directional Light Photometric Sphere Area Light Photometric Spot Light Photometric Disc Area Light Photometric Rectangle Area Light Photometric Rube Area Light Create Light from code The following sample code can be used to instantiate a new basic new point light entity in a scene. protected override void CreateScene() { // Create a new light entity. Entity pointLightEntity = new Entity() .AddComponent(new Transform3D()) .AddComponent(new PointLight() { Color = Color.Red, Intensity = 3, LightRange = 10 }); // Add the light entity to the entity manager. this.Managers.EntityManager.Add(pointLightEntity); } Basic light properties These are the basic properties that almost all the lights have. Property Description Is Enabled If the light is on / off. Shadow Enabled Enable/disable shadow mapping for this light. Photometric / Non photometric lights Photometric lights use photometric (lighting units) allowing you to define precisely lights as they would be in the real world. They are configured using physical parameters. Evergine supports both photometric and non photometric lights, and it offers all the same light types duplicated depending this choice. Common Photometric properties Every photometric light (no matter what type it is) defines this parameters, additionally each specific type of light defines its own intensity unit properties. Property Description Color By Temperature Indicates if the light color will be overrided using the light temperature. Temperature The light temperature in Kelvin (K). When ColorByTemperature is true, the light color is overrided by the light temperature. Note Light Intensity unit depends on the light type (for example, PointLights are measured in Lumens, whether DirectionalLights are measured in Lux) Non photometric lights properties For contrary, if you create regular lights, you could use the basic light properties: Property Description Color The RGB color tint of the light. Intensity The light intensity value in a non-standard unit. Greater values will produce brighter illumination. Types of lights There are different types of lights, each one useful for a different scenario. In general terms, lights are divided in two general categories: Directional Lights: These lights have unlimited boundaries, and every object is affected by this light type. Volume Lights: The light influence is delimited by a range. Area Lights: A subtype of volume lights that emulate lights coming from an area instead of a point in the space. Directional Lights / Photometric Directional Light Directional lights are lights that comes uniformly from one direction and at infinited distance. They're used to simulate distant light sources like the sun. It uses the forward vector of its entity Transform3D for calculating the light direction. In case of using a Photometric Directional Light, the intensity is measured using the following property: Photometric Property Description Illuminance (Lux) The light Illuminance measured in Lux. Indicates the total luminous flux incident on a surface, per unit area. Volume Lights Volume lights are a common type of lights in which the light source comes from a specific point in the space, and its intensity decays with the distance. Property Description Light Range The light range in meters. In case of using a Volume Photometric Lights, the intensity is measured using the following property: Photometric Property Description Luminous Power (Lumen) The light luminous flux in Lumen Point Light / Photometric Point Light A point light is located at a point in space and it emits light in all directions equally within its sphere range. Its intensity decays with distance from the light, reaching zero at its maximum range. It's useful for local lights like lamps. Spot Light / Photometric Spot Light A spot light is also placed in a specific location and it has a range over which the light decays. However, spot lights are also constrained by an angle, defining a cone shape light. Specific Properties Property Description InnerConeAngle The inner angle of the spotlight cone. OuterConeAngle The outter angle of the spotlight cone Tube Area lights / Photometric Tube Area Light Tube Area Lights are lights that comes from a line segment and have a range and a emission thickness. They are usefull for simulating neon lights. Specific Properties Property Description Length The length of the tube light volume. Radius The radius of the tube light volume. Sphere Area Lights / Photometric Sphere Area Light Sphere Area Lights behave like a phyical sphere emiting light rather a point light. They create much more softer lighting and can use for creating dynamic environment. Specific Properties Property Description Radius The radius of the sphere light volume. Disc Area Lights / Photometric Disc Area Light Disc Area Lights emit their light from a disc with a specified radius and at a maximum range. Useful for creating artificial soft lights. Specific Properties Property Description Radius The radius of the disc area light. Rectangle Area Lights / Photometric Rectangle Area Light Rectangle Area Lights emit their light from a rectangle with a specified with and height, at a maximum range. Useful for creating indoor window lighting, for example. Specific Properties Property Description Width The width of the rectangle area light. Height The height of the rectangle area light. Note Area lights needs to make complex calculation in order to properly simulate their shape. Therefore, they are significantly more performance heavy than their punctual counterparts (Point, Spot and Directional). Shadows To enable light cast shadows, the following properties has been added: Property Description Shadow Enabled Enable/disable shadow mapping for this light. Debug Mode Debug ShadowMap cascades used to generate this light shadow. Shadow Opacity Value [0-1] that represents the total opacity of the shadow. 1 by default. Shadow Bias Shadow bias for this specific light. Choosing the correct bias value allows to control shadows artifacts like a Moiré-like patern or Peter panning"
  },
  "manual/graphics/linebatch/custom_linebatch.html": {
    "href": "manual/graphics/linebatch/custom_linebatch.html",
    "title": "Create custom LineBatch | Evergine Doc",
    "keywords": "Create custom LineBatch In most cases will be enough to use the default linebatch3D provided by the RenderManager, but sometimes is useful to create your custom line batch because you can modify its global transformation or you can modify its render behavior without impact in the default line batch included in Evergine. In these cases will be interesting to create an independent custom line batch. How to create a custom linebatch public class MyDrawable : Drawable3D { [BindService] private AssetsService assetsService = null; private GraphicsContext graphicsContext; private LineBatch3D lineBatch; protected override bool OnAttached() { this.graphicsContext = Application.Current.Container.Resolve<GraphicsContext>(); var layer = this.assetsService.Load<RenderLayerDescription>(DefaultResourcesIDs.OpaqueRenderLayerID); // Create custom line batch 3D this.lineBatch = new LineBatch3D(this.graphicsContext, layer); // Add line batch to render this.Managers.RenderManager.AddRenderObject(this.lineBatch); return base.OnAttached(); } protected override void OnActivated() { // Enable line batch when the component is activated this.lineBatch.IsEnabled= true; base.OnActivated(); } protected override void OnDeactivated() { // Disable line batch when the component is deactivated this.lineBatch.IsEnabled= false; base.OnDeactivated(); } protected override void OnDetach() { // Remove line batch from render when the component is detached this.Managers.RenderManager.RemoveRenderObject(this.lineBatch); base.OnDetach(); } public override void Draw(DrawContext drawContext) { // Draw a sample blue cone this.lineBatch.DrawCone(0.5f, 1.0f, Vector3.UnitY, Vector3.Down, Color.Blue); } } Result Some interesting properties The line batch has a Transform property that could be used to apply transformations (translation, rotation, or scale) to all elements added to the batch. For example, if you use the line batch to draw a CAD map you can use this property to rotate the whole map. The line batch by default requires that every frame the elements to draw will be added to the batch, but in some cases is interesting to create a static batch and draw the same elements every frame because any new element will not need to be added to the batch at runtime, so you can use the property ResetAfterRender to indicates that the line batch doesn't reset the batch every frame. this.lineBatch = new LineBatch3D(this.graphicsContext, layer) { ResetAfterRender = false, };"
  },
  "manual/graphics/linebatch/index.html": {
    "href": "manual/graphics/linebatch/index.html",
    "title": "Line Batch | Evergine Doc",
    "keywords": "Line Batch Linebatch is very useful when you need to create a huge amount of line in your scene. For example for dummy objects or helpers in your scene. The more interesting thing of this feature is that all lines that you created are batch in a single drawcall so you archieve a good performance. This feature is only available from code. You can find the LineBatch3D and LineBatch2D in the scene RenderManager. The lines are compounds by two vertex and an edge so you cannot control the thickness of the lines. If you want to draw thickness lines see the Line3D. In this section Using Linebatch Create custom Linebatch"
  },
  "manual/graphics/linebatch/using_linebatch.html": {
    "href": "manual/graphics/linebatch/using_linebatch.html",
    "title": "Using LineBatch | Evergine Doc",
    "keywords": "Using LineBatch LineBatch only can be used from code. In the scene RenderManager you will find the LineBatch3D to draw lines in 3d space and the LineBatch2D to draw lines in 2d space. If you want to add a debug or helper mode to your entity, you can add a Drawable3D component to your scene and from this you will be access to the linebatch: The following example draw a red line from (0,0,0) to (0,1,0) From your scene.cs protected override void CreateScene() { ... // Add dummy entity to your scene var dummyEntity = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MyDrawable()); this.Managers.EntityManager.Add(dummyEntity); } Drawable component implementation // Drawable component using lineBatch3D public class MyDrawable : Drawable3D { public override void Draw(DrawContext drawContext) { this.RenderManager.LineBatch3D.DrawLine(Vector3.Zero, Vector3.Up, Color.Red); } } The lineBatch3D not only draw lines but can also draw Point, Sphere, Box ... And LineBatch2D is similar but with shapes (Circle, Square ...) Note LineBatch3D must be used with Drawable3D and LineBatch2D with Drawable2D_ LineBatch This section shows with examples all geometries that LineBatch can draw: DrawArc Vector3 origin = Vector3.Zero; Color color = Color.White; this.RenderManager.LineBatch3D.DrawArc(ref origin, 0.5f, 0.5f, ref color); DrawAxis this.RenderManager.LineBatch3D.DrawAxis(Matrix4x4.Identity, 1.0f); DrawBoundingBox this.RenderManager.LineBatch3D.DrawBoundingBox(new BoundingBox(Vector3.Zero, Vector3.One), Color.White); DrawBoundingFrustum this.RenderManager.LineBatch3D.DrawBoundingFrustum(new BoundingFrustum(Matrix4x4.Identity), Color.White); DrawBoundingOrientedBox this.RenderManager.LineBatch3D.DrawBoundingOrientedBox(new BoundingOrientedBox(Vector3.Zero, Vector3.One * 0.5f, Quaternion.CreateFromAxisAngle(Vector3.Right, MathHelper.PiOver4)), Color.White); DrawBoundingSphere this.RenderManager.LineBatch3D.DrawBoundingSphere(new BoundingSphere(Vector3.Zero, 1.0f), Color.White); DrawRectangle this.RenderManager.LineBatch3D.DrawRectangle(Vector3.Zero, Vector3.One, Color.White); DrawCircle this.RenderManager.LineBatch3D.DrawCircle(Vector3.Zero, 1.0f, Color.White); DrawCone this.RenderManager.LineBatch3D.DrawCone(0.5f, 1.0f, Vector3.Zero, Vector3.Down, Color.White); DrawCube this.RenderManager.LineBatch3D.DrawCube(Vector3.Zero, Vector3.One, Color.White); DrawForward this.RenderManager.LineBatch3D.DrawForward(Matrix4x4.Identity, 1.0f); DrawPoint this.RenderManager.LineBatch3D.DrawPoint(Vector3.Zero, 0.5f, Color.White); DrawRay this.RenderManager.LineBatch3D.DrawRay(Vector3.Zero, Vector3.Forward, Color.White); DrawTriangle this.RenderManager.LineBatch3D.DrawTriangle(new Vector3(-0.5f, 0, 0), new Vector3(0, 1.0f, 0), new Vector3(0.5f, 0, 0), Color.White);"
  },
  "manual/graphics/lines_3d.html": {
    "href": "manual/graphics/lines_3d.html",
    "title": "Lines 3D | Evergine Doc",
    "keywords": "Lines 3D Coming soon"
  },
  "manual/graphics/low_level_api/buffer.html": {
    "href": "manual/graphics/low_level_api/buffer.html",
    "title": "Buffer | Evergine Doc",
    "keywords": "Buffer A Buffer represents a block of memory that can be used in GPU operations. You can use buffers to store a wide variety of data, including position vectors, normal vectors, texture coordinates in a vertex buffer, indexes in an index buffer for example. Creation To create a buffer, first you need to create the BufferDescription struct: // Populate some data for the buffer... Vector4[] vertexData = new Vector4[] { new Vector4(0.0f, 0.2f, 0.0f, 1.0f), new Vector4(1.0f, 0.0f, 0.0f, 1.0f), new Vector4(0.2f, -0.2f, 0.0f, 1.0f), new Vector4(0.0f, 1.0f, 0.0f, 1.0f), new Vector4(-0.2f, -0.2f, 0.0f, 1.0f), new Vector4(0.0f, 0.0f, 1.0f, 1.0f), }; uint expectedSize = (4 * 4) * (uint)vertexData.Length; BufferFlags expectedFlags = BufferFlags.VertexBuffer; ResourceUsage expectedUsage = ResourceUsage.Default; // Create the BufferDescription.... BufferDescription bufferDescription = new BufferDescription(expectedSize, expectedFlags, expectedUsage); // Create the Buffer Buffer buffer = this.GraphicsContext.Factory.CreateBuffer(vertexData, ref bufferDescription); BufferDescription Property Type Description SizeInBytes uint Retrieves or sets the size of the new buffer. Flags BufferFlags Buffer flags describing buffer type. CpuAccess ResourceCpuAccess Specifies the types of CPU access allowed for this buffer. Usage ResourceUsage Usage of this buffer. StructureByteStride int The structure byte stride. ResourceUsage Identifies expected resource usage during rendering. ResourceUsage Description Default A resource that requires read and write access by the GPU, Default value. Immutable A resource that can only be read by the GPU. It cannot be written by the GPU, and cannot be accessed at all by the CPU. Dynamic A resource that is accessible by both the GPU (read only) and the CPU (write only). Staging A resource that supports data transfer (copy) from the GPU to the CPU. BufferFlags Identifies how to bind a buffer. This flag gives a hint to the graphics API of how this buffer will be used. BufferFlags Description None No option. VertexBuffer Bind a buffer as a vertex buffer to the input-assembler stage. IndexBuffer Bind a buffer as an index buffer to the input-assembler stage. ConstantBuffer Bind a buffer as a constant buffer to a shader stage. This flag may NOT be combined with any other bind flag. ShaderResource Bind a buffer or texture to a shader stage. AccelerationStructure Bind a buffer to used in a raytracing stage. RenderTarget Bind a texture as a render target for the output-merger stage. UnorderedAccess Bind a buffer as unordered access resource. BufferStructured Bind a buffer as structured buffer resoruce. IndirectBuffer Bind a buffer as indirect buffer to the input-assembler stage. ResourceCpuAccess Specifies the types of CPU access allowed for a resource. ResourceCpuAccess Description None Not specified, Default value. Write The CPU can be write this resource. Read The CPU can be read this resources. Using Buffers How to update a Default Buffer (Buffer created with ResourceUsage.Default) In that case, you just need to execute the GraphicsContext.UpdateBufferData(...) method: var vertexData = new Vector4[] { new Vector4(0.0f, 0.2f, 0.0f, 1.0f), new Vector4(1.0f, 0.0f, 0.0f, 1.0f), new Vector4(0.2f, -0.2f, 0.0f, 1.0f), new Vector4(0.0f, 1.0f, 0.0f, 1.0f), new Vector4(-0.2f, -0.2f, 0.0f, 1.0f), new Vector4(0.0f, 0.0f, 1.0f, 1.0f), }; // Creates a Buffer without data... uint sizeInBytes = (4 * 4) * (uint)vertexData.Length; var bufferDescription = new BufferDescription(sizeInBytes, BufferFlags.VertexBuffer, ResourceUsage.Default); var buffer = this.GraphicsContext.Factory.CreateBuffer(ref bufferDescription); // Update buffer... this.GraphicsContext.UpdateBufferData(buffer, vertexData); How to copy a Default Buffer into another Default Buffer In that case you need to execute the CommandBuffer.CopyBufferDataTo(...) method. To do this, you need to obtains a CommandBuffer instance and enqueue the copy command: var vertexData = new Vector4[] { new Vector4(0.0f, 0.2f, 0.0f, 1.0f), new Vector4(1.0f, 0.0f, 0.0f, 1.0f), new Vector4(0.2f, -0.2f, 0.0f, 1.0f), new Vector4(0.0f, 1.0f, 0.0f, 1.0f), new Vector4(-0.2f, -0.2f, 0.0f, 1.0f), new Vector4(0.0f, 0.0f, 1.0f, 1.0f), }; // Creates the source buffer with some vertex data... var description = new BufferDescription( 4 * 4 * (uint)vertexData.Length, BufferFlags.VertexBuffer, ResourceUsage.Default); var buffer = this.GraphicsContext.Factory.CreateBuffer(vertexData, ref description); // Creates an empty buffer with the same size and properties as before... var bufferCopyDescription = new BufferDescription( (4 * 4) * (uint)vertexData.Length, BufferFlags.VertexBuffer, ResourceUsage.Default); var bufferCopy = this.GraphicsContext.Factory.CreateBuffer(ref bufferCopyDescription); // Creates a CommandBuffer to execute the copy command... var queue = this.GraphicsContext.Factory.CreateCommandQueue(); var command = queue.CommandBuffer(); command.Begin(); // Execute the CopyBufferDataTo() commandBuffer method to copy data from one buffer to another... command.CopyBufferDataTo(buffer, bufferCopy, buffer.Description.SizeInBytes); // Commit and submit the commandBuffer... command.End(); command.Commit(); queue.Submit(); queue.WaitIdle(); buffer.Dispose(); bufferCopy.Dispose(); queue.Dispose(); How to read a Default Buffer content (by using a Staging Buffer) In order to read a Default Buffer, you need to copy previously the content into a Staging Buffer. Once you do this, you could map the Stagging Buffer to CPU Memory and access the data without problems: var vertexData = new Vector4[] { new Vector4(0.0f, 0.2f, 0.0f, 1.0f), new Vector4(1.0f, 0.0f, 0.0f, 1.0f), new Vector4(0.2f, -0.2f, 0.0f, 1.0f), new Vector4(0.0f, 1.0f, 0.0f, 1.0f), new Vector4(-0.2f, -0.2f, 0.0f, 1.0f), new Vector4(0.0f, 0.0f, 1.0f, 1.0f), }; // Create the source buffer with some data... var description = new BufferDescription( 4 * 4 * (uint)vertexData.Length, BufferFlags.VertexBuffer, ResourceUsage.Default); var buffer = this.GraphicsContext.Factory.CreateBuffer(vertexData, ref description); // Creates the staging buffer... var stagingDescription = new BufferDescription( 4 * 4 * (uint)vertexData.Length, BufferFlags.None, ResourceUsage.Staging, // Use Staging as ResourceUsage... ResourceCpuAccess.Read); var stagingBuffer = this.GraphicsContext.Factory.CreateBuffer(ref stagingDescription); // Copy the buffer data like the previous example... var queue = this.GraphicsContext.Factory.CreateCommandQueue(); var command = queue.CommandBuffer(); command.Begin(); command.CopyBufferDataTo(buffer, stagingBuffer, buffer.Description.SizeInBytes); command.End(); command.Commit(); queue.Submit(); queue.WaitIdle(); // To read the buffer data, Map the buffer into the CPU memory... var readableResource = this.GraphicsContext.MapMemory(stagingBuffer, MapMode.Read); // Checks if the staging buffer content is the same as that we use before to create // the default buffer... for (int i = 0; i < vertexData.Length; i++) { Vector4* pointer = (Vector4*)(readableResource.Data + (i * sizeof(Vector4))); Assert.Equal(*pointer, vertexData[i]); } // Unmap the memory to free the CPU Memory resources... this.GraphicsContext.UnmapMemory(stagingBuffer); buffer.Dispose(); stagingBuffer.Dispose(); queue.Dispose(); How to update a Dynamic Buffer from CPU A Dynamic Buffer could be updated directly from CPU. To do this, you only need to map a Buffer and write the data directly to the mapped pointer: var vectorSize = (uint)Unsafe.SizeOf<Vector4>(); var vertexData = new Vector4[] { new Vector4(0.0f, 0.2f, 0.0f, 1.0f), new Vector4(1.0f, 0.0f, 0.0f, 1.0f), new Vector4(0.2f, -0.2f, 0.0f, 1.0f), new Vector4(0.0f, 1.0f, 0.0f, 1.0f), new Vector4(-0.2f, -0.2f, 0.0f, 1.0f), new Vector4(0.0f, 0.0f, 1.0f, 1.0f), }; var dynamicDescription = new BufferDescription( vectorSize * (uint)vertexData.Length, BufferFlags.VertexBuffer, ResourceUsage.Dynamic, ResourceCpuAccess.Write); var dynamicBuffer = this.GraphicsContext.Factory.CreateBuffer(ref dynamicDescription); // Map the write staging and leave mapped... var writableResource = this.GraphicsContext.MapMemory(dynamicBuffer, MapMode.Write); Vector4* pointer = (Vector4*)writableResource.Data; for (int i = 0; i < vertexData.Length; i++) { *pointer = vertexData[i]; pointer++; } // Once the buffer us unmapped, the new buffer content is accesible by the GPU... this.GraphicsContext.UnmapMemory(dynamicBuffer); dynamicBuffer.Dispose();"
  },
  "manual/graphics/low_level_api/commandbuffer.html": {
    "href": "manual/graphics/low_level_api/commandbuffer.html",
    "title": "CommnadBuffer | Evergine Doc",
    "keywords": "CommnadBuffer Coming soon"
  },
  "manual/graphics/low_level_api/commandqueue.html": {
    "href": "manual/graphics/low_level_api/commandqueue.html",
    "title": "CommandQueue | Evergine Doc",
    "keywords": "CommandQueue Coming soon"
  },
  "manual/graphics/low_level_api/computepipeline.html": {
    "href": "manual/graphics/low_level_api/computepipeline.html",
    "title": "ComputePipeline | Evergine Doc",
    "keywords": "ComputePipeline Coming soon"
  },
  "manual/graphics/low_level_api/framebuffer.html": {
    "href": "manual/graphics/low_level_api/framebuffer.html",
    "title": "Framebuffer | Evergine Doc",
    "keywords": "Framebuffer Framebuffers represent a collection of memory attachments that are used by a render pass instance. Examples of these memory attachments include the color textures and depth texture. Creation A framebuffer provides the attachments that a RenderPass needs while rendering. // Create Render Target FrameBuffer var rTColorTargetDescription = new TextureDescription() { Format = PixelFormat.R8G8B8A8_UNorm, Width = rtSize, Height = rtSize, Depth = 1, ArraySize = 1, Faces = 1, Flags = TextureFlags.RenderTarget | TextureFlags.ShaderResource, CpuAccess = ResourceCpuAccess.None, MipLevels = 1, Type = TextureType.Texture2D, Usage = ResourceUsage.Default, SampleCount = TextureSampleCount.None, }; var rTColorTarget = this.graphicsContext.Factory.CreateTexture(ref rTColorTargetDescription); var rTDepthTargetDescription = new TextureDescription() { Format = PixelFormat.D24_UNorm_S8_UInt, Width = rtSize, Height = rtSize, Depth = 1, ArraySize = 1, Faces = 1, Flags = TextureFlags.DepthStencil, CpuAccess = ResourceCpuAccess.None, MipLevels = 1, Type = TextureType.Texture2D, Usage = ResourceUsage.Default, SampleCount = TextureSampleCount.None, }; var rTDepthTarget = this.graphicsContext.Factory.CreateTexture(ref rTDepthTargetDescription); var depthAttachment = new FrameBufferAttachment(rTDepthTarget, 0, 1); var colorsAttachment = new[] { new FrameBufferAttachment(rTColorTarget, 0, 1) }; this.rTFrameBuffer = this.graphicsContext.Factory.CreateFrameBuffer(depthAttachment, colorsAttachment); How to use First, you need to create a graphics pipeline that use the framebuffer output description: var trianglePipelineDescription = new GraphicsPipelineDescription() { PrimitiveTopology = PrimitiveTopology.TriangleList, InputLayouts = triangleVertexLayouts, ResourceLayouts = new[] { triangleResourceLayout }, Shaders = new GraphicsShaderStateDescription() { VertexShader = triangleVertexShader, PixelShader = trianglePixelShader, }, RenderStates = new RenderStateDescription() { RasterizerState = RasterizerStates.None, BlendState = BlendStates.Opaque, DepthStencilState = DepthStencilStates.None, }, Outputs = this.rTFrameBuffer.OutputDescription, }; this.trianglePipelineState = this.graphicsContext.Factory.CreateGraphicsPipeline(ref trianglePipelineDescription); And finally you can use to start a renderpass: // Render to texture var commandBuffer = this.commandQueue.CommandBuffer(); commandBuffer.Begin(); RenderPassDescription renderPassDescription = new RenderPassDescription(this.rTFrameBuffer, new ClearValue(ClearFlags.Target, Color.CornflowerBlue)); commandBuffer.BeginRenderPass(ref renderPassDescription); commandBuffer.SetViewports(this.rTViewports); commandBuffer.SetScissorRectangles(this.scissors); commandBuffer.SetGraphicsPipelineState(this.trianglePipelineState); commandBuffer.SetResourceSet(this.triangleResourceSet); commandBuffer.SetVertexBuffers(this.triangleVertexBuffers); commandBuffer.Draw((uint)this.triangleVertexData.Length); commandBuffer.EndRenderPass(); commandBuffer.End(); commandBuffer.Commit(); this.commandQueue.Submit(); this.commandQueue.WaitIdle();"
  },
  "manual/graphics/low_level_api/graphicscontext.html": {
    "href": "manual/graphics/low_level_api/graphicscontext.html",
    "title": "GraphicsContext | Evergine Doc",
    "keywords": "GraphicsContext The GraphicsContext is the central class for displaying your application. It's used to create and manage graphic resources. Like the majority of Low-level-API classes, GraphicsContext is an abstract class that exposes common functionality of each graphics API (e.g. DirectX, Vulkan, Metal...). To use this API just need to create or get access to the GraphicsContext instance indicating the properly implementation depending of which graphic backend are you interested. In the following example, we are creating a DirectX11 GraphicsContext: var graphicsContext = new Evergine.DirectX11.DX11GraphicsContext(); Use the specify constructor to initialize a concrete graphics API. API Class DirectX 11 new DX11GraphicsContext() DirectX 12 new DX12GraphicsContext() Vulkan new VKGraphicsContext() OpenGL new GLGraphicsContext() Metal new MTLGraphicsContext() Initialize the Device Once you have created the Graphics context, in order to create resources and render your content, you need to create the graphics device: graphicsContext.CreateDevice(); Validation Layer To enable debug graphics mode you must added the ValidationLayer object to the device constructor, this will show you the native and internal errors: // Add a ValidationLayer instance in the CreateDevice invocation... graphicsContext.CreateDevice(new ValidationLayer()); By default, the ValidationLayer use exception to notify any issue, but it is possible to change it: Notify Method Declaration Description Exception new ValidationLayer() Throws exception with each internal error and stop the execution. Trace new ValidationLayer(ValidationLayer.NotifyMethod.Trace) Display all errors in console without stopping the execution Event new ValidationLayer(ValidationLayer.NotifyMethod.Event) The ValidationLayer.Error event allow to obtains the error messages Initialize Swapchain Once you have the GraphicsContext you can use it to create the swapchain and use it to render on a surface. // Create a windows... var windowSystem = new Evergine.WindowsForms.FormsWindowsSystem(); var window = windowSystem.CreateWindow(windowsTitle, width, height); // Create a swapchain descriptor and assign the surface info... var swapChainDescriptor = new SwapChainDescription() { Width = window.Width, Height = window.Height, SurfaceInfo = info, ColorTargetFormat = PixelFormat.R8G8B8A8_UNorm, ColorTargetFlags = TextureFlags.RenderTarget | TextureFlags.ShaderResource, DepthStencilTargetFormat = PixelFormat.D24_UNorm_S8_UInt, DepthStencilTargetFlags = TextureFlags.DepthStencil, SampleCount = this.SampleCount, IsWindowed = true, RefreshRate = 60, SurfaceInfo = window.SurfaceInfo }; // Finally, create the swapchain... var swapChain = this.graphicsContext.CreateSwapChain(swapChainDescriptor); swapChain.VerticalSync = false; To create the surface first you need to select an UI technology: UI Class Windows Forms Evergine.Forms.FormsWindowsSystem WPF Evergine.WPF.WPFWindowsSystem SDL Evergine.SDL.SDLWindowsSystem Android Evergine.AndroidView.AndroidWindowsSystem iOS Evergine.iOSView.iOSWindowsSystem UWP Evergine.UWPView.UWPWindowsSystem WinUI Evergine.WinUI.WinUIWindowsSystem MixedReality Evergine.MixedReality.MixedRealityWindowsSystem Web Evergine.Web.WebWindowsSystem Create from scratch Visit the Low-Level test samples to learn how to create an application from scratch using this cross-platform API."
  },
  "manual/graphics/low_level_api/graphicspipeline.html": {
    "href": "manual/graphics/low_level_api/graphicspipeline.html",
    "title": "GraphicsPipeline | Evergine Doc",
    "keywords": "GraphicsPipeline Coming soon"
  },
  "manual/graphics/low_level_api/index.html": {
    "href": "manual/graphics/low_level_api/index.html",
    "title": "Low-level API | Evergine Doc",
    "keywords": "Low-level API Evergine uses a custom low level graphics API to send commands to the GPU. This is a cross-platform agnostic library to run on top of DirectX, Vulkan, OpenGL and Metal. In this section we will take a look to all the classes and objects used by Evergine to share data with the GPU. It is a new graphics API inspired on the latest advances on DirectX 12, Vulkan and Metal to get the maximum graphics performance when using once of this APIs but also with backward compatibility to support APIs like DirectX 11, OpenGL and WebGL. In this section GraphicsContext ResourceFactory Buffer Texture Sampler Swapchain Framebuffer QueryHeap"
  },
  "manual/graphics/low_level_api/queryheap.html": {
    "href": "manual/graphics/low_level_api/queryheap.html",
    "title": "QueryHeap | Evergine Doc",
    "keywords": "QueryHeap A query heap contains an array of GPU queries. A query heap allows batch a set of GPU queries to get better performance. Creation To create a QueryHeap, first you need to construct a QueryHeapDescription: QueryHeap queryHeap; uint maxQueries = 4; QueryHeapDescription desc = new QueryHeapDescription() { Type = QueryType.Timestamp, QueryCount = maxQueries, }; this.queryHeap = this.graphicsContext.Factory.CreateQueryHeap(ref desc); QueryType Value Description Timestamp Indicates the query is for high definition GPU and CPU timestamps. Occlusion Indicates the query is for depth/stencil occlusion counts. BinaryOcclusion Indicates the query is for a binary depth/stencil occlusion statistics. Timestamp queries You can obtain timestamps as part of a command list (rather than a CPU-side call on a command queue) via timestamp queries. How to use timestamp queries ulong[] results; var commandBuffer = this.commandQueue.CommandBuffer(); commandBuffer.Begin(); commandBuffer.WriteTimestamp(this.queryHeap, 0); commandBuffer.UpdateBufferData(this.constantBuffer, ref worldViewProj); commandBuffer.SetViewports(this.viewports); commandBuffer.SetScissorRectangles(this.scissors); var renderPassDescription = new RenderPassDescription(this.frameBuffer, ClearValue.Default); commandBuffer.BeginRenderPass(ref renderPassDescription); commandBuffer.SetGraphicsPipelineState(this.pipelineState); commandBuffer.SetResourceSet(this.resourceSet); commandBuffer.SetVertexBuffers(this.vertexBuffers); commandBuffer.Draw((uint)this.vertexData.Length / 2); commandBuffer.EndRenderPass(); commandBuffer.WriteTimestamp(this.queryHeap, 1); commandBuffer.End(); commandBuffer.Commit(); this.commandQueue.Submit(); this.commandQueue.WaitIdle(); this.queryHeap.ReadData(0, 4, this.results); How to show timestamp results this.surface.MouseDispatcher.DispatchEvents(); this.surface.KeyboardDispatcher.DispatchEvents(); commandBuffer.SetViewports(this.viewports); this.uiRenderer.NewFrame(gameTime); double gpuFrequency = this.graphicsContext.TimestampFrequency; double time1 = ((this.results[1] - this.results[0]) / gpuFrequency) * 1000.0; double time2 = ((this.results[3] - this.results[2]) / gpuFrequency) * 1000.0; ImGui.SetNextWindowSize(new System.Numerics.Vector2(300, 100)); ImGui.Begin(\"Timmings\"); ImGui.Text($\"Draw: { time1.ToString(\"0.0000\") } ms\"); ImGui.Text($\"ImGui: { time2.ToString(\"0.0000\") } ms\"); ImGui.End(); this.uiRenderer.Render(commandBuffer); Occlusion queries Hardware occlusion queries were one of the most eagerly awaited graphics hardware features in a long time. This feature makes it possible for an application to ask the 3D API whether or not any pixels would be drawn if a particular object were rendered. With this feature, applications can check whether or not the bounding boxes of complex objects are visible; if the bounds are occluded, the application can skip drawing those objects. QueryHeap creation uint maxQueries = 4; QueryHeapDescription desc = new QueryHeapDescription() { Type = QueryType.Occlusion, QueryCount = maxQueries, }; var queryHeap = this.graphicsContext.Factory.CreateQueryHeap(ref desc); How to use occlusion queries // Draw var commandBuffer = this.commandQueue.CommandBuffer(); commandBuffer.Begin(); commandBuffer.UpdateBufferData(this.constantBuffer0, ref viewProj); commandBuffer.UpdateBufferData(this.constantBuffer1, ref worldViewProj); commandBuffer.SetViewports(this.viewports); commandBuffer.SetScissorRectangles(this.scissors); var renderPassDescription = new RenderPassDescription(this.frameBuffer, ClearValue.Default); commandBuffer.BeginRenderPass(ref renderPassDescription); commandBuffer.SetGraphicsPipelineState(this.pipelineState); commandBuffer.SetResourceSet(this.resourceSet0); commandBuffer.SetVertexBuffers(this.vertexBuffers); commandBuffer.BeginQuery(this.queryHeap, 0); commandBuffer.Draw((uint)this.vertexData.Length / 2); commandBuffer.EndQuery(this.queryHeap, 0); commandBuffer.EndRenderPass(); commandBuffer.End(); commandBuffer.Commit(); this.commandQueue.Submit(); this.commandQueue.WaitIdle(); this.queryHeap.ReadData(0, 1, this.results); How to show occlusion results this.surface.MouseDispatcher.DispatchEvents(); this.surface.KeyboardDispatcher.DispatchEvents(); commandBuffer.SetViewports(this.viewports); this.uiRenderer.NewFrame(gameTime); ImGui.SetNextWindowSize(new System.Numerics.Vector2(300, 100)); ImGui.Begin(\"Occlusion Test\"); ImGui.Text($\"Samples: { this.results[0] } \"); ImGui.End(); this.uiRenderer.Render(commandBuffer);"
  },
  "manual/graphics/low_level_api/raytracingpipeline.html": {
    "href": "manual/graphics/low_level_api/raytracingpipeline.html",
    "title": "RaytracingPipeline | Evergine Doc",
    "keywords": "RaytracingPipeline Coming soon"
  },
  "manual/graphics/low_level_api/resourcefactory.html": {
    "href": "manual/graphics/low_level_api/resourcefactory.html",
    "title": "ResourceFactory | Evergine Doc",
    "keywords": "ResourceFactory ResourceFactory is the factory class responsible to create all low-level objects like a native Texture, Sampler, GraphicsPipeline among other things... Once you have the GraphicsContext instance you can use it to access to the ResourceFactory and start creating Low-Level objects. In this example, to create a VertexBuffer object: var vertexBufferDescription = new BufferDescription((uint)Unsafe.SizeOf<VertexPositionNormalTexture>() * (uint)vertexData.Length, BufferFlags.VertexBuffer, ResourceUsage.Default); var vertexBuffer = this.graphicsContext.Factory.CreateBuffer(vertexData, ref vertexBufferDescription); All the common Low-Level objects (textures, buffer, ...) are represented by abstract classes, and ResourceFactory allows you to create them in the same way for all the supported APIs. So, for example, a ResourceFactory of DX11GraphicsContext always will create DX11 graphics resources (DX11Texture, DX11Buffer and so on). Objects The complete list of objects that you can create using the ResourceFactory are: Buffer Texture Sampler Framebuffer Shader QueryHeap ResourceLayout GraphicsPipeline ComputePipeline RayTracingPipeline ResourceSet CommandQueue CommandBuffer"
  },
  "manual/graphics/low_level_api/resourcelayout.html": {
    "href": "manual/graphics/low_level_api/resourcelayout.html",
    "title": "ResourceLayout | Evergine Doc",
    "keywords": "ResourceLayout Coming soon"
  },
  "manual/graphics/low_level_api/resourceset.html": {
    "href": "manual/graphics/low_level_api/resourceset.html",
    "title": "ResourceSet | Evergine Doc",
    "keywords": "ResourceSet Coming soon"
  },
  "manual/graphics/low_level_api/sampler.html": {
    "href": "manual/graphics/low_level_api/sampler.html",
    "title": "Sampler | Evergine Doc",
    "keywords": "Sampler A Sampler State is a low level object that encapsulates how a texture will be sampled in your application. Creation To create a sampler, first you need to create the SamplerStateDescription struct: // Linear clamp sampler state var samplerDescription = new SamplerStateDescription() { Filter = TextureFilter.MinLinear_MagLinear_MipLinear, AddressU = TextureAddressMode.Clamp, AddressV = TextureAddressMode.Clamp, AddressW = TextureAddressMode.Clamp, MinLOD = -1000, // DirectX -float.MaxValue | OpenGL -1000 MaxLOD = 1000, // DirectX float.MaxValue | OpenGL 1000 MipLODBias = 0f, MaxAnisotropy = 1, ComparisonFunc = ComparisonFunction.Never, BorderColor = SamplerBorderColor.OpaqueWhite, }; var samplerState = this.graphicsContext.Factory.CreateSamplerState(ref samplerDescription); SamplerStateDescription Property Type Description Filter TextureFilter Filtering method to use when sampling a texture. AddressU TextureAddressMode Method to use for resolving a u texture coordinate that is outside the 0 to 1 range. AddressV TextureAddressMode Method to use for resolving a v texture coordinate that is outside the 0 to 1 range. AddressW TextureAddressMode Method to use for resolving a w texture coordinate that is outside the 0 to 1 range. MipLODBias float Offset from the calculated mipmap level. For example, if Direct3D calculates that a texture should be sampled at mipmap level 3 and MipLODBias is 2, then the texture will be sampled at mipmap level 5. MaxAnisotropy uint Clamping value used if D3D11_FILTER_ANISOTROPIC or D3D11_FILTER_COMPARISON_ANISOTROPIC is specified in Filter. Valid values are between 1 and 16. ComparisonFunc ComparisonFunction A function that compares sampled data against existing sampled data. BorderColor SamplerBorderColor Border color. MinLOD float Lower end of the mipmap range to clamp access to, where 0 is the largest and most detailed mipmap level and any level higher than that is less detailed. MaxLOD float Upper end of the mipmap range to clamp access to, where 0 is the largest and most detailed mipmap level and any level higher than that is less detailed. This value must be greater than or equal to MinLOD. TextureFilter Defines texture filtering modes for a texture stage. Value Description MinPoint_MagPoint_MipPoint Use point sampling for minification, magnification, and mip-level sampling. MinPoint_MagPoint_MipLinear Use point sampling for minification and magnification; use linear interpolation for mip-level sampling. MinPoint_MagLinear_MipPoint Use point sampling for minification; use linear interpolation for magnification; use point sampling for mip-level sampling. MinPoint_MagLinear_MipLinear Use point sampling for minification; use linear interpolation for magnification and mip-level sampling. MinLinear_MagPoint_MipPoint Use linear interpolation for minification; use point sampling for magnification and mip-level sampling. MinLinear_MagPoint_MipLinear Use linear interpolation for minification; use point sampling for magnification; use linear interpolation for mip-level sampling. MinLinear_MagLinear_MipPoint Use linear interpolation for minification and magnification; use point sampling for mip-level sampling. MinLinear_MagLinear_MipLinear Use linear interpolation for minification, magnification, and mip-level sampling. Anisotropic Use anisotropic interpolation for minification, magnification, and mip-level sampling. TextureAddressMode Your application can assign texture coordinates to any vertex of any primitive. Typically, the u- and v-texture coordinates that you assign to a vertex are in the range of 0.0 to 1.0 inclusive. However, by assigning texture coordinates outside that range, you can create certain special texturing effects. Value Description Wrap Tile the texture at every (u,v) integer junction. For example, for u values between 0 and 3, the texture is repeated three times. Mirror Flip the texture at every (u,v) integer junction. For u values between 0 and 1. Clamp Texture coordinates outside the range [0.0, 1.0] are set to the texture color at 0.0 or 1.0, respectively. Border Texture coordinates outside the range [0.0, 1.0] are set to the border color specified in SamplerStateDescription. Mirror_One Takes the absolute value of the texture coordinate (thus, mirroring around 0), and then clamps to the maximum value. MaxAnisotropy Retrieves a value that indicates the maximum valid value for anisotropic filtering. Valid values are between 1 and 16. Presets To make the sampler construction easy, you can use Evergine.Common.SamplerStates to describe the most common sampler descriptions: Value PointClamp PointWrap PointMirror LinearClamp LinearWrap LinearMirror AnisotropicClamp AnisotropicWrap AnisotropicMirror Default initialization public void SetDefault() { this.Filter = TextureFilter.MinLinear_MagLinear_MipLinear; this.AddressU = TextureAddressMode.Clamp; this.AddressV = TextureAddressMode.Clamp; this.AddressW = TextureAddressMode.Clamp; this.MinLOD = -1000; // DirectX -float.MaxValue | OpenGL -1000 this.MaxLOD = 1000; // DirectX float.MaxValue | OpenGL 1000 this.MipLODBias = 0f; this.MaxAnisotropy = 1; this.ComparisonFunc = ComparisonFunction.Never; this.BorderColor = SamplerBorderColor.OpaqueWhite; } Presets initialization PointClamp = SamplerStateDescription.Default; PointClamp.Filter = TextureFilter.MinPoint_MagPoint_MipPoint; PointWrap = SamplerStateDescription.Default; PointClamp.Filter = TextureFilter.MinPoint_MagPoint_MipPoint; PointWrap.AddressU = TextureAddressMode.Wrap; PointWrap.AddressV = TextureAddressMode.Wrap; PointWrap.AddressW = TextureAddressMode.Wrap; PointMirror = SamplerStateDescription.Default; PointClamp.Filter = TextureFilter.MinPoint_MagPoint_MipPoint; PointMirror.AddressU = TextureAddressMode.Mirror; PointMirror.AddressV = TextureAddressMode.Mirror; PointMirror.AddressW = TextureAddressMode.Mirror; LinearClamp = SamplerStateDescription.Default; LinearWrap = SamplerStateDescription.Default; LinearWrap.AddressU = TextureAddressMode.Wrap; LinearWrap.AddressV = TextureAddressMode.Wrap; LinearWrap.AddressW = TextureAddressMode.Wrap; LinearMirror = SamplerStateDescription.Default; LinearMirror.AddressU = TextureAddressMode.Mirror; LinearMirror.AddressV = TextureAddressMode.Mirror; LinearMirror.AddressW = TextureAddressMode.Mirror; AnisotropicClamp = SamplerStateDescription.Default; AnisotropicClamp.Filter = TextureFilter.Anisotropic; AnisotropicWrap = SamplerStateDescription.Default; AnisotropicWrap.Filter = TextureFilter.Anisotropic; AnisotropicWrap.AddressU = TextureAddressMode.Wrap; AnisotropicWrap.AddressV = TextureAddressMode.Wrap; AnisotropicWrap.AddressW = TextureAddressMode.Wrap; AnisotropicMirror = SamplerStateDescription.Default; AnisotropicMirror.Filter = TextureFilter.Anisotropic; AnisotropicMirror.AddressU = TextureAddressMode.Mirror; AnisotropicMirror.AddressV = TextureAddressMode.Mirror; AnisotropicMirror.AddressW = TextureAddressMode.Mirror;"
  },
  "manual/graphics/low_level_api/shader.html": {
    "href": "manual/graphics/low_level_api/shader.html",
    "title": "Shader | Evergine Doc",
    "keywords": "Shader Coming soon"
  },
  "manual/graphics/low_level_api/swapchain.html": {
    "href": "manual/graphics/low_level_api/swapchain.html",
    "title": "Swapchain | Evergine Doc",
    "keywords": "Swapchain A swap chain is a collection of buffers that are used for displaying frames to the user. Each time an application presents a new frame for display, the first buffer in the swap chain takes the place of the displayed buffer. This process is called swapping or flipping. A graphics adapter holds a pointer to a surface that represents the image being displayed on the monitor, called a front buffer. As the monitor is refreshed, the graphics card sends the contents of the front buffer to the monitor to be displayed. However, this leads to a problem when rendering real-time graphics. The heart of the problem is that monitor refresh rates are very slow in comparison to the rest of the computer. Common refresh rates range from 60 Hz (60 times per second) to 100 Hz. If your application is updating the front buffer while the monitor is in the middle of a refresh, the image that is displayed will be cut in half with the upper half of the display containing the old image and the lower half containing the new image. This problem is referred to as tearing. Creation To create a Swapchain, first you need to create the SwapChainDescription struct: // Create a windows... var windowSystem = new Evergine.WindowsForms.FormsWindowsSystem(); var window = windowSystem.CreateWindow(windowsTitle, width, height); // Create a swapchain descriptor and assign the surface info... var swapChainDescriptor = new SwapChainDescription() { Width = window.Width, Height = window.Height, SurfaceInfo = info, ColorTargetFormat = PixelFormat.R8G8B8A8_UNorm, ColorTargetFlags = TextureFlags.RenderTarget | TextureFlags.ShaderResource, DepthStencilTargetFormat = PixelFormat.D24_UNorm_S8_UInt, DepthStencilTargetFlags = TextureFlags.DepthStencil, SampleCount = this.SampleCount, IsWindowed = true, RefreshRate = 60, SurfaceInfo = window.SurfaceInfo }; // Finally, create the swapchain... var swapChain = this.graphicsContext.CreateSwapChain(swapChainDescriptor); swapChain.VerticalSync = false; SwapChainDescription Property Type Description SurfaceInfo SurfaceInfo Surface information. Width uint The swapchain buffers width. Height uint The swapchain buffers height. RefreshRate uint The screen refresh rate. ColorTargetFormat PixelFormat The pixel format of the color target. ColorTargetFlags TextureFlags The color texture flags for binding to pipeline stages. The flags ca be combined by a logical OR. DepthStencilTargetFormat PixelFormat The pixel format of the depthstencil target. DepthStencilTargetFlags TextureFlags The depth texture flags for binding to pipeline stages. The flags ca be combined by a logical OR. SampleCount TextureSampleCount The sampler count of this swapchain. IsWindowed bool Whether the output is in windowed mode. TextureFlags Identifies how to bing a texture. TextureFlags Description None Not specified, Default value. ShaderResource A texture usable as a ShaderResourceView. RenderTarget A texture usable as render target. UnorderedAccess A texture usable as an unordered access buffer. DepthStencil A texture usable as a depth stencil buffer. GenerateMipmaps Enables MIP map generation by GPU. TextureSampleCount Describes the number of samples to use in a Texture. TextureSampleCount Description None Not multisample. Default value. Count2 Multisample count of 2 pixels. Count4 Multisample count of 4 pixels. Count8 Multisample count of 8 pixels. Count16 Multisample count of 16 pixels. Count32 Multisample count of 32 pixels."
  },
  "manual/graphics/low_level_api/texture.html": {
    "href": "manual/graphics/low_level_api/texture.html",
    "title": "Texture | Evergine Doc",
    "keywords": "Texture A Texture object in a low-level API is a 2D (even 1D and 3d textures exists) used to provide details to objects or to map information. Please, reade the Graphics Texture section for high level asset and how to use in Evergine Studio. Creation To create a texture, first you need to create the TextureDescription struct: uint expectedSize = 256; ResourceUsage expectedUsage = ResourceUsage.Default; var description = new TextureDescription() { Type = TextureType.Texture2D, Width = expectedSize, Height = expectedSize, Depth = 1, ArraySize = 1, Faces = 1, Usage = expectedUsage, CpuAccess = ResourceCpuAccess.None, Flags = TextureFlags.None, Format = expectedFormat, MipLevels = 1, SampleCount = TextureSampleCount.None, }; var texture = this.GraphicsContext.Factory.CreateTexture(ref description); TextureDescription Property Type Description Type TextureType Texture type TextureType. Format PixelFormat Texture format PixelFormat. Width uint Texture width (in texels). Height uint Texture height (in texels). Depth uint Texture Depth (in texels). ArraySize uint Number of textures in the texture array. Faces uint Number of texture faces useful in TextureCube and TextureCubeArray. MipLevels uint The maximum number of mipmap levels in the texture. Flags TextureFlags The texture flags TextureFlags. Usage ResourceUsage Value that identifies how the texture is to be read from and written to. SampleCount TextureSampleCount The number of samples in this texture. CpuAccess ResourceCpuAccess Flags ResourceCpuAccess to specify the type of CPU access allowed. TextureType Specify the texture type. TextureType Description Texture2D Represent a two dimensions texture. Texture2DArray Represent an array of 2D textures. Texture1D Represent a one dimension texture. Texture1DArray Represent an array of 1D textures. TextureCube Represent a 6 faces texture cube. TextureCubeArray Represent an array of textures cube. Texture3D Represent a three dimensions texture. PixelFormat Specify the bytes format used in each texel. The most common format are: PixelFormat Description R8_UNorm A single-component, 8-bit unsigned-normalized-integer format that supports 8 bits for the red channel. R8G8_UNorm A two-component, 16-bit unsigned-normalized-integer format that supports 8 bits for the red channel and 8 bits for the green channel. R8G8B8A8_UNorm A four-component, 32-bit unsigned-normalized-integer format that supports 8 bits per channel including alpha. R8G8B8A8_UNorm_SRgb A four-component, 32-bit unsigned-normalized integer sRGB format that supports 8 bits per channel including alpha. R16_Float A single-component, 16-bit floating-point format that supports 16 bits for the red channel. R16_SInt A single-component, 16-bit signed-integer format that supports 16 bits for the red channel. R16_UInt A single-component, 16-bit unsigned-integer format that supports 16 bits for the red channel. R16_UNorm A single-component, 16-bit unsigned-normalized-integer format that supports 16 bits for the red channel. R16G16_Float A two-component, 32-bit floating-point format that supports 16 bits for the red channel and 16 bits for the green channel. R16G16_SInt A two-component, 32-bit signed-integer format that supports 16 bits for the red channel and 16 bits for the green channel. R16G16_UInt A two-component, 32-bit unsigned-integer format that supports 16 bits for the red channel and 16 bits for the green channel. R16G16B16A16_Float A four-component, 64-bit floating-point format that supports 16 bits per channel including alpha. R16G16B16A16_SInt A four-component, 64-bit signed-integer format that supports 16 bits per channel including alpha. R16G16B16A16_UInt A four-component, 64-bit unsigned-integer format that supports 16 bits per channel including alpha. R32_Float A single-component, 32-bit floating-point format that supports 32 bits for the red channel. R32_SInt A single-component, 32-bit signed-integer format that supports 32 bits for the red channel. R32_UInt A single-component, 32-bit unsigned-integer format that supports 32 bits for the red channel. R32G32_Float A two-component, 64-bit floating-point format that supports 32 bits for the red channel and 32 bits for the green channel. R32G32_SInt A two-component, 64-bit signed-integer format that supports 32 bits for the red channel and 32 bits for the green channel. R32G32_UInt A two-component, 64-bit unsigned-integer format that supports 32 bits for the red channel and 32 bits for the green channel. R32G32B32_Float A three-component, 96-bit floating-point format that supports 32 bits per color channel. R32G32B32_SInt A three-component, 96-bit signed-integer format that supports 32 bits per color channel. R32G32B32_UInt A three-component, 96-bit unsigned-integer format that supports 32 bits per color channel. R32G32B32A32_Float A four-component, 128-bit floating-point format that supports 32 bits per channel including alpha. 1 R32G32B32A32_SInt A four-component, 128-bit signed-integer format that supports 32 bits per channel including alpha. 1 R32G32B32A32_UInt A four-component, 128-bit unsigned-integer format that supports 32 bits per channel including alpha. 1 Note See the PixelFormat enum at Evergine.Common namespace for the complete list. TextureFlags Identifies how to bing a texture. TextureFlags Description None Not specified, Default value. ShaderResource A texture usable as a ShaderResourceView. RenderTarget A texture usable as render target. UnorderedAccess A texture usable as an unordered access buffer. DepthStencil A texture usable as a depth stencil buffer. GenerateMipmaps Enables MIP map generation by GPU. ResourceUsage Identifies expected resource usage during rendering. ResourceUsage Description Default A resource that requires read and write access by the GPU, Default value. Immutable A resource that can only be read by the GPU. It cannot be written by the GPU, and cannot be accessed at all by the CPU. Dynamic A resource that is accessible by both the GPU (read only) and the CPU (write only). Staging A resource that supports data transfer (copy) from the GPU to the CPU. TextureSampleCount Describes the number of samples to use in a Texture. TextureSampleCount Description None Not multisample. Default value. Count2 Multisample count of 2 pixels. Count4 Multisample count of 4 pixels. Count8 Multisample count of 8 pixels. Count16 Multisample count of 16 pixels. Count32 Multisample count of 32 pixels. ResourceCpuAccess Specifies the types of CPU access allowed for a resource. ResourceCpuAccess Description None Not specified, Default value. Write The CPU can be write this resource. Read The CPU can be read this resources. Usage examples How to create and fill a texture by code uint expectedSize = 256; var description = new TextureDescription() { Type = TextureType.Texture2D, Width = expectedSize, Height = expectedSize, Depth = 1, ArraySize = 1, Faces = 1, Usage = ResourceUsage.Default, CpuAccess = ResourceCpuAccess.None, Flags = TextureFlags.None, Format = PixelFormat.R8G8B8A8_UNorm, MipLevels = 1, SampleCount = TextureSampleCount.None, }; float[] data = Enumerable.Range(0, (int)(expectedSize * expectedSize)).Select(i => (float)i).ToArray(); var rowPitch = Common.Graphics.Helpers.GetRowPitch(expectedSize, PixelFormat.R8G8B8A8_UNorm); var slicePitch = Common.Graphics.Helpers.GetSlicePitch(rowPitch, expectedSize, PixelFormat.R8G8B8A8_UNorm); var pinnedHandle = GCHandle.Alloc(data, GCHandleType.Pinned); IntPtr dataPointer = Marshal.UnsafeAddrOfPinnedArrayElement(data, 0); var databox = new DataBox[] { new DataBox(dataPointer, rowPitch, slicePitch) }; var texture = this.GraphicsContext.Factory.CreateTexture(databox, ref description); pinnedHandle.Free(); How to update a default texture uint expectedSize = 256; ResourceUsage expectedUsage = ResourceUsage.Default; PixelFormat expectedFormat = PixelFormat.R8G8B8A8_UNorm; var description = new TextureDescription() { Type = TextureType.Texture2D, Width = expectedSize, Height = expectedSize, Depth = 1, ArraySize = 1, Faces = 1, Usage = expectedUsage, CpuAccess = ResourceCpuAccess.None, Flags = TextureFlags.None, Format = expectedFormat, MipLevels = 1, SampleCount = TextureSampleCount.None, }; var texture = this.GraphicsContext.Factory.CreateTexture(ref description); float[] data = Enumerable.Range(0, (int)(expectedSize * expectedSize)).Select(i => (float)i).ToArray(); this.GraphicsContext.UpdateTextureData(texture, data); texture.Dispose(); How to copy from other device texture uint expectedSize = 256; var description = new TextureDescription() { Type = TextureType.Texture2D, Width = expectedSize, Height = expectedSize, Depth = 1, ArraySize = 1, Faces = 1, Usage = ResourceUsage.Default, CpuAccess = ResourceCpuAccess.None, Flags = TextureFlags.None, Format = PixelFormat.R8G8B8A8_UNorm, MipLevels = 1, SampleCount = TextureSampleCount.None, }; var texture = this.GraphicsContext.Factory.CreateTexture(ref description); float[] data = Enumerable.Range(0, 256 * 256).Select(i => (float)i).ToArray(); this.GraphicsContext.UpdateTextureData(texture, data); var textureCopy = this.GraphicsContext.Factory.CreateTexture(ref description); var queue = this.GraphicsContext.Factory.CreateCommandQueue(); var command = queue.CommandBuffer(); command.Begin(); command.CopyTextureDataTo(texture, textureCopy); command.End(); command.Commit(); queue.Submit(); queue.WaitIdle(); texture.Dispose(); textureCopy.Dispose(); queue.Dispose(); How to set data in a staging texture uint expectedSize = 256; var description = new TextureDescription() { Type = TextureType.Texture2D, Width = expectedSize, Height = expectedSize, Depth = 1, ArraySize = 1, Faces = 1, Usage = ResourceUsage.Staging, CpuAccess = ResourceCpuAccess.Write | ResourceCpuAccess.Read, Flags = TextureFlags.None, Format = PixelFormat.R8G8B8A8_UNorm, MipLevels = 1, SampleCount = TextureSampleCount.None, }; var texture = this.GraphicsContext.Factory.CreateTexture(ref description); float[] data = Enumerable.Range(0, (int)(expectedSize * expectedSize)).Select(i => (float)i).ToArray(); this.GraphicsContext.UpdateTextureData(texture, data); texture.Dispose(); How to map and read a staging texture uint expectedSize = 256; var description = new TextureDescription() { Type = TextureType.Texture2D, Width = expectedSize, Height = expectedSize, Depth = 1, ArraySize = 1, Faces = 1, Usage = ResourceUsage.Staging, CpuAccess = ResourceCpuAccess.Write | ResourceCpuAccess.Read, Flags = TextureFlags.None, Format = PixelFormat.R8G8B8A8_UNorm, MipLevels = 1, SampleCount = TextureSampleCount.None, }; var texture = this.GraphicsContext.Factory.CreateTexture(ref description); float[] data = Enumerable.Range(0, (int)(expectedSize * expectedSize)).Select(i => (float)i).ToArray(); this.GraphicsContext.UpdateTextureData(texture, data); var mappedResource = this.GraphicsContext.MapMemory(texture, MapMode.Read); for (int y = 0; y < expectedSize; y++) { for (int x = 0; x < expectedSize; x++) { int offset = ((y * ((int)mappedResource.RowPitch / sizeof(float))) + x) * sizeof(float); float* pointer = (float*)(mappedResource.Data + offset); int index = (y * (int)expectedSize) + x; Assert.Equal(data[index], *pointer); } } this.GraphicsContext.UnmapMemory(texture); texture.Dispose();"
  },
  "manual/graphics/materials/create_materials.html": {
    "href": "manual/graphics/materials/create_materials.html",
    "title": "Create Materials | Evergine Doc",
    "keywords": "Create Materials Materials describe the appearance of object surfaces and how they react to light. Create a Material asset in Evergine Studio You can create a material click button on from Assets Details panel to deploy a create menu options and click on the option \"Create material\" Inspect materials in Asset Details You can find the material assets in the Assets Details panel when you select a folder in the Project Explorer. Material files in content directory The material file has the .wemt extension. Create a new Material from code The following sample code can be used to create a new material and apply to an entity in your scene. In that case the material will be created using the StandardEffect effect and the Opaque render layer: protected override void CreateScene() { var assetsService = Application.Current.Container.Resolve<AssetsService>(); // Load the effect... Effect standardEffect = assetsService.Load<Effect>(EvergineContent.Effects.StandardEffect); // Load a Render Layer description... RenderLayerDescription layer = assetsService.Load<RenderLayerDescription>(EvergineContent.RenderLayers.Opaque); // Create your own material... Material material = new Material(standardEffect); material.LayerDescription = layer; // Apply to an entity Entity primitive = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new SphereMesh()) .AddComponent(new MeshRenderer()); this.Managers.EntityManager.Add(primitive); }"
  },
  "manual/graphics/materials/index.html": {
    "href": "manual/graphics/materials/index.html",
    "title": "Materials | Evergine Doc",
    "keywords": "Materials Materials describe the appearance of object surfaces and how they react to light. This allows you to simulate properties like roughness, reflection, specular to create realistic materials of the real world as metal, plastic, concrete, etc. Matertial and Effects The materials are based on an Effect so you first need to create one or use an existing Effect. While an effect defines all the properties and possibilities, a Material sets specific values for each property defined in the associated effect. Default Materials Default Evergine project template imports the Evergine.Core package and this package includes the Default Material that you can use to simulate a large amount of surfaces. Materials are a type of asset and have a dedicated Editor Material Editor. In this section Create Materials Using Materials Material Editor Material Decorators"
  },
  "manual/graphics/materials/material_decorators.html": {
    "href": "manual/graphics/materials/material_decorators.html",
    "title": "Material Decorators | Evergine Doc",
    "keywords": "Material Decorators Material Decorator is a C# class that simplify the use of custom materials to the users. It allows to define how the effect resource layout elements are shown in the Material Editor. To generate material decorator of your effects you need to use the Effect Editor. Generate Material Decorator From the Effect Editor toolbox push the button to generate the Material decorator. A new c# class will be created in your C# solution that you will see from Visual Studio Example Parting from the following effect resource layout block (This is the section marked in the effect code with the [Begin_ResourceLayout] and [End_ResourceLayout] tags): [Begin_ResourceLayout] cbuffer PerDrawCall : register(b0) { float4x4 WorldViewProj : packoffset(c0); [WorldViewProjection] }; cbuffer Parameters : register(b1) { float3 Color : packoffset(c0); [Default(0.3, 0.3, 1.0)] }; [End_ResourceLayout] The generated Material Decorator will be the following: //------------------------------------------------------------------------------ // <auto-generated> // This code was generated by a tool. // // Changes to this file may cause incorrect behavior and will be lost if // the code is regenerated. // </auto-generated> //------------------------------------------------------------------------------ namespace DocumentationWorkBench.Effects { using Evergine.Common.Graphics; using Evergine.Framework.Graphics; using Evergine.Framework.Graphics.Effects; using Evergine.Mathematics; [Evergine.Framework.Graphics.MaterialDecoratorAttribute(\"67d3f67f-e1f0-4075-894d-5a58d3697fb6\")] public partial class MyGraphicEffect : Evergine.Framework.Graphics.MaterialDecorator { public MyGraphicEffect(Evergine.Framework.Graphics.Effects.Effect effect) : base(new Material(effect)) {} public MyGraphicEffect(Evergine.Framework.Graphics.Material material) : base(material) {} public Evergine.Mathematics.Matrix4x4 PerDrawCall_WorldViewProj { get { return this.material.CBuffers[0].GetBufferData<Evergine.Mathematics.Matrix4x4>(0);} set { this.material.CBuffers[0].SetBufferData(value, 0); } } public Evergine.Mathematics.Vector3 Parameters_Color { get { return this.material.CBuffers[1].GetBufferData<Evergine.Mathematics.Vector3>(0); } set { this.material.CBuffers[1].SetBufferData(value, 0); } } } } The above Material decorator example shows how the effect resource layout is translated to get/set class properties to make easier use of the effect from code. Now when you open a material asset from Material Editor and the material uses MyGraphicEffect its properties as: Customize Material Decorators Editing the material decorator c# class you can modify how the properties are shown in the Material Editor. For example, the Color property appears as Vector3 property because this is the type using in the HLSL effect code. You can change it to use a Color picker to configure this parameter with the following code change: public Evergine.Common.Graphics.Color Parameters_Color { get { Vector3 v = this.material.CBuffers[1].GetBufferData<Vector3>(0); return Color.FromVector3(ref v); } set { this.material.CBuffers[1].SetBufferData(value.ToVector3(), 0); } } After this change, if you reload the project in the Evergine Studio, the Material Editor shows its color property as:"
  },
  "manual/graphics/materials/material_editor.html": {
    "href": "manual/graphics/materials/material_editor.html",
    "title": "Material Editor | Evergine Doc",
    "keywords": "Material Editor Material Editor allows editing the material assets. Double click over a material asset shown in Assets Details will open this editor. The editor is composed of 3 main parts: Viewport Shows the result of the current material configuration. The viewport has a toolbox on the top side that allows change the primitive shown or change the background color. The primitive displays in the viewport are lighting by two lights (front and back) that you can move using the mouse. Actions Description Left mouse button To rotate the camera around the primitive. Right mouse button To rotate two lights around the primitive. Mouse wheel To make zoom in/out camera. Effect The properties of the material will be defined by the effect used. So first to edit or configure your material you need the select the effect that you want to use. The Standard effect is the default effect used when you create a new material. Properties The properties panel displays all effect properties or the Material decorator properties associated with the effect. You can configure your materials to change their properties and the result will be shown on the Viewport."
  },
  "manual/graphics/materials/using_materials.html": {
    "href": "manual/graphics/materials/using_materials.html",
    "title": "Using Materials | Evergine Doc",
    "keywords": "Using Materials In this document you will learn how to load and use Materials in your applications. Load Material from code The following sample code can be used to instantiate an existing material asset and apply to an entity in your scene. protected override void CreateScene() { var assetsService = Application.Current.Container.Resolve<AssetsService>(); // Load Material Material defaultMaterial = assetsService.Load<Material>(EvergineContent.Materials.DefaultMaterial); // Apply to an entity Entity primitive = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent() { Material = defaultMaterial }) .AddComponent(new SphereMesh()) .AddComponent(new MeshRenderer()); this.Managers.EntityManager.Add(primitive); } How to apply material to an entity from Evergine Studio. Select an entity from Viewport or Entities Hierarchy and then the Entity Details panel shows the entity component list. To apply a new material set the Material property in the Material Component of your entity."
  },
  "manual/graphics/meshes.html": {
    "href": "manual/graphics/meshes.html",
    "title": "Meshes | Evergine Doc",
    "keywords": "Meshes Coming soon"
  },
  "manual/graphics/models/create_model_from_code.html": {
    "href": "manual/graphics/models/create_model_from_code.html",
    "title": "How to create a model from code | Evergine Doc",
    "keywords": "How to create a model from code Coming soon"
  },
  "manual/graphics/models/import_model.html": {
    "href": "manual/graphics/models/import_model.html",
    "title": "Import a Model | Evergine Doc",
    "keywords": "Import a Model In Evergine Studio, importing an 3d model file will create a Model asset, as explained in this article. Embedded assets Depending of the 3d model file, when importing a Model it may also import other embedded assets. A new folder will be created at the same level than the 3d file, with the name [ModelName]_Embedded (So, for example, the file car.fbx will generate a folder named car_Embedded). Asset Location Description Material [ModelName]_Embedded/Materials The most common one. Every material that the 3d file uses is generated as a specific Material asset and referenced into the Model. Texture [ModelName]_Embedded/Textures Some 3d file formats can embed textures. For every embedded Texture an asset is generated. Sampler [ModelName]_Embedded/Samplers Some 3d file formats specify the sampler states for their textures. In this case Samplers are generated. Inspect Models in Asset Details You can find the model assets in the Assets Details panel when you select a folder in the Project Explorer. The picture shows both the Model asset and the Embededed folder for all the generated assets. Model files in content directory Models imported in Evergine create an aditional metadata .wemd file. Supported formats: Evergine supports the following model formats: Extension Description .gltf, .glb Open source format. Kronos Group created this format for 3D web, AR, VR, games and 3D advertising. It’s the first really well-defined standard for 3D. This file format supports geometry, materials, textures, colors, and animations. This includes PBR (Physical Based Rendering). GLTF is based on JSON, so it stores some data in external files like textures (JPEG or PNG), shaders (GLSL), or geometry and animation data (BIN). GLB files store all this data internally. The Khronos groups is continually improving and updating this file format. It is quickly becoming the standard for 3D shopping. .fbx FBX is a proprietary 3D file format. AutoDesk then purchased them in 2006 and continued supporting FBX. It is widely used in the film and video game industry. It supports geometry, appearance (color and texture), as well as animations (skeletal and morphs). FBX is most popular for animation and is used as an exchange format between different programs like Maya, 3DSMax, AutoCAD, Roman’s CAD, and others. .obj An OBJ file contains information about the geometry of 3D objects. Developers use the file format for exchanging information and in CAD and 3D printing applications. OBJ files can support unlimited colors, and one file can define multiple objects. The current version is 3.0. The objects in an OBJ file are defined by polygon faces and normals, curves, texture maps, and surfaces. OBJ is a vector file, which makes the defined objects scalable. .3ds A 3DS file is a 3D image format used by Autodesk 3D Studio. It contains mesh data, material attributes, bitmap references, smoothing group data, viewport configurations, camera locations, and lighting information. 3DS files may also include object animation data. .dae A DAE file is a Digital Asset Exchange file format that is used for exchanging data between interactive 3D applications. This file format is based on the COLLADA (COLLAborative Design Activity) XML schema which is an open standard XML schema for the exchange of digital assets among graphics software applications."
  },
  "manual/graphics/models/index.html": {
    "href": "manual/graphics/models/index.html",
    "title": "Models | Evergine Doc",
    "keywords": "Models Models are files that contains 3d objects like characters, objects, and all kind of elements. Evergine supports importing models created by 3D editing software (like 3ds Max, Blender or CAD software like Solidworks.) but also supports procedural model generation. Models are crucial assets in every graphical application because can provide high quality and realism. In Evergine a Model is a graphical element that combines multiple parts to conform a full 3D object in a scene. It can contains the following parts: Node Hierarchy. MeshContainers Material references. Skinning information. Animation Clips Node Hierarchy A Model contains a node graph that covers all the objects of the 3d model. They are covered in the NodeContent class. They can have geometry or just serve as dummy element, but all of them contains a transform information (position, orientation and scale) and a list of children. Model contains the hierarchy in these two properties: Property Description AllNodes The list of all the NodeContents, whether they are in the root level or not. RootNodes The indices of the NodeContent elements of the AllNodes list that contains the nodes at root level (they don't have a parent). For example, if we have this node hierarchy: A Model would have them stored in this way: NodeContent The NodeContent refers to the class that represents one node. It contains the basic information of a node of a 3d model: Property Description Name Name of the node Children Array of the NodeContent children of the node. Translation Local Vector3 translation of the node. Orientation Local Quaternion orientation of the node. Scale Local Vector3 scale of the node. Mesh MeshContainer reference of the node. That means the node has geometry attached to it. null if the node doesn't have geometry. Skin Reference of a SkinContent element, meaning the node is a skinned mesh. null if the node is not skinned. MeshesContainers In a Model mesh containers are where the 3d geometry is stored. It usually represents one full object, and it can contains one or more Meshes (see this section for more information.). The main reason for the MeshContainer concept is that a geometry attached to a node can contain multiple sub-elements with a different material each of them. One MeshContainer can be referenced by one or more nodes, making the model more memory efficient. Property Description Name Name of the mesh container. Skin Reference of the SkinContent for skinning information (if it's the case). BoundingBox The Bounding box of the mesh. Meshes List of the meshes of this container Materials The model also contain a list of a tuple containing the material id and the material name referenced by its meshes. The Mesh class contains a parameter called MaterialIndex which tells us the index of this list we're refering to. Skinning Information Evergine supports Skinning animation. With this method a geometry is deformed following the transformations of a set of bones. To support it the model class contains an array of SkinContent class, that represents the skinning information of a geometry. Every MeshContainer and Node that as a skinned mesh references one of the elements of the array. The SkinContent class contain this information: Property Description Name Name of the skinning content. Nodes Array of integer that contain the indices of the AllNodes list of the Model class. They are the bone nodes of the skeleton. InverseBindPoses Array of Matrix4x4. For every node of the skeleton there is a matrix in that contains the inverse bind pose relative to its parent. RootJoint Int that represents the index in the Nodes array of the root bone of the skinning. Using this properties animating the nodes referenced by this SkinContent will animate the geometry accordingly. In order for the skinned mesh to work properly, the Vertex Buffer of the skinned mesh needs the following fields: Semantic Type Description BoneIndices ´int4short´ Vector that contains the indices of the bones (from the Nodes list) affected by this vertex. BoneWeights float4 For every bone from he previous field, the weight indicates the percentaje of the box transforms that applies to this vertex. Animations In Evergine model animation is achieved by animating nodes. If those nodes are part of a skinning bones, it will also animate the skinned mesh accordingly. In this section Import Models Using Model Level of Detail Model Editor"
  },
  "manual/graphics/models/level_of_detail.html": {
    "href": "manual/graphics/models/level_of_detail.html",
    "title": "Level of Detail (LOD) for models | Evergine Doc",
    "keywords": "Level of Detail (LOD) for models The Level of detail (LOD) is a technique which reduces the number of GPU operations that Evergine requires to render distant models. This technique allows Evergine to reduce the model complexity it renders for an Entity based on its distance from the camera. It is possible to configure LOD levels in your external 3D modeling application and Evergine automatically creates and configures the required Entity and components for you when you drag&drop the model into a scene. The representation inside the hierarchy tree will be a parent entity with one child entity for every LOD defined level. The parent entity must be contained in a LODGroup component which allows you to configure all the parameters to correctly render this model depending on the distance to the camera and each child will have a LODEntry component with the information required by the LODGroup to define each level. LOD Levels A LOD level in a model defines which meshes (or materials) Evergine will render that model. LOD Levels are identified by an index, where Level 0 is the most detailed level. LOD selection Evergine select a LOD Level of a model depending on the area covered by this model in the camera. Examples: If the model is too close to the camera, we select the lowest LOD (more details). In that case, the model cover the 80% of the screen: If the model is far away, it appears smaller, and because of this, Evergine will select highest LOD levels. In that case LOD 1 is selected because the model is now at 15% of screen coverage: Here is the same example in movement: LOD Components To setup LOD in your entities, you need to use LODGroup and LODEntry components. LODGroup The LODGroup component manages the Level of Detail of the owner's entity. It will collect all LODEntry components defined in the entity hierarchy to conform the meshes assigned to each LOD Levels. They will calculate the screen coverage of the meshes in the camera, and select the properly level accordingly. Property Default Description LOD Levels 0.25, 12.5, 6.25, ... Indicates the minimum screen coverage required per each LOD Level (for example, by default LOD 0 requires that the entity covers at least 25% of the screen). LOD Bias 1 This property is used as a factor to the calculated screen coverage. This will be used to increase or reduce the overall LOD levels. For example, a LOD Bias of 0.5 cause that a model with a screen coverage of 40% is processed as 20%. In general terms, values lower than 1 tend to incrase the LOD levels, and values greater than 1 will produce of lower LOD levels. LODEntry You need to add a LODEntry component to assign a LOD level to a specified entity with a Mesh. This component requires a MeshRenderer component, which will enable or disable depending the overal LOD Level. By default, when a LOD Level is selected, it will enable all LODEntry MeshRenderer associated components, and they will disable the others LODEntry MeshRenderers. Property Default Description LOD 0 Indicates the LOD Level assigned to this entity DisableEntity false If this value is true, it will disable or enable the entire Entity, instead of only enable or disable the MeshRenderer. Importing LOD Models You can create models with different levels of details in an external 3D application for use Evergine's LOD system. If you name these meshes (and nodes) correctly, Evergine automatically creates and configures an Entity with LODGroup and LODEntry components for them when it imports the Model. To import a Model with LOD level into Evergine, you must do the following: In your external 3D application (Blender for example), follow the application's process to create as many LOD meshes as you need. Name the meshes according to the following naming convention: ExampleMeshName_LOD0 for the first LOD level (the most detailed version) ExampleMeshName_LOD1 ExampleMeshName_LOD2 Export your Model as a compatible Evergine model format (.glb or .fbx for example) Import the model into Evergine. Evergine recognizes the grouped Meshes and naming convention, and automatically creates LODGroup and LODEntry components when the model is instantiated in the scene. This is an example of a model in Blender with 4 LOD Level defined:"
  },
  "manual/graphics/models/model_editor.html": {
    "href": "manual/graphics/models/model_editor.html",
    "title": "Model Editor | Evergine Doc",
    "keywords": "Model Editor Model Editor allows the editing of model assets. Double click over a Model asset shown in Assets Details will open this editor. The editor is composed of 5 main parts: Viewport Shows the Model with the current configuration. If the model is animated, it will show the current animation state of the animation toolbar. Toolbar Helps with the model visualization. Has the following options: Item Description Toggles the Grid visualization. / Toggles the visualization from Solid (default) to Wireframe. Toggles the Bounding box visualization of the model. Toggles the Hierarchy visualization of the model. Toggles the normals visualization of the vertices. Toggles the UV checker visualization of the model. Resets the camera position. Changes the background color. ##Playback controls In case that the model has animations, the Playback Toolbar allows to play the selected clip. Control Description / Plays / Stop the current clip animationS. The timeline slider. The handle will mark the current time in the animation, and its position can be modified. Controls the Speed Factor of the reproduction. By default is 1.00. Properties Panel with all the Model properties. They don't depend on the profile. Property Description SwapSwindingOrder Flips the normals of the model when activated.. GenerateTangentSpace When checked will generate the tangent coordinates of every vertex. Useful when dealing with normal mapping. ExportAnimations For exporting the animation information of the model. ExportAsRaw Will export the model as the source file (.fbx for example), instead of creating a binary Evergine asset.. Animation clip properties For every animation contained in the model, it will show the following information. Property Description Index The animation order. Name the name of the clip. This string will be used in the Animation3D when we want to play the animation. Duration Timestamp of the duration."
  },
  "manual/graphics/models/using_models.html": {
    "href": "manual/graphics/models/using_models.html",
    "title": "Using Models | Evergine Doc",
    "keywords": "Using Models In this document you will learn how to load and use Models in your applications. Load Model from code The following sample code can be used to instantiate an existing Model asset (camera.fbx in this case) into your scene, creating a entity hierarchy. protected override void CreateScene() { var assetsService = Application.Current.Container.Resolve<AssetsService>(); // Load Model assets Model cameraModel = assetsService.Load<Model>(EvergineContent.Models.Camera_fbx); // Apply to an entity Entity camera = cameraModel.InstantiateModelHierarchy(assetsService); // Alternatively this method accepts the name of the root entity. Entity camera = cameraModel.InstantiateModelHierarchy(\"coolCamera\", assetsService); // Adds the entity into the scene. this.Managers.EntityManager.Add(camera); } How to create models from Evergine Studio. For instantiating a Model into a Scene just drag the model asset from the Asset Details Panel into your Scene. This will instantiate a full hierarchy of entities into the Scene. The new created hiearchy has the next kind of entities: Node Type Description Root The root node of the hierarchy. If the model contains animation this entity will contain the Animation3D component. Node Nodes without geometry attached. They are empty entities with just a Transform3D with the specific position, scale and orientation. Mesh Node that contains geometry. It contains the following components: MeshComponent, specifying with mesh from the model it will show. MaterialComponent. For every different material from the entity mesh, a component will be created. MeshRenderer for rendering the geometry. Skin If the node contains a skinned mesh, it will have the same components of a normal Mesh node, but using SkinnedMeshRenderer component instead. As you can see this, entity hierarchy mirrors the inner Model asset structure seen here."
  },
  "manual/graphics/particles/create_particles.html": {
    "href": "manual/graphics/particles/create_particles.html",
    "title": "Create Particle Systems | Evergine Doc",
    "keywords": "Create Particle Systems Particle systems simulates and renders a large amount of quads called particles. Create a Particle Systems asset in Evergine Studio You can create a material click button on from Assets Details panel to deploy a create menu options and click on the option \"Create particles\" Inspect Particle Systems in Asset Details You can find the particle system assets in the Assets Details panel when you select a folder in the Project Explorer. Particle System files in content directory The particle system file has the .weps extension. Create a new Particle System from code The following sample code can be used to create a new particle system and apply to an entity in your scene: var assetsService = Application.Current.Container.Resolve<AssetsService>(); var graphicsContext = Application.Current.Container.Resolve<GraphicsContext>(); // Sets its particle emitter. ParticleEmitterDescription emitterDesc = new ParticleEmitterDescription() { ParticleTexture = EvergineContent.Textures.particle_png, ParticleSampler = EvergineContent.Samplers.LinearClampSampler, RenderLayer = EvergineContent.RenderLayers.Alpha, MaxParticles = 1000, InitLife = 2, InitSpeed = 1, InitSize = 0.1f, InitColor = Color.Red, }; ParticlesEmitter emitter = new ParticlesEmitter(emitterDesc, graphicsContext, assetsService); // Creates the asset and sets its emitter. ParticleSystem particleSystem = new ParticleSystem(); particleSystem.AddEmitter(emitter);"
  },
  "manual/graphics/particles/index.html": {
    "href": "manual/graphics/particles/index.html",
    "title": "Particle System | Evergine Doc",
    "keywords": "Particle System Evergine supports complex particle system simulations, taking advantage of the Compute Shaders whenever it's possible. Basics Particles are elements that are used in a large number to create a wide range of visual effects like sparks, clouds, fire, etc. Evergine has a Particle System asset so developers can reuse create, adjust and reuse it in your scene. GPU vs CPU Evergine supports GPU particles simulation when the platform is supported. These simulations are based in compute shaders, allowing a fast and efficient representation of large amount of particles compared to the classical CPU particles. In this section Create Particle Systems Using Particles Particles Editor Particle properties Particle shapes Particle spawn Particle forces"
  },
  "manual/graphics/particles/particle_forces.html": {
    "href": "manual/graphics/particles/particle_forces.html",
    "title": "Particle Forces | Evergine Doc",
    "keywords": "Particle Forces Particle forces are component that once attached to an entity can alter the behavior of a Particle System. They mostly alter the velocity of the particles, according to a set of rules. Creating Forces in Evergine Studio. In the Entities Hierarchy panel of your Scene Editor, click the \"Add Entity and select Particles, then choose the kind of force you want to create: Point attractor Entity attractor Wind force Drag force We discuss the force types later in this article. This will create a new entity the Scene. The new created entity has a Transform3D component and a new component, depending of the type of the force (See below). Creating Forces by Code The following sample code can be used to instantiate an existing material asset and apply to an entity in your scene. protected override void CreateScene() { // Creates tje entity. A Point Attractor in this sample. Entity particleForce = new Entity() .AddComponent(new Transform3D()) .AddComponent(new PointAttractorForce() { ForceCategory = CollisionCategory3D.All }); this.Managers.EntityManager.Add(particleForce); } Common Properties All particle forces share a common base class (BaseForce). This class contains the following properties, making them accesible to all forces: Property Default value Description Strength 1 Factor of how much the force affects the particles. Force Category All Category of the force. A force only will affect particle systems that share the same ForceCategory. Range Enabled true If the force has a maximum range effect. Range 1 The maximum distance whether tha particle system is affected by the force. Decay Enabled true If true the particle force will decay over its range. Type of Forces As stated previously, Evergine supports the following particle system forces: Point Attractor Force This force gravitationally pulls all the particles into a point. In addition of the common properties of the base class, Particle Attractor Force adds properties for managing a cutout behavior. Cutout effect The cutout effect is a behavior affecting particles that goes closer to the attractor center. Instead of being gravitationally pulled to the center and orbiting around that point, The cutout effect causes to drag particles directly to its center, without escaping or keep orbiting. This is useful when we want to control particles that go to a specific point without keep endlessly orbiting around its center. Property Default value Description Cutout Enabled false Enables the cutout behavior of the force. Cutout Strength All The intensity where the particles are dragged into a vortex pit. Cutout Range true The range where the particles start falling into the force center. Entity Attractor Force The Entity Attractor Force is similar to Point Attractor Force, but the particles are attracted to an Entity. Specifically to all the meshes contained in an entity. It has the same Cutout parameters as previously stated, but in addition it has the following properties: Property Default value Description Source Entity Path null The path of the target entity of the force. Source Topology Vertex The part of the mesh topology towards the particles are attracted. Its values can be Vertex or Triangle Note In this version Evergine does not spport Triangle as Source Topology. Wind Force The Wind Force push the particles in the entity's forward direction around its effect area (if the range is enabled). It doesn't have additional parameters. Its strength controls how much the particles are pushed by the wind. Drag Force The Wind Force drags the particles around its effect area (if the range is enabled). It doesn't have additional parameters. Its strength controls how much the particles are dragged by the force."
  },
  "manual/graphics/particles/particle_properties.html": {
    "href": "manual/graphics/particles/particle_properties.html",
    "title": "Particle System properties | Evergine Doc",
    "keywords": "Particle System properties A particle system has the next properties. Particles Material Group that manages the properties relative to the particle material. Property Default value Description Texture particle.png The texture that will appear in each particle quad. Sampler State LinearClampSampler The texture sampler state. RenderLayer Additive The texture render layer. General properties Properties that controls the basic aspects of the particle system. Property Default value Description Max Particles 1000 The maximum amount of particles the emitter can handle. The emission will be paused if it reaches this number. Gravity 0 Sets the gravity of the particle system. Gravity value of 9.8 Drag 0 The amount of resistance the particle will find. Drag value of 0. Drag value of 2. Simulation Space World Sets the simulation space of the particle. In Local space the particle positions stays relative to its emitter transform. In World space all coordinates are global. Simulation Space value of World. Simulation Space value of Local. Random Precission Medium The precision of the random values generated, both in GPU and CPU simulations. High precision will randomize better but with a performance cost and the Low precision will improve performance but produce less random values. Medium precission (default value) offers a good balance. Life The life configuration panel controls the particle remaining life time when it's emitted. Property Default value Description Init Life mode Constant Sets whether the life time of the particle is Constant (Init Life) or RandomBetweenTwoConstants (A random value between Init Life and Init Life2). Init Life 5 The initial color of the particle. Init Life 2 5 The second value of the initial color range. The next video shows how different life parameters behave: Color The initial color configuration panel controls the particle color when it's emitted. Property Default value Description Init Color mode Constant Sets whether the initial color is Constant (Init Color) or RandomBetweenTwoConstants (A random value between Init Color and Init Color2). Init Color White The initial color of the particle. Init Color 2 White The second value of the initial color range. Preserve Highlights false If true, the tint effect of the initial color on the particle texture will decay when the texture color gets whiter. The next image shows two examples of initial color setting. Size The initial size configuration panel controls the particle size when it's emitted. Property Default value Description Init Size mode Constant Sets whether the initial size is Constant (Init Size) or RandomBetweenTwoConstants (A random value between Init Size and Init Size). Init Size 0.1 The initial size of the particle. Init Size 2 0.1 The second value of the initial size range. The next image shows two examples of initial size setting. Speed The initial speed configuration panel controls the particle speed magnitude (in space units per second) when it's emitted. The initial Velocity of the particle will be the initial direction vector multiplied by the Speed of the particle. Property Default value Description Init Speed mode Constant Sets whether the initial speed is Constant (Init Speed) or RandomBetweenTwoConstants (A random value between Init Speed and Init Speed). Init Speed 0.1 The initial speed of the particle. Init Speed 2 0.1 The second value of the initial speed range. The next video shows how different speed parameters behave: Angle The initial angle configuration panel controls the particle quad angle when it's emitted. Property Default value Description Init Angle mode Constant Sets whether the initial angle is Constant (Init Angle) or RandomBetweenTwoConstants (A random value between Init Angle and Init Angle). Init Angle 0 The initial angle of the particle in degrees. Init Angle 2 0 The second value of the initial angle range in degrees. The next image shows three examples of initial angle setting. Angular Speed The angular speed configuration panel controls the particle rotation speed when it's emitted. The quad will spin facing the camera with this angular speed. Property Default value Description Init Angular Speed mode Constant Sets whether the initial angular speed is Constant (Init Angular Speed) or RandomBetweenTwoConstants (A random value between Init Angular Speed and Init Angular Speed). Init Angular Speed 0 The initial angular speed of the particle. Init Angular Speed 2 0 The second value of the initial angular speed range. The next video shows a random angular speed of [-180, 180]. Shapes The Shape group defines all the properties to manage the volume or surface where the particles can be emitted. Property Default value Description Shape type Point Enum that contains all the shape emitter types. Currently the options are: Point Sphere Box Circle Entity Edge More information about these shapes here: Particle Shapes. Spawn The spawn information The Spawn property defines when and how many particles are emitted. Property Default value Description Spawn type Rate Enum that contains all the spawn emitter types. Currently the options are: Rate Burst Distance More information about the spawn management here: Particle Spawn. Color over life These properties manages how the particle color changes over life. In a next Evergine version we will implement a proper Gradient Color editor, but meanwhile we've defined the color over life of the particle using the next properties according to this diagram: Note The color of the gradient is applied as a tint over the initial color of the particle. Property Default value Description Color Animated false Sets whether the particle color is animated through its life time. Color Over Life 1 Transparent Color of the first point of the animation. Color Over Life 2 White Color of the second point of the animation. Color Over Life 3 White Color of the third point of the animation. Color Over Life 4 Transparent Color of the fourth point of the animation. Color Over Life 2 Position 0.2 Position in the curve of the second point of the animation. Must be in the range [0, 1] Color Over Life 3 Position 0.8 Position in the curve of the third point of the animation. Must be in the range [0, 1] The next video shows a particle system using the color gradient previously seen. Size over life These are the properties that controls how the particles change their size over their lifetime. In a next Evergine version we will implement a proper Curve Editor_ editor, but meanwhile we've defined the size over life of the particle using the next properties according to this diagram: Note The size of the curve is applied as a multiplier over the initial size of the particle. Property Default value Description Size Animated false Sets whether the particle size is animated through its life time. Size Over Life 1 0 Size multiplier of the first point of the animation. Size Over Life 2 1 Size multiplier of the second point of the animation. Size Over Life 3 1 Size multiplier of the third point of the animation. Size Over Life 4 0 Size multiplier of the fourth point of the animation. Size Over Life 2 Position 0.2 Position in the curve of the second point of the animation. Must be in the range [0, 1] Size Over Life 3 Position 0.8 Position in the curve of the third point of the animation. Must be in the range [0, 1] The next video shows a particle system using the size curve previously seen. Noise The noise panel allows to apply a turbulence field into the particle system. All the properties of this panel are used to control the parameters of that field. Property Default value Description Noise Enabled false Sets whether the particle system is affected by the noise field. Noise Strength 1 How much the particles are affected by the noise field. Bigger value will generate more chaos! Noise Size 1 The scale of the noise field. Big values cause a more wavy noise, small values will change the behavior among close particles. Noise Frequency 1 Represents the period at which the noise data is sampled. Noise Speed (1, 1, 1) The velocity vector that the noise field is moving. The next video shows a particle system affected by noise (Strength 1, Size 10, Frequency 1, Speed (0, -1, 0)). Forces This panel is used to allow the particle system to be affected by forces and to tune which one can affect them. The different available forces are explained in this article. Property Default value Description Forces Enabled false Sets whether the particle system is affected by the forces. Forces Category All The particle system is only affected by forces with the same Forces Category property. The next video shows a particle system affected by a Point Attractor Force."
  },
  "manual/graphics/particles/particle_shapes.html": {
    "href": "manual/graphics/particles/particle_shapes.html",
    "title": "Particle emission shapes | Evergine Doc",
    "keywords": "Particle emission shapes The particle system emission shape controls the position and direction of the particles when they are spawned. Common parameters All the emission shapes shares the next properties: Property Default value Description Randomize Direction 0 Randomness factor of the particle direction. A value of 0 mean no random directions, 1 is total chaos, while intermediate values sort of adds a little bit of randomness while keeping the overall particle direction of the emission shape. Velocity Offset (0, 0, 0) Velocity that it's added on top of initial particle velocity. Point Shape Point Shape emits the particles from the same position, and using the Up vector of the particles entity as direction. Sphere Shape Sphere Shape uses a sphere for the emission. Its parameters controls the size of the emission sphere, whether the particles are emitted from sphere volume or only from its surface, and the direction of the new particles (from the sphere center or following the up vector). Property Default value Description Radius 1 The radius of the emission sphere From Surface _false True if the particles are spawned from the sphere carcass, false if they are placed from inside the sphere volume. From Center true If the particles are moving from the sphere center or all to the same direction (Up vector). Box Shape Box Shape uses a box for the emission. Its parameters controls the size of the emission box. Property Default value Description Size (1, 1, 1) The size of the emission cube From Center true If the particles are moving from the box center or all to the same direction (Up vector). Circle Shape **Circle Shape** uses a circle for the emission. Its parameters controls the size of the emission circle and whether the particles are emitted from the circle area or its circumference, and the direction of the new particles (from the circle center or following the up vector). Property Default value Description Radius 1 The radius of the emission circle From Circumference false True if the particles are spawned from the circle circumference, false if they are placed from inside the circle area. From Center true If the particles are moving from the center or all to the same direction (Up vector). Edge Shape Edge Shape uses a line for the emission. Its parameters controls the length of the edge or line and the direction of the new particles (from the line or following the up vector). Property Default value Description Length 1 The length of the emission line From Center true If the particles are moving from the center or all to the same direction (Up vector). Entity Shape Entity Shape is an emission shape where particles are emitted from a specific Entity of the scene. More specifically, the particle system uses the meshes of the entity and its children. Note The target entity is not defined in the Entity Shape Emitter because it's part of the Particle System asset hence is not bound to any specific scene. It's defined in the ParticlesComponent component, which can have access to the scene entities. Property Default value Description EmissionSource Vertex The part of the mesh topology where the particles are emitted. Its values can be Vertex or Triangle."
  },
  "manual/graphics/particles/particle_spawn.html": {
    "href": "manual/graphics/particles/particle_spawn.html",
    "title": "Particle spawn | Evergine Doc",
    "keywords": "Particle spawn This module controls when the particles are spawned in our particle system. We mainly have 3 types: Rate Spawn Burst Spawn Distance Spawn. Rate Spawn The rate spawn emits uniformly the particles according to a rate. It can emit for a specific amount of time or indefinitely. It has the following properties: Property Default Value Description Rate 10 Particles per second that will be emitted by the particle system. Duration 0 Duration in seconds of the particle system emission. A value of 0 will emit indefinitely until we manually stop the emission. Burst Spawn The burst spawn emits all the particles available by the system (The Max Particles property) at the same time. It doesn't have additional properties. Distance Spawn The distance spawn emits at a rate proportionally to the particle system speed. A static particle system won't emit particles, but a moving one will emit accordingly to its speed and a specific defined factor. It has the following properties: Property Default Value Description Distance Rate 0 The amount of particles that are spawned per unit of distance translated by the emitter since the last frame. Duration 0 Duration in seconds of the particle system emission. A value of 0 will emit indefinitely until we manually stop the emission."
  },
  "manual/graphics/particles/particles_editor.html": {
    "href": "manual/graphics/particles/particles_editor.html",
    "title": "Particle System Editor | Evergine Doc",
    "keywords": "Particle System Editor Particle system Editor allows the editing of particle system assets. Double click over a Particle System asset shown in Assets Details will open this editor. The editor is composed of 4 main parts: Viewport Shows the Particle System with the current configuration. The user can orbit, zoom and pan the camera. Actions Description Left mouse button To rotate the camera around the particle system. Right mouse button To rotate two lights around the particle system. Mouse wheel To make zoom in/out camera. Toolbar Helps with the model visualization. Has the following options: Item Description Toggles the Grid visualization. Toggles the Emitter shape gizmo visualization. Forces particles simulation to be made using CPU instead of GPU when this option is activated, even when GPU particles are available. Resets the camera position. Changes the background color. ##Playback controls This bar controls some aspect of the particle simulation life. Control Description Resets the particle system. / The timeline slider. The handle will mark the current time in the animation, and its position can be modified. Controls the Time Factor of the particle system. The same behavior of the Particles Component property. Particles Properties Panel with all the Particle System properties. They don't depend on the profile."
  },
  "manual/graphics/particles/using_particles.html": {
    "href": "manual/graphics/particles/using_particles.html",
    "title": "Using Particles | Evergine Doc",
    "keywords": "Using Particles In this document you will learn how to load and use Particle Systems in your applications. How to create particle systems from Evergine Studio. For instantiating a Particle System into a Scene just drag the particle system asset from the Asset Details Panel into your Scene. This will create a new entity the Scene. The new created entity has a Transform3D component and 2 new components: ParticlesComponent This component loads the Particle System asset and manages its simulation and resources. It has the following methods: Property Description StartEmitting () Starts the particle system emission if it was stopped StopEmitting () Stops the particle system emission if it was already emitting Reset () Resets all the particles to its initial state And the following properties: Property Type Description ParticleSystem ParticleSystem Selects the particle system asset into this entity. Force CPU Particles boolean Forces the particle system to use a CPU simulation, even if the platform supports GPU particles. (false by default) Emit Automatically boolean When true, starts emitting particles when the scene is loaded. If false it will be idle until it's manually started. (true by default) Life Factor float Factor applied to the life time of the particle. 2 will cause particles life reduced by half. (1 by default) Time Factor float Time factor applied to the whole particle simulation. A value of 2 will cause particles move at double speed, while 0.5 will slow down particles to half speed. (1 by default) Load Particle System from code The following sample code can be used to create a new Particle force entity in your scene. protected override void CreateScene() { // Load Particle System ParticleSystem particlesAsset = this.Managers.AssetSceneManager.Load<ParticleSystem>(EvergineContent.Particles.MyParticleSystem); // Apply to an entity Entity particlesEntity = new Entity() .AddComponent(new Transform3D()) .AddComponent(new ParticlesComponent() { ParticleSystem = particlesAsset }) .AddComponent(new ParticlesRenderer()); this.Managers.EntityManager.Add(particlesEntity); }"
  },
  "manual/graphics/postprocessing_graph/create_postprocessing_graphs_from_code.html": {
    "href": "manual/graphics/postprocessing_graph/create_postprocessing_graphs_from_code.html",
    "title": "Create Postprocessing Graph from code | Evergine Doc",
    "keywords": "Create Postprocessing Graph from code Although we recommend to create PostProcessing using Evergine Studio, you can achieve this task from code. The following sample code can be used to create a new postprocessing graph and apply it to an entity in your scene. protected override void CreateScene() { var graphicsContext = Application.Current.Container.Resolve<GraphicsContext>(); // Create compute effect var shaderSource = @\" [Begin_ResourceLayout] Texture2D input : register(t0); RWTexture2D<float4> Output : register(u0); SamplerState Sampler : register(s0); [End_ResourceLayout] [Begin_Pass:Default] [Profile 11_0] [Entrypoints CS = CS] [numthreads(8, 8, 1)] void CS(uint3 threadID : SV_DispatchThreadID) { float2 outputSize; Output.GetDimensions(outputSize.x, outputSize.y); float2 uv = (threadID.xy + 0.5) / outputSize; float4 color = input.SampleLevel(Sampler, uv, 0); Output[threadID.xy] = float4(color.x,0,0,1); } [End_Pass] \"; Effect computeEffect = new EffectFromCode(graphicsContext, shaderSource); // Create Postprocessing Graph PostProcessingGraphDescription graphDescription = new PostProcessingGraphDescription(); // Create start node PostProcessingNode render = new PostProcessingNode(); var renderColorTextureOPort = new PostProcessingOutputNodePort(\"ColorTexture\", new PostProcessingNodePortLoadableType<Texture>()); var renderDepthTextureOPort = new PostProcessingOutputNodePort(\"DepthTexture\", new PostProcessingNodePortLoadableType<Texture>()); render.AddOutputPort(renderColorTextureOPort); render.AddOutputPort(renderDepthTextureOPort); graphDescription.StartNode = render; render.IntegrityNodeCheck(graphDescription); // Create end node PostProcessingNode screen = new PostProcessingNode(); var screenColorTextureIPort = new PostProcessingInputNodePort(\"ColorTexture\", new PostProcessingNodePortLoadableType<Texture>()); screen.AddInputPort(screenColorTextureIPort); graphDescription.EndNode = screen; screen.IntegrityNodeCheck(graphDescription); // Create my custom node PostProcessingNode myNode = new PostProcessingNode(computeEffect); myNode.ThreadGroupDivisorX = 8; myNode.ThreadGroupDivisorY = 8; myNode.ThreadGroupDivisorZ = 1; graphDescription.AddNode(myNode); // Connect nodes myNode.GetInputPortByName(\"input\").AddConnection(new PostProcessingPortConnection(renderColorTextureOPort)); myNode.GetOutputPortByName(\"Output\").AddConnection(new PostProcessingPortConnection(screenColorTextureIPort)); PostProcessingGraph graph = new PostProcessingGraph(graphDescription); // Add postprocessing graph to scene Entity postprocessingVolume = new Entity() .AddComponent(new Transform3D()) .AddComponent(new PostProcessingGraphRenderer() { ppGraph = graph }); this.Managers.EntityManager.Add(postprocessingVolume); }"
  },
  "manual/graphics/postprocessing_graph/create_postprocessing_graphs.html": {
    "href": "manual/graphics/postprocessing_graph/create_postprocessing_graphs.html",
    "title": "Create Postprocessing Graph | Evergine Doc",
    "keywords": "Create Postprocessing Graph The Post-Processing graph is a group of nodes connected that apply visual effects to the output render before drawing on the screen. Each node is a compute effect. Create a Postprocessing Graph asset in Evergine Studio You can create a postprocessing graph click button on from the Assets Details panel to deploy a create menu options and click on the option \"Create Post-Processing Graph\" Inspect Postprocessing Graph in Asset Details You can find the postprocessing graph assets in the Assets Details panel when you select a folder in the Project Explorer. Postprocessing Graph files in content directory The postprocessing graph file has the .wepp extension."
  },
  "manual/graphics/postprocessing_graph/custom_postprocessing_graph.html": {
    "href": "manual/graphics/postprocessing_graph/custom_postprocessing_graph.html",
    "title": "Custom Postprocessing graph | Evergine Doc",
    "keywords": "Custom Postprocessing graph This section, it is explained how to create your custom postprocessing graph. This could be useful if you want to create and test effects that are not available in the default postprocessing graph. Example For this example, we are going to create a simple filter that renders only the red component of input render. First, create a compute effect from the Assets Details panel: Write the code of our custom filter from Effect Editor: [Begin_ResourceLayout] Texture2D input : register(t0); RWTexture2D<float4> Output : register(u0); SamplerState Sampler : register(s0); [End_ResourceLayout] [Begin_Pass:Default] [Profile 11_0] [Entrypoints CS = CS] [numthreads(8, 8, 1)] void CS(uint3 threadID : SV_DispatchThreadID) { float2 outputSize; Output.GetDimensions(outputSize.x, outputSize.y); float2 uv = (threadID.xy + 0.5) / outputSize; float4 color = input.SampleLevel(Sampler, uv, 0); Output[threadID.xy] = float4(color.x,0,0,1); } [End_Pass] Create a new Postprocessing graph asset from Assets Details panel After creating the postprocessing graph asset make double click on the asset to open the Postprocessing Graph Editor. You can see an empty postprocessing graph where the render node connects directly with the Screen node. Drag our compute effect from the Available effects panel to the graph editor to create a new node. Then connect render node Color texture port with Custom node Input port and Custom node Output port with Screen node Color Texture port. After saving the graph, you can see the result on the viewport panel. To use your custom postprocessing graph in your scene read more details in using postprocessing graph section Special Nodes There is a special Node named Enable that you can use to enable or disable an effect in your graph. Enable node has two inputs where input0 port connects with path without applying the effect and input1 port that connect with the path with the effect apply. Using its Enabled parameter you can select which path will be used by the output port. Everyone analyzes the graph before using and discard the no-used paths. For example, we are going to add this special node out before example. Special [Output] metatags There are special compute effect metatags used by the Postprocessing graph. The metatags Output could be used to define the output texture of any node. By default, the output texture is created using the first input texture information but you can configure it with Output metatags. With these special metatags, you can define the with, height, and pixel format of the node output texture. Output overloading [Output(ReferencedInput)] [Output(ReferencedInput, ScaleFactor)] [Output(ReferencedInput, ScaleFactor, PixelFormat)] [Output(width, height, PixelFormat)] The metatag parameters are: Parameter Description ReferenceInput Input name used to get width, height and Pixel format of the output texture. ScaleFactor Defines the scale factor apply to the width and height of the ReferenceInput to get the output width and height dimensions. PixelFormat Defines the pixel format of the output texture. Width Defines the width dimension of the output texture. Height Defines the height dimension of the output texture. Example In the following example the Depth input texture has 1920x1080 dimension and D24_UNorm_S8_UInt pixel format. Texture2D<float> Depth : register(t0); RWTexture2D<float4> PositionOutput : register(u0); [Output(Depth, 1, R16G16B16A16_Float)] RWTexture2D<float2> VelocityOutput : register(u1); [Output(Depth, 0.5, R16G16_Float)] RWTexture2D<float> LinealDepthOutput : register(u2); [Output(500, 500, R32_Float)] The result of the resolve the output tags will be: Output Texture Dimensions Pixel Format PositionOutput 1920x1080 R16G16B16A16_Float VelocityOutput 960x540 R16G16_Float LinealDepthOutput 500x500 R32_Float Postprocessing Graph Decorator You can create a c# class extending from PostProcessingGraphDecorator to define how your custom postprocessing graph is displayed on the PostProcessingGraphRenderer component. Only need to implement the GenerateUI method using the Editor extensions. More details about here."
  },
  "manual/graphics/postprocessing_graph/default_postprocessing_graph/anti-aliasing.html": {
    "href": "manual/graphics/postprocessing_graph/default_postprocessing_graph/anti-aliasing.html",
    "title": "Fast approximate anti-aliasing (FXAA) | Evergine Doc",
    "keywords": "Fast approximate anti-aliasing (FXAA) Fast approximate anti-aliasing is a screen-space anti-aliasing algorithm created by Timothy Lottes at Nvidia. The main advantage of this technique over conventional spatial anti-aliasing is that it does not require large amounts of computing power. It achieves this by smoothing undesirable jagged edges (\"jaggies\") as pixels, according to how they appear on-screen, rather than analyzing the 3D model itself, as in conventional spatial anti-aliasing. Since it is not based on the actual geometry, it will smooth not only edges between triangles, but also edges inside alpha-blended textures, or those resulting from pixel shader effects, which are immune to the effects of multisample anti-aliasing (MSAA). The downsides are that high contrast texture maps are blurred, that FXAA must be applied before rendering the HUD elements of a game lest it affect them too, and that polygonal details smaller than one pixel that would have been captured and rendered by MSAA and SSAA will not be captured and rendered by FXAA alone."
  },
  "manual/graphics/postprocessing_graph/default_postprocessing_graph/bloom.html": {
    "href": "manual/graphics/postprocessing_graph/default_postprocessing_graph/bloom.html",
    "title": "Bloom, Dirt, LightShaft and Lensflare | Evergine Doc",
    "keywords": "Bloom, Dirt, LightShaft and Lensflare In this section, multiples effects are explained because the process steps to calculate them are similar and they was implemented together for performance reasons. Bloom The effect produces fringes (or feathers) of light extending from the borders of bright areas in an image, contributing to the illusion of an extremely bright light overwhelming the camera or eye capturing the scene. Parameter Description Threshold The pixels with a luminance higher than threshold value will apply the effect. Color Intensity Defines the bloom color intensity in the final blend image. Intensity Defines how the render image and the bloom output will be blended. Dirt This effect tries to simulate when the camera lens is dirty and some lens stains are visible. Parameter Description Texture Dirt texture used. Intensity Dirt intentsity or how the render image and the dirt output will be blended. LightShaft LightShaft also known as God Rays allows the viewer to see beams of light shining across the environment. Parameter Description Min.Threshold Minimun luminance of the pixel will be affected by the effect. Max. Threshold Maximum luminance of the pixel will be affected by the effect. Scale Size of the effect. Intensity Defines how the render image and the bloom output will be blended. LensFlare A Lens flare happens when light is scattered or flared in a lens system, often in response to a bright light, producing a sometimes undesirable artifact in the image. This happens through light scattered by the imaging mechanism itself, for example through internal reflection and forward scatter from material imperfections in the lens. Parameter Description Ghost Count The ghost image number. Ghost Spacing The distance between ghost elements. Ghost threshold The pixels with upper luminance value to the threshold will be used to generate the ghost elements. Halo radius External halo radius. Halo thickness External halo thickness. Halo threshold The pixels with upper luminance value to the threshold will be used to generate the halo. Halo Chro. Aberration Amout of chromatic aberration. Intensity Defines how the render image and the bloom output will be blended."
  },
  "manual/graphics/postprocessing_graph/default_postprocessing_graph/depth_of_field.html": {
    "href": "manual/graphics/postprocessing_graph/default_postprocessing_graph/depth_of_field.html",
    "title": "Depth of Field (DoF) | Evergine Doc",
    "keywords": "Depth of Field (DoF) The Depth of Field (DoF) effect tries to simulate the blur or bokeh observed in out-of-focus areas in the camera field of vision. The depth of field can be calculated based on focal length, distance to subject, the acceptable circle of confusion size, and aperture. A particular depth of field may be chosen for technical or artistic purposes. Parameter Description Debug Mode Mode debug enabled draws red the nearest areas, green the focus areas and blue the farest areas. Only for debug proposes. Focal Region Size of the focus area. Bokeh Shape Shape uses to simulate bokeh effect: Circle, Pentagon, Hexagon, Heptagon. Bokeh Size Size of the bokeh shapes. Bokeh rotation Angle of the bokeh shapes. Near Fade Power Blur border size around the nearest areas. Tip The Focal Distance is a Camera parameter."
  },
  "manual/graphics/postprocessing_graph/default_postprocessing_graph/fidelity_super_resolution.html": {
    "href": "manual/graphics/postprocessing_graph/default_postprocessing_graph/fidelity_super_resolution.html",
    "title": "Fidelity Super Resolution | Evergine Doc",
    "keywords": "Fidelity Super Resolution AMD FidelityFX Super Resolution (FSR) uses cutting-edge upscaling technologies to help boost your framerates in select titles1 and deliver high-quality, high-resolution gaming experiences, without having to upgrade to a new graphics card. More details here. This effect allows scale low-resolution image to the final resolution image improving the performance of your render. FSR support 4 recommended configurations: FSR 1.0 quality mode Description Scale Factor Input Resolution Output Resolution Ultra quality Ultra quality mode produces an image with quality virtually indistinguishable from native rendering. It should be selected when the highest quality is desired. 1.3x 1477x831 1970x1108 2646x1108 2954x1662 1920x1080 2560x1440 3440x1440 3840x2160 Quality Quality mode produces a super resolution image with quality representative of native rendering, with a sizeable performance gain. 1.5x 1280x720 1706x960 2293x960 2560x1440 1920x1080 2560x1440 3440x1440 3840x2160 Balanced Balanced mode produces a super resolution image approximating native rendering quality, with a major performance gain compared to native. 1.7x 1129x635 1506x847 2024x847 2259x1270 1920x1080 2560x1440 3440x1440 3840x2160 Performance Performance mode visibly impacts image quality and should only be selected in situations where needing additional performance is critial. 2.0x 960x540 1280x720 1720x720 1920x1080 1920x1080 2560x1440 3440x1440 3840x2160"
  },
  "manual/graphics/postprocessing_graph/default_postprocessing_graph/fog.html": {
    "href": "manual/graphics/postprocessing_graph/default_postprocessing_graph/fog.html",
    "title": "Fog | Evergine Doc",
    "keywords": "Fog This effect allows simulate distance and height fog. Fog can be considered a type of low-lying cloud usually resembling stratus, and is heavily influenced by nearby bodies of water, topography, and wind conditions. Parameter Description Color Fog color. Mode Defines 3 modes to calculate the fog distance: Lineal, Exponential and ExponentialSquare. Distance enabled Enabled/Disabled depth distance. Distance Density Amount of density with the fog distance. Height Enabled Enabled/Disabled fog in height. Height Maximum height with fog. Height Density Amount of density with the fog height."
  },
  "manual/graphics/postprocessing_graph/default_postprocessing_graph/index.html": {
    "href": "manual/graphics/postprocessing_graph/default_postprocessing_graph/index.html",
    "title": "Default Postprocessing graph | Evergine Doc",
    "keywords": "Default Postprocessing graph Default Evergine project template imports the Evergine.Core package and this package includes the Default Post-Processing graph with the most important post-processing visual effect common in a project. Default Postprocessing effects The complete list of postprocessing effects cover by the default postprocessing graph are: Screen Space Ambient Occlusion (SSAO) Screen Space Reflection (SSR) Fog Temporal Anti-Aliasing (TAA) Motion Blur Depth of Field (DoF) Bloom, Dirt, LensFlare, LightShaft Fidelity Super Resolution (FSR) Sharpen Tonemapping, Chromatic aberration, Vignette, Grain, Distortion Fast approximate anti-aliasing (FXAA) Using default postprocessing graph from Evergine studio These effects can be configured from PostprocessingGraphRenderer component inside of a postprocessing volume entity."
  },
  "manual/graphics/postprocessing_graph/default_postprocessing_graph/motion_blur.html": {
    "href": "manual/graphics/postprocessing_graph/default_postprocessing_graph/motion_blur.html",
    "title": "Motion Blur | Evergine Doc",
    "keywords": "Motion Blur Motion Blur blurs the image based on the camera motion. This effect is the apparent streaking of moving objects in a photograph or a sequence of frames, such as a film or animation. It results when the image being recorded changes during the recording of a single exposure, due to rapid movement or long exposure. Parameter Description Decay Factor Visibility factor of the samples. Num. Samples Number of samples used."
  },
  "manual/graphics/postprocessing_graph/default_postprocessing_graph/screen_space_ambient_occlusion.html": {
    "href": "manual/graphics/postprocessing_graph/default_postprocessing_graph/screen_space_ambient_occlusion.html",
    "title": "Screen Space Ambient Occlusion (SSAO) | Evergine Doc",
    "keywords": "Screen Space Ambient Occlusion (SSAO) The Screen Space Ambient Occlusion (SSAO) effect approximates Ambient Occlusion in realtime, as an image post-processing effect. It darkens creases, holes and surfaces that are close to each others. In real life, such areas tend to block out or occlude ambient light, hence they appear darker. This effect uses the depth framebuffer data and throws several rays into a range in different directions from each pixel. If a ray hits with any geometry the pixel will become darker. The darkest pixel will be the one that all rays hit with something, and in the other part, the lightest pixel will occurs where all his rays didn't hit with anything. In the above picture you can see first the scene without effect applied, then the scene with the SSAO output, and finally the blend combination between original render and SSAO output. SSAO Parameters Parameter Description SPP Samples per pixel. Range Range around the point using to calculate close geometries. Power Amount of darkness Scale Bias -- Intensity Intensity of the blend with render image."
  },
  "manual/graphics/postprocessing_graph/default_postprocessing_graph/screen_space_reflection.html": {
    "href": "manual/graphics/postprocessing_graph/default_postprocessing_graph/screen_space_reflection.html",
    "title": "Screen Space Reflection (SSR) | Evergine Doc",
    "keywords": "Screen Space Reflection (SSR) Screen Space Reflections (SSR) effect adds real time local reflections to the object surfaces. This effect is in screen space, which means reflected can be only objects that are already on the screen. The effect uses the depth buffer to throw rays from object surfaces to calculate the reflection. Parameter Description Num. Rays Number of rays per pixel. Max. Reflection Distance Maximum ray reflection distance Refinement steps After the ray hit detection, number of steps of the refinement hit algoritm. Pixel Thickness Helps to avoid depth shadow areas. Max. Roughness Maximun roughness value which the reflection will be calculated. Intensity Blend with original render. Debug Mode Enable Allows to check only the reflection output."
  },
  "manual/graphics/postprocessing_graph/default_postprocessing_graph/sharpen.html": {
    "href": "manual/graphics/postprocessing_graph/default_postprocessing_graph/sharpen.html",
    "title": "Sharpen | Evergine Doc",
    "keywords": "Sharpen Sharpen effect is a technique for increasing the apparent sharpness of an image. This effect is recommended to use in combination with effects that their output can apply blur to the image as Temporal Anti-Aliasing (TAA) or Fidelity Super Resolution (FSR). Parameter Description Amount Increase/reduce the sharpness effect."
  },
  "manual/graphics/postprocessing_graph/default_postprocessing_graph/temporal_anti_aliasing.html": {
    "href": "manual/graphics/postprocessing_graph/default_postprocessing_graph/temporal_anti_aliasing.html",
    "title": "Temporal Anti-Aliasing | Evergine Doc",
    "keywords": "Temporal Anti-Aliasing Tempral Anti-Aliasing (TAA) is a spatial anti-aliasing technique for computer-generated video that combines information from past frames and the current frame to remove jaggies in the current frame. In TAA, each pixel is sampled once per frame but in each frame the sample is at a different location at subpixel level. Pixels sampled in past frames are blended with pixels sampled in the current frame to produce an anti-aliased image. This effect improves the quality of anti-aliasing FXAA result. This effect doesn't require any setting parameters."
  },
  "manual/graphics/postprocessing_graph/default_postprocessing_graph/tonemapping.html": {
    "href": "manual/graphics/postprocessing_graph/default_postprocessing_graph/tonemapping.html",
    "title": "Tonemapping, Chromatic aberration, Vignette, Grain, Distortion | Evergine Doc",
    "keywords": "Tonemapping, Chromatic aberration, Vignette, Grain, Distortion In this section, multiples effects are explained because the process steps to calculate them are similar and they was implemented together for performance reason. Tonemapping Tonemapping is a technique used in image processing and computer graphics to map one set of colors to another to approximate the appearance of high-dynamic-range images in a medium that has a more limited dynamic range. In the above image the left side is the render without tonemapping applied and the right side is the result of to apply tonemapping to the left side. Parameter Description HDR Enabled Enabled/Disabled mapping of High Dynamic Range (HDR) to Low Dynamic Range (LDR). Curve Defines how to map the image color to output image. There are the following curves availables: Reinhard, ReinhardSQ, LumaReinhard, Filmic, ACES, RombindAHouse. Default curve is ACES. LUT Enable Enabled/Disabled use the Lookup Table (LUT) Texture table to map de colors LUT Texture Represents a Lookup Table (LUT) 16x16x16 color neutral unwrapped to a 256x16 texture. There are two LUT Texture samples: HDR: Vintage: Chromatic aberration Chromatic aberration, also known as “color fringing” or “purple fringing”, is a common optical problem that occurs when a lens is either unable to bring all wavelengths of color to the same focal plane, and/or when wavelengths of color are focused at different positions in the focal plane. Chromatic aberration is caused by lens dispersion, with different colors of light travelling at different speeds while passing through a lens. As a result, the image can look blurred or noticeable colored edges (red, green, blue, yellow, purple, magenta) can appear around objects, especially in high-contrast situations. In the above image the left side is the render withou chromatic aberration applied and the right side is the result of to apply chromatic aberration to the left side. Parameter Description Strength Defines the distance between color bands. Offset Defines vector direction of the aberration. Grain If you’ve watched a film and seen speckles on the screen in random patterns, you’ve seen film grain. Originally, the actual grains in film grain were small particles of silver halide, the primary photosensitive substance used in chemical film. These particles are randomly distributed artifacts throughout the image. Parameter Description Intensity Defines the intensity of the grain effect. Vignette A vignette is a decrease in brightness of a photograph towards its edges compared to the image centre. Vignetting is often an undesired effect caused by camera settings that are not suitable for the given light situation. However, the effect can also be added subsequently to create noticeable changes in the picture’s mood and perception by making subtle changes only. Parameter Description Power Defines the intensity of the vignette effect. Radio Defines the radio of the effect respect to the center image. Distortion This is a visual effect that simulates the effect produced by the refraccion of the light. Refraction produces when you see through the vidrio or the fire smoke could be some examples of this effect. Tip This effect required to use the Distortion Material include in the Evergine.core package"
  },
  "manual/graphics/postprocessing_graph/index.html": {
    "href": "manual/graphics/postprocessing_graph/index.html",
    "title": "Postprocessing Graph | Evergine Doc",
    "keywords": "Postprocessing Graph Post-Processing graph allows you to apply visual effects like Tonemapping, Depth of Field, Temporal Anti-Aliasing, SSAO, SSR ... to your scene final render. The Post-Processing graph is a group of nodes connected that apply visual effects to the output render before drawing on the screen. Each node is a compute effect. With Postprocessing you could obtain a good looking cinematic appearance for your applications: Postprocessing Disabled Postprocessing Enabled Postprocessing and compute effects A postprocessing graph is a group of compute effect nodes that apply effects to the first render node and connect the result to the last Screen node. You can create a postprocessing graph with single or multiples nodes depends on your proposal. Post-Processing graphs are a type of asset and have a dedicated Editor Post-Processing Graph Editor. Default Postprocessing Graph InEvergine, the default project template imports the Evergine.Core package and this package includes the Default Post-Processing graph with the most important post-processing visual effect common in a project. So in the most of time, you will use this asset reserving to create a new custom post-processing graph only to create new visual effects or to improve performance needs. In this section Create Postprocessing Graph Using Postprocessing Graph Postprocessing Graph Editor Default postprocessing graph Custom postprocessing graph Create custom Postprocessing Graph from code"
  },
  "manual/graphics/postprocessing_graph/postprocessing_graph_editor.html": {
    "href": "manual/graphics/postprocessing_graph/postprocessing_graph_editor.html",
    "title": "Postprocessing Graph Editor | Evergine Doc",
    "keywords": "Postprocessing Graph Editor Postprocessing Graph Editor allows editing the Postprocessing Graph assets. Double click over a postprocessing graph asset shown in Assets Details will open this editor. The editor is composed of 3 main parts: Graph Editor Compute Effects collection Viewport. Graph Editor The Graph Editor allows you to create graph nodes to connect the start node (Render) with the last node (Screen). The nodes are computed effects and his parameters, input, and output are defined by his ResourceLayout block. Node elements Description Name Located on top of the Node is the name of the compute effect used. Divisors Allows configure the ThreadGroupDivisor X,y and Z to dispatch the compute effect. Parameters Allows configuring constant buffer or structure buffer properties to the compute effect. Input Allows set Textures and Samplers to the compute effect. Output Allows set RWTextures to the compute effect. Tip The node inputs can only be connected with a single node output but a node output can be connected with multiple node inputs. Toolbox The toolbox is located on the top side of the graph editor and allows to manipulate the graph view. Icon Description Delete the selected node. Execute an algorithm to relocated nodes and avoid node overlapping. Zoom in/out the graph. Center the view over the graph. Actions Description Left mouse button Selection tool. Allows to select a single or multiple nodes. Right mouse button Cut tool. Allows the draw a cut line to break conections. Medium mouse button Pan tool. Allows to move along the graph. Mouse wheel Allows to make zoom in/out over the graph. Compuse Effects Collection In this panel, you can find all compute effects existing in the project and drag an effect to the graph editor to use it. Viewport The vewport allows to inspect the result of the postprocessing graph applied to the scene: Note To refresh the graph changes in the viewport, you need to save all graph changes. Icon Description The combobox allows to select the current scene for the viewport. Opens the camera settings to configure all its parameters. The camera changes are not stored so it is only for testing proposes. The viewport allows you a simple interaction to easily inspect the scene: Actions Description W, S, D, A Move camera along the scene. Right mouse button Rotates camera. Medium mouse button Camera panning. Mouse wheel Camera zoom in/out."
  },
  "manual/graphics/postprocessing_graph/using_postprocessing_graphs.html": {
    "href": "manual/graphics/postprocessing_graph/using_postprocessing_graphs.html",
    "title": "Using Postprocessing Graph | Evergine Doc",
    "keywords": "Using Postprocessing Graph In this document, you will learn how to load and use Postprocesing Graph in your applications. Load Postprocessing Graph from code The following sample code can be used to instantiate an existing postprocessing graph asset and apply it in your scene. protected override void CreateScene() { var assetsService = Application.Current.Container.Resolve<AssetsService>(); var graph = assetsService.Load<PostProcessingGraph>(EvergineContent.PostprocessingGraph.MyPostProcessingGraph); // Add postprocessing graph to scene Entity postprocessingVolume = new Entity() .AddComponent(new Transform3D()) .AddComponent(new PostProcessingGraphRenderer() { ppGraph = graph }); this.Managers.EntityManager.Add(postprocessingVolume); } How to apply Postprocessing graph to a scene from Evergine Studio. You can apply a postprocessing graph to your scene click on button from Entities Hierarchy panel and select Post-processing Volume Postprocessing Volume is an entity in your scene composed of 3 components: Transform3D PostProcessingGraphRenderer BoxCollider3D With the PostProcessingGraphRenderer component, you can configure it to work in two modes. Mode Description Global All cameras in your scene will be affected by the postprocessing graph. Volume The cameras enter into the volume defines by a BoxCollider will be affected by the postprocessing graph. In addition, you can configure the LayerOrder to execute the postprocessing in your scene. For example, you can execute the postprocessing after drawing all entities of your scene but before the UI entities. Finally, The PostProcessingGraphRenderer allows to load a Postprocessing Graph asset and displays all his nodes or his associated decorator."
  },
  "manual/graphics/primitives.html": {
    "href": "manual/graphics/primitives.html",
    "title": "Primitives | Evergine Doc",
    "keywords": "Primitives Evergine has a 3D primitives collection that you can use for prototyping proposes. The primitives are easier and faster to use so they are very useful when you are making tests or your creating a prototype scene. Primitive collection: Capsule Cone Cube Cylinder Plane Pyramid Sphere Teapot Torus The main difference of using primitives instead of a Model is that in that case the Mesh is generated procedurally, instead of obtaining it from an asset. It allows you to parametrice the way this mesh is generated. Create primitive from Evergine Studio From the Entity Hierarchy panel click on button and go to Primitives 3D submenu. Create primtive from code To create a primitive only need to create an entity with the following components: protected override void CreateScene() { var assetsService = Application.Current.Container.Resolve<AssetsService>(); var material = assetsService.Load<Material>(EvergineContent.Materials.DefaultMaterial); Entity cubeEntity = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new CubeMesh()) // A cube primitive .AddComponent(new MeshRenderer()); this.Managers.EntityManager.Add(cubeEntity); } Tip To create primitive only need to change CubeMesh component by CapsuleMesh, ConeMesh, CylinderMesh, PlaneMesh, PyramidMesh, SphereMesh, TeapotMesh or TorusMesh component. Cube Parameters The component for create this primitive is CubeMesh Parameter Description Size The size of the cube. Must be greater than 0. UVHorizontalFlip Value indicating whether to flip the horizontal texture coordinate. UVVerticalFlip Value indicating whether to flip the vertical texture coordinate. InitialU The horizontal texture coordinate offset. InitialV The vertical texture coordinate offset. UTile The horizontal texture coordinate scale factor. VTile The vertical texture coordinate scale factor. Sphere Parameters The component for create this primitive is SphereMesh Parameter Description Diameter The diameter of the sphere. Must be greater than 0. Tessellation The tessellation of the sphere. Must be greater than 3. UVHorizontalFlip Value indicating whether to flip the horizontal texture coordinate. UVVerticalFlip Value indicating whether to flip the vertical texture coordinate. Plane Parameters The component for create this primitive is PlaneMesh Parameter Description PlaneNormal The normal of the plane. Availables values: XPositive YPositive ZPositive XNegative YNegative ZNegative Width The width of the plane. Must be greater than 0. Height The height of the plane. Must be greater than 0. TwoSides Value indicating whether the plane has two sides. By default botton face of the plane is not generated. UMirror Value indicating whether to flip the horizontal texture coordinate. VMirror Value indicating whether to flip the vertical texture coordinate. UOffset Apply an offset to the horizontal texture coordinates. VOffset Apply an offset to the vertical texture coordinates. UTile The horizontal texture coordinate scale factor. VTile The vertical texture coordinate scale factor. Origin Represent the pivot in a normalized position. By default the value is (0.5, 0.5), which indicatest that the PlaneMesh position is measured from the center of the plane. Teapot Parameters The component for create this primitive is TeapotMesh Parameter Description Size The size of the teapot. Must be greater than 0. Tessellation The tessellation of the teapot. Must be greater than 3. Capsule Parameters The component for create this primitive is CapsuleMesh Parameter Description Height The height of the capsule. Must be greater than 0. Radius The radius of the capsule. Must be greater than 0. Tessellation the tessellation of the capsule. Must be even. Cone Parameters The component for create this primitive is ConeMesh Parameter Description Height The height of the cone. Must be greater than 0. Diameter The diameter of the cone. Must be greater than 0. Tessellation The tessellation of the cone. Cylinder Parameters The component for create this primitive is CylinderMesh Parameter Description Height The height of the cylinder. Must be greater than 0. Diameter The diameter of the cylinder. Must be greater than 0. Tessellation The tessellation of the cylinder. Must be greater than 3. Pyramid Parameters The component for create this primitive is PyramidMesh Parameter Description Size The size of the pyramid. Must be greater than 0. Torus Parameters The component for create this primitive is TorusMesh Parameter Description Diameter The diameter of the torus. Must be greater than 0. Thickness The thickness of the torus. Must be greater than 0. Tessellation The tessellation of the torus. Must be greater than 3."
  },
  "manual/graphics/render_layers.html": {
    "href": "manual/graphics/render_layers.html",
    "title": "Render Layers | Evergine Doc",
    "keywords": "Render Layers Coming soon"
  },
  "manual/graphics/rendering_overview.html": {
    "href": "manual/graphics/rendering_overview.html",
    "title": "Rendering Overview | Evergine Doc",
    "keywords": "Rendering Overview Coming soon"
  },
  "manual/graphics/samplers.html": {
    "href": "manual/graphics/samplers.html",
    "title": "Samplers | Evergine Doc",
    "keywords": "Samplers Coming soon"
  },
  "manual/graphics/sprites.html": {
    "href": "manual/graphics/sprites.html",
    "title": "Sprites | Evergine Doc",
    "keywords": "Sprites image Sprites are 2D images used in a 2D context. They are simulated in Evergine as quad oriented to the camera. The Sprites are batched automatically for the SpriteBatch allowing drawing a huge amount of sprites with a single drawcall. The Sprite could be useful to create special UI elements over your 3D scene as a compass, or indicators. The Sprites are not available from de Evergine Studio by now so if you want to create a sprite you need to create it from code. Create Sprite from Code The Sprite is an entity with the components Sprite and SpriteRenderer. To draw a Sprite is necessary to create a Camera2D. A Camera3D and a Camera2D can be combined in a scene to draw the 3d objects layer and 2d objects as a new layer over the 3D. In the following example are created a Camera2D and a Sprite. protected override void CreateScene() { var assetsService = Application.Current.Container.Resolve<AssetsService>(); // Load default texture Texture texture = assetsService.Load<Texture>(EvergineContent.Textures.car_png); // Load default sampler SamplerState linearClampSampler = assetsService.Load<SamplerState>(EvergineContent.Samplers.LinearClampSampler); // Load a Render Layer description... RenderLayerDescription layer = assetsService.Load<RenderLayerDescription>(EvergineContent.RenderLayers.Alpha); // Add Sprite var sprite = new Entity() .AddComponent(new Transform2D() { Origin = Vector2.One * 0.5f, // Center pivot }) .AddComponent(new Sprite() { Texture = texture, Sampler = linearClampSampler, }) .AddComponent(new SpriteRenderer() { Layer = layer }); this.Managers.EntityManager.Add(sprite); // Add Camera var camera2d = new Entity(\"Camera\") .AddComponent(new Transform2D()) .AddComponent(new Camera2D() { BackgroundColor = Color.CornflowerBlue * 0.3f, }); this.Managers.EntityManager.Add(camera2d); } The result:"
  },
  "manual/graphics/supported_backends/directx11.html": {
    "href": "manual/graphics/supported_backends/directx11.html",
    "title": "DirectX 11 | Evergine Doc",
    "keywords": "DirectX 11 DirectX 11 is one of the most popular graphics APIs used on graphics applications created by Microsoft and supported on all computers using the Windows OS. Microsoft announce DirectX 11 at the Gamefest 2008 event in Seattle and since then three updates were released: DirectX 11.1 was included in Windows 8 DirectX 11.2 was included in Windows 8.1 DirectX 11.3 was released in 2015 and was included in Windows 10. Currently is the most popular graphics API on Windows platform and supported on all devices running Windows 10. This is the default graphics APIs used by the Evergine Studio as well. Supported DirectX 11 devices Windows 8/10/11 x64/x86 desktop Windows 10/11 ARM64/x86 tablet HoloLens 2 ARM64 Checking DirectX 11 version The DxDiag tool reports detailed information about the DirectX components and drivers installed on your Windows system and it is available on Windows 7/8/10/11. To get the latest DirectX version on your system used Windows update, not all version of Windows can run the latest version of DirectX 11. For more information visit Microsoft support. Create a Graphics Context To create a graphics context based on DirectX11 just write: GraphicsContext graphicsContext = new Evergine.DirectX11.DX11GraphicsContext(); graphicsContext.CreateDevice(); Build & Run DirectX 11 is used by default for Evergine so not actions are required to active it. You can run on DirectX 11 by clicking on File -> Build & Run -> Windows.DirectX11 or just press F5 inside the Evergine Studio."
  },
  "manual/graphics/supported_backends/directx12.html": {
    "href": "manual/graphics/supported_backends/directx12.html",
    "title": "DirectX 12 | Evergine Doc",
    "keywords": "DirectX 12 DirectX12 is the latest Microsoft graphics API available on all devices running Windows 10/11. Microsoft announced this version at GDC on March 20 2014, and was officially launched alongside Windows 10 on July 29 2015. At October 2 2018 the DirectX Raytracing support was added to the API and it become very popular quickly and currently all the latest GPUs from Nvidia and AMD come with Raytracing support which is supported on Evergine low level API so DirectX12 support Evergine raytracing API. Supported DirectX 12 devices Windows 10/11 x64/x86 desktop Windows 10/11 ARM64/x86 tablet Checking DirectX 12 version The DxDiag tool reports detailed information about the DirectX components and drivers installed on your Windows system and it is available on Windows 10/11. To get the latest DirectX version on your system used Windows update. For more information visit Microsoft support. Create a Graphics Context To create a graphics context based on DirectX12 just write: GraphicsContext graphicsContext = new Evergine.DirectX12.DX12GraphicsContext(); graphicsContext.CreateDevice(); Build & Run You can select DirectX 12 API support during the new project creation from the Evergine launcher. If the project already exists you can add the DirectX 12 support from Evergine Studio by clicking on Settings -> Project Settings. Selecting and adding the profile for Windows (DirectX12). You can run on DirectX 12 by clicking on File -> Build & Run -> Windows.DirectX12."
  },
  "manual/graphics/supported_backends/index.html": {
    "href": "manual/graphics/supported_backends/index.html",
    "title": "Supported Graphics Backends | Evergine Doc",
    "keywords": "Supported Graphics Backends Evergine supported multiples graphics APIs including DirectX11, DirectX12, Vulkan, OpenGL, Metal and WebGL/WebGPU. In the previous image you can see and overall diagram where all this APIs are represented. All the most recent APIs are supported and on top of all of them exists a common low level graphics API used by Evergine to abstract the final graphics API used by the applications. In this section DirectX11 DirectX12 OpenGL Vulkan Metal WebGPU"
  },
  "manual/graphics/supported_backends/metal.html": {
    "href": "manual/graphics/supported_backends/metal.html",
    "title": "Metal | Evergine Doc",
    "keywords": "Metal Metal is a low-level graphics and compute API created by Apple and is the default API on MacOS and iOS devices. Metal has been available since June 2 2014 on iOS devices powered by Apple A7 or later and sice June 8 2015 on MacOS running OS X El capitan. Evergine uses Metal API on MacOS and iOS devices like iPhone and IPad but also supports Vulkan graphics API on those platforms through MoltenVK which allows to convert a subset of Vulkan API to run on top of Metal on MacOS and iOS operating systems. Supported Metal devices MacOS x64/x86/ARM64 desktop iOS iPhone and iPad Checking Metal version It is the default graphics API used on iOS and MacOS devices and Apple updates through updates. To make sure you have the latest version just update to the latest OS version. Create a Graphics Context To create a graphics context based on Metal just write: GraphicsContext graphicsContext = new Evergine.Metal.MTLGraphicsContext(); graphicsContext.CreateDevice(); Build & Run The project template will be available soon."
  },
  "manual/graphics/supported_backends/opengl.html": {
    "href": "manual/graphics/supported_backends/opengl.html",
    "title": "OpenGL | Evergine Doc",
    "keywords": "OpenGL Open Graphics Library (OpenGL) is the most widely adopted 2D and 3D graphics API in the industry cross-platform. Silicon Graphics Inc, began developing OpenGL in 1991 and released it on June 30 1992, and now is a technology maintained by Khronos Group. Evergine use OpenGL graphics API on Web platform and Windows desktop but is deprecating this technology in favor of Vulkan, which is the new modern graphics API created by the Khronos Group. OpenGL is used on Web platform by Evergine through the version named WebGL. This is the default version supported in the most popular browsers. Chrome, Edge, Firefox support WebGL 2.0 Safari supports WebGL 1.0 Supported OpenGL devices Windows 8/10/11 x64/x86 desktop Web Browsers desktop, tablet and mobile. Checking OpenGL version If you a running Windows 7 or later the OpenGL library has already been installed on your system. To check the OpenGL version available on your system just find out the control panel of your graphics card or you can download the OpenGL Hardware Capability Viewer. Create a Graphics Context To create a graphics context based on OpenGL just write: GraphicsContext graphicsContext = new Evergine.OpenGL.GLGraphicsContext(); graphicsContext.CreateDevice(); To create a graphics context based on WebGL just write: GraphicsContext graphicsContext = new Evergine.OpenGL.GLGraphicsContext(GraphicsBackend.WebGL2); graphicsContext.CreateDevice(); Build & Run You can select OpenGL API support during the new project creation from the Evergine launcher. Desktop If the project already exists you can add the OpenGL support from Evergine Studio by clicking on Settings -> Project Settings. Selecting and adding the profile for Windows (OpenGL). You can run on OpenGL by clicking on File -> Build & Run -> Windows.OpenGL. WebGL To support Web Platforms based on WebGL versions you also need to add from project setting the WebGL Template, selecting WebGL 2.0 or 1.0 depends your project needs."
  },
  "manual/graphics/supported_backends/vulkan.html": {
    "href": "manual/graphics/supported_backends/vulkan.html",
    "title": "Vulkan | Evergine Doc",
    "keywords": "Vulkan Vulkan is a low-overhead cross-platform API, It is the most recently API created by Khronos Group and targets high-performance real-time 3D graphics applications. Compared with DirectX 11 and OpenGL, Vulkan offer higher performance and more balanced CPU and GPU usage and provides a considerably lower-level API and parallel tasking for the application. Khronos Group announced this API for first time at GDC 2015 referred to as the \"next generation OpenGL initiative\" or \"OpenGL next\" but finally the rename it to Vulkan. Vulkan is not backwards compatible with OpenGL and is derived from AMD's Mantle API which was donated by AMD to Khronos to try to standardize a low level API across the industry. Raytracing support was added recently to Vulkan and the API is very similar to DirectX12 Raytracing API so Vulkan support Evergine raytracing API. Supported Vulkan devices Windows 8/10/11 x64/x86 desktop Linux x64/x86 desktop Android ARMv7/ARM64 tablet and mobile MacOS x64/x86/ARM64 desktop (using MoltenVK) iOS ARM64 tablet and mobile (using MoltenVK) Checking Vulkan version To make sure that you can visualize the rendering options Vulkan has available on your development PC, you will need to download and install the latest version of video card drivers for your graphics card. Download it from: Nvidia AMD It is highly recommended download LunarSDK to install the latest developing and debugging tools for Vulkan graphics API. Create a Graphics Context To create a graphics context based on Vulkan just write: GraphicsContext graphicsContext = new Evergine.Vulkan.VKGraphicsContext(); graphicsContext.CreateDevice(); Build & Run You can select Vulkan API support during the new project creation from the Evergine launcher. If the project already exists you can add the Vulkan support from Evergine Studio by clicking on Settings -> Project Settings. Selecting and adding the profile for Windows (Vulkan). You can run on Vulkan by clicking on File -> Build & Run -> Windows.Vulkan."
  },
  "manual/graphics/supported_backends/webgpu.html": {
    "href": "manual/graphics/supported_backends/webgpu.html",
    "title": "WebGPU | Evergine Doc",
    "keywords": "WebGPU WebGPU is a working project to become in a future web standard API for accelerated graphics and compute, aiming to provide modern 3D graphics and computation capabilities on Web platform. It is developed by the W3C for the Web community group with engineers from Apple, Mozilla, Microsoft, Google and others. This technology run on top of the latest graphics APIs like DirectX 12, Vulkan and Metal. Chrome, Edge and Firefox already support it as experimental feature so you need to move to beta channel and active it. Evergine uses WebGPU on Web platforms when it is available otherwise use WebGL which is the most supported graphics technology today. To check the implementation status of this technology visit webgpu.io. Supported WebGPU devices Chrome, Edge, Safari and firefox browsers on Desktop, tablet and mobile. Checking WebGPU version Browser Check command Chrome The WebGPU flag must be enabled by writing this in your browser: chrome://flags/#enable-unsafe-webgpu Edge The WebGPUflag must be enabled by writing this in your browser: edge://flags/#enable-unsafe-webgpu Create a Graphics Context To create a graphics context based on WebGPU just write: GraphicsContext graphicsContext = new Evergine.WebGPU.WGPUGraphicsContext(); graphicsContext.CreateDevice(); Build & Run on WebGPU The project template will be available soon."
  },
  "manual/graphics/textures/create_texture_from_code.html": {
    "href": "manual/graphics/textures/create_texture_from_code.html",
    "title": "Create a texture from code | Evergine Doc",
    "keywords": "Create a texture from code The most common use Texture is assigning them into Materials and Components. However, it's perfectly valid to use and even create a Texture from code. Load Texture asset from code As explained in this article, it's perfectly possible. Here is a sample code for creating a primitive entity with a Diffuse material. protected override void CreateScene() { AssetSceneManager assets = this.Managers.AssetSceneManager; // Loading 'Diffuse.png' located in 'Content/Textures/ Texture diffuseTexture = assets.Load<Texture>(EvergineContent.Textures.Diffuse**png); // We create a standard material and assign the texture as diffuse channel. StandardMaterial materialDecorator = new StandardMaterial(assets.Load<Effect>(EvergineContent.Effects.StandardEffect)); material.BaseColorTexture = diffuseTexture; // We create a primitive Entity teapot = new Entity(\"texturedTeapot\") .AddComponent(new Transform3D()) .AddComponent(new TeapotMesh()) .AddComponent(new MaterialComponent(){ Material = materialDecorator.Material}) .AddComponent(new MeshRenderer()); this.Managers.EntityManager.Add(teapot); } Create a Texture from code Creating a Texture demands a little bit more effort, and it's defined in this article. Bassically we need to define two main things: TextureDescription structure. DataBoxes with the texture data. TextureDescription The TextureDescription struct that contains all the specifications of the Texture so the graphic card can properly load the buffer data accordingly and be able to extract all their information. Property Values Description TextureType Texture2D, Texture2DArray, Texture1D, Texture1DArray, TextureCube, TextureCubeArray, Texture3D The type of the texture. Width unsigned integer Width of the texture (first dimmension). The maximum value is defined by the device hardware. Height unsigned integer Height of the texture (second dimmension). The maximum value is defined by the device hardware. Depth unsigned integer Depth of the texture (third dimension). Used in Texture3D. The maximum value is defined by the device hardware. ArraySize unsigned integer The number of textures in a Texture Array (either 1D, 2D or Cube). Faces unsigned integer The number of texture faces used in TextureCube and TextureCubeArray. MipLevels unsigned integer Maximum number of mipmap levels in the Texture. ResourceUsage Default: Requires read and write acces from the GPU. Immutable: Can only be read by GPU. Cannot be writtend or accessed by CPU. Dynamic: Can be accessed by the GPU (read only) and the CPU (write only). Used for textures updated in CPU. Staging: Supports data transfer (copy) from the GPU to the CPU. Type of access of the Texture. Usage None, Count2, Count4, Count8, Count16, Count32 Number of samples in the Texture. DataBoxes A DataBox represents a data buffer that contains all pixel of an element of texture. Every mipmap level, array slice or cube face defines its own DataBox."
  },
  "manual/graphics/textures/import_textures.html": {
    "href": "manual/graphics/textures/import_textures.html",
    "title": "Import a texture | Evergine Doc",
    "keywords": "Import a texture In Evergine Studio, importing an image file will create a Texture asset, as explained in this article. Inspect Textures in Asset Details You can find the texture assets in the Assets Details panel when you select a folder in the Project Explorer. Texture files in content directory Textures imported in Evergine create an aditional metadata .wetx file. Supported formats: Evergine supports the following image formats: Extension Compression Alpha Bits per pixel Supported texture types .jpg, jpeg Lossy compression. Configurable No 24 Texture2D .png Lossless compression Yes 8, 24, 48 Texture2D .bmp No compression Yes (not common) 24 Texture2D .gif Indexed colors Yes 8 Texture2D .tga No Yes 32 Texture2D .hdr No No 48 or 96 (high dynamic range) Texture2D .dds Yes (S3, DXT1, DXT3, DXT5) Yes Multiple Texture2D, Texture2DArray, Texture1D, Texture1DArray, TextureCube, TextureCubeArray, Texture3D .ktx Yes (ETC1S) Yes Multiple Texture2D, Texture2DArray, Texture1D, Texture1DArray, TextureCube, TextureCubeArray, Texture3D Sampler State association A Texture graphic resource needs a Sampler State asset for properly filtering it. That's why the Texture asset contains a reference to a SamplerState of your project. This way the Texture has a default SamplerState associated. Evergine will automatically use it."
  },
  "manual/graphics/textures/index.html": {
    "href": "manual/graphics/textures/index.html",
    "title": "Textures | Evergine Doc",
    "keywords": "Textures Textures are assets that usually contains an image. In Evergine they are mostly used in materials to provide color detail in your application. How the texture is viewed in your application depends of the material itself. For example, a texture can be used as diffuse or emissive color information. Textures can also be used in other areas, like Sprites or UI elements. Mipmapping Evergine also supports mipmapping. It can generate or loads the successive half reduction of the texture, consisting the mip levels. This process is crucial when dealing with Texture Filtering (anisotropic, linear, bilinear, etc). Texture types Evergine supports these basic GPU textures types (They are detailed in this section) Texture2D Texture1D Texture1DArray Texture2DArray TextureCube TextureCubeArray Texture3D Supported file types. Evergine supports importing the following texture types: .png .jpg .jpeg .bmp .gif .tga .dds .ktx .hdr Note Evergine will only import the first frame of any animated image file like .gif and will load like an static Texture. In this section Texture types Import Textures How to create a texture from code Texture Editor"
  },
  "manual/graphics/textures/texture_editor.html": {
    "href": "manual/graphics/textures/texture_editor.html",
    "title": "Texture Editor | Evergine Doc",
    "keywords": "Texture Editor Texture Editor allows the editing of texture assets. Double click over a Texture asset shown in Assets Details will open this editor. The editor is composed of 3 main parts: Viewport Shows the Texture with the current configuration. It contains a label with the following information: TextureType Resolution in pixels Pixel Format. Size of the texture on disc. Note An example of descriptive text would be Texture2D 4096x4096 px R8G8B8A8_Unorm.- Toolbox Helps with the texture visualization. Has the following options: Item Description Each button enables or disables the Texture channels. Slider that sets the current Mipmap level of the texture. This control will be hidden in case of textures without mipmapping Sets the background color on the Viewport. Properties Panel with all the Texture properties. They don't depend on the profile. Property Description GenerateMipmaps If Evergine will generate all the mipmaps for the Texture. PremultipliedAlpha If the RGB channels are multiplied by the Alpha (A) channel. Sampler The SamplerState asset that will defines how the Texture is sampled and filtered. NinePathType (Currently not supported ) Sets the ninepath information of the image. It defines how the texture can stretch un a ui component. Profile Properties Properties that can be changed in every app profile. Property Description ScalingType Sets how the texture will be scaled: Original: Don't affect the image size. Percentage: Scale the image using the ScaledPercentage value. FreeForm: Set the Texture size directy using the ScaledWidth and ScaledHeight. PowerOfTwo: Scales to the smallest power of two size per dimension. SquarePowerOfTwo: Scales to the smallest square power of two size per dimension. ScaledPercentage Defines the scale factor of the texture when using Percentage scaling type. 1.0 by default. ScaledWidth Defines the width of Texture when using Freeform scaling type. ScaledHeight Defines the height of Texture when using Freeform scaling type. PixelFormat Defines the size, elements and name of Texture pixels."
  },
  "manual/graphics/textures/textureTypes.html": {
    "href": "manual/graphics/textures/textureTypes.html",
    "title": "Texture Types | Evergine Doc",
    "keywords": "Texture Types Modern graphic APIs like DirectX12 or Vulkan supports a wide range of Texture types because there are many ways that a Texture can be accessed and presented. Evergine supports all of them. However, some of them are only loaded using some specific image formats (like .dds or .ktx); Texture 2D The classic two dimensional texture. It has width and height and as a graphic resource can be accessed using two texture coordinates UV. Almost every image file can be imported as a Texture2D asset because it's the most common one. Usage They are used for almost everything, like Sprites, Diffuse channel, Normal mapping. Texture 2D Array The same as the Texture2D above, but it contains an array of textures. Modern graphics APIs support texture arrays, which is an array of textures with the same size and format. They are treated as the same graphic resource, it also can have mipmapping information and can be sampled using an extra texture coordinate that indicates which array item is needed to be sampled. Usage They are useful, for example, for creating sprite atlas or animations, which every element of the array containing a frame of the animation or atlas. Texture 1D Texture that only contains only one dimension ( width ) and are sampled using only one texture coordinate. Usage Useful when dealing with one dimensional info like gradients, for example. Texture 1D Array Array of one dimensional textures as specified before. It's sampled using two texture coordinates: one for the texture address and another for the element in the array. Usage Using multiple gradients for a custom shader. Texture Cube It's commonly known as Cubemap. It's a texture that contains 6 individual 2D textures that each form one side of a cube. It has the useful property that they can be sampled using a direction vector instead of texture coordinates. It's like having a 1x1x1 cube and a vector with the sampling direction at its center. It will return the texel placed in the intersection point. Like in the following diagram. Usage TextureCubes are mainly used as environment textures for IBL, like radiance and irradiance texture cubes for specular or diffuse components, or creating a Skyboxes for creating the background environment of your scene. Texture Cube Array Texture Cube Arrays take a step further than the normal cubemap and stores an array of texture cubes, so they need an aditional coordinate to retrieve the element of the array. Usage TextureCubesArray is an intersting choice when creating Reflection Probes of your scene. in one resource can store all the probes of the scene. Texture 3D A 3D Texture is an image that contains information in three dimensions rather the standard two. Usage They are commonly used to simulate volumetric effects such fog, smoke, for raymarching operations or for sprite animation blending."
  },
  "manual/index.html": {
    "href": "manual/index.html",
    "title": "Evergine manual | Evergine Doc",
    "keywords": "Evergine manual These pages contain information about how to use Evergine. This manual helps you learn how to use Evergine and its associated tools. You can read it from start to finish, or use it as a reference Note This documentation is a work-in-progress and updated regularly with new content. If you find something that can be improved, please contact us in the feedback repository In this section Getting started Basics Evergine Studio Platforms Graphics Input Audio Physics XR Extensions Addons"
  },
  "manual/input/button_states.html": {
    "href": "manual/input/button_states.html",
    "title": "Button states | Evergine Doc",
    "keywords": "Button states The input API defines four valid states for a button. The transition between different states is the same no matter which platform Evergine is running. When an input key event is raised by the operating system, it is received by an ButtonStateTracker that handles the state changes of the key. States State Description Released The key or button is not pressed for more than one frame in a row. Pressing The key or button is pressed and is transitioning from Released to Pressed state. This state is an intermediate state (rising edge) and will last only one frame. Pressed The key or button is pressed for more than one frame in a row. Releasing The key or button is not pressed and is transitioning from Pressed to Released state. This state is an intermediate state (falling edge) and will last only one frame. Tip Pressing and Releasing states are very useful to trigger an action only once every time a key is pressed."
  },
  "manual/input/index.html": {
    "href": "manual/input/index.html",
    "title": "Input | Evergine Doc",
    "keywords": "Input Reading the application input is the most essential part of providing user interaction in graphic applications. Every application should support at least one input device. Evergine captures keyboard, mouse and touch from different surfaces and map the results into a unified API with same key definitions and expected behavior. Input and Surfaces In Evergine, each application Surface exposes different Input dispatchers. So, for example, an Evergine application could launch two separate windows, and each surface may be interacted only when this window is focused. In this section Button states Keyboard Mouse Touch"
  },
  "manual/input/keyboard.html": {
    "href": "manual/input/keyboard.html",
    "title": "Keyboard | Evergine Doc",
    "keywords": "Keyboard The keyboard is the most common input device on desktop platforms. You can acces the keyboard state by using KeyboardDispatcher. KeyboardDispatcher The KeyboardDispatcher is a class used to track keyboard key events. public abstract class KeyboardDispatcher { public event EventHandler<KeyCharEventArgs> KeyChar; public event EventHandler<KeyEventArgs> KeyDown; public event EventHandler<KeyEventArgs> KeyUp; public bool IsKeyDown(Keys key); public ButtonState ReadKeyState(Keys key); } Events Events Description KeyChar It occurs when a key is pressed and a character is generated. KeyChar event is useful for text input. KeyDown and KeyUp This events are available to track keyboard pressed keys but it is recommended to use IsKeyDown and ReadKeyState methods. IsKeyDown: Gets a value indicating whether the current state of a keyboard key is Pressing or Pressed. ReadKeyState: Gets the current state of a keyboard key. Using KeyboardDispatcher The KeyboardDispatcher can be found within the Display or Surface objects. The following sample code can be used to access the keyboard dispatcher from a Component or Service. [BindService] protected GraphicsPresenter graphicsPresenter; protected override void Update(TimeSpan time) { KeyboardDispatcher keyboardDispatcher = this.graphicsPresenter.FocusedDisplay?.KeyboardDispatcher; if (keyboardDispatcher?.ReadKeyState(Keys.A) == ButtonState.Pressing) { // Do something } }"
  },
  "manual/input/mouse.html": {
    "href": "manual/input/mouse.html",
    "title": "Mouse | Evergine Doc",
    "keywords": "Mouse Mouse is the most common input device on desktop platforms. You can access the mouse state by using the MouseDispatcher. MouseDispatcher The MouseDispatcher is a class used to track mouse button events. It inherits from PointerDispatcher so it can be used to produce touch events using the mouse. public abstract class MouseDispatcher : PointerDispatcher { public MouseButtons State { get; } public Point ScrollDelta { get; } public Point PositionDelta { get; } public Point Position { get; } public abstract CursorTypes CursorType { get; } public bool IsMouseOver { get; } public event EventHandler<MouseButtonEventArgs> MouseButtonUp; public event EventHandler<MouseButtonEventArgs> MouseButtonDown; public event EventHandler<MouseEventArgs> MouseLeave; public event EventHandler<MouseEventArgs> MouseEnter; public event EventHandler<MouseEventArgs> MouseMove; public event EventHandler<MouseScrollEventArgs> MouseScroll; public bool IsButtonDown(MouseButtons button); public ButtonState ReadButtonState(MouseButtons button); public bool TrySetCursorPosition(Point position); public abstract bool TrySetCursorType(CursorTypes cursorType); } Properties It gives you the following properties: Properties Description State Gets a flag enum that indicates which mouse buttons are pressed at the current frame. Position Gets the mouse absolute screen position at the current frame. PositionDelta property gets the mouse delta position since the last frame. In other words, it describes how much the mouse has. ScrollDelta Gets the mouse scroll increment since the last frame. The value X indicates a horizontal scroll increment. The value is positive if the mouse wheel is rotated to the right or negative if the mouse wheel is rotated to the left. The value Y indicates a vertical scroll increment. The value is positive if the mouse wheel is rotated in an upward direction (away from the user) or negative if the mouse wheel is rotated in a downward direction (toward the user). CursorType property gets the mouse active cursor type. IsMouseOver Indicates if the mouse is over the Surface. Events You can be subscribed to events to ve notified when mouse state change: Events Description MouseMove and MouseScroll This events track changes in mouse position and scroll. MouseButtonDown and MouseButtonUp This events are available to track mouse pressed buttons but it is recommended to use IsButtonDown and ReadButtonState methods: IsButtonDown: Gets a value indicating whether the current state of a mouse button is Pressing or Pressed. ReadButtonState: Gets the current state of a mouse button. MouseLeave and MouseEnter They indicate if the mouse enter or leave the Surface, so, they track changes in the IsMouseOver property. Useful Methods Events Description TrySetCursorPosition Try to update the cursor position. When this method is supported by the patform, this method will return true. TrySetCursorType Try to update the cursor type. When this method is supported by the patform, this method will return true. Using MouseDispatcher The MouseDispatcher can be found within the Display or Surface objects. The following sample code can be used to access the mouse dispatcher from a Component or Service. [BindService] protected GraphicsPresenter graphicsPresenter; protected override void Update(TimeSpan time) { MouseDispatcher mouseDispatcher = this.graphicsPresenter.FocusedDisplay?.MouseDispatcher; if (mouseDispatcher?.ReadButtonState(MouseButtons.Left) == ButtonState.Pressing) { // Do something } }"
  },
  "manual/input/touch.html": {
    "href": "manual/input/touch.html",
    "title": "Touch | Evergine Doc",
    "keywords": "Touch Touch is the most common input system on mobile devices. Pointers are points on the device screen corresponding to finger touches. Devices with multi-touch functionality support multiple simultaneous pointers. PointerDispatcher The PointerDispatcher is a class used to track pointer events. public abstract class PointerDispatcher { public IList<PointerPoint> Points { get; } public event EventHandler<PointerEventArgs> PointerDown; public event EventHandler<PointerEventArgs> PointerUp; public event EventHandler<PointerEventArgs> PointerMove; } Points property gets a list of points detected inside the surface at the current frame. PointerDown, PointerUp and PointerMove events are available to track touch and pointer points. PointerPoint The PointerPoint class give the following information: Property Description Id A number that is uniquely associated to this touch. It is usually given by the underliying platform. Position The pointer position in screen coordinates State Get the current pointer state. See Button States for more information. Using PointerDispatcher The PointerDispatcher can be found within the Display or Surface objects. The following sample code can be used to access the pointer dispatcher from a Component or Service. [BindService] protected GraphicsPresenter graphicsPresenter; protected override void Update(TimeSpan time) { PointerDispatcher pointerDispatcher = this.graphicsPresenter.FocusedDisplay?.PointerDispatcher; if (keyboardDispatcher == null) { return; } foreach (PointerPoint p in keyboardDispatcher.Points) { if (p.State == ButtonState.Pressing) { // Do something } } }"
  },
  "manual/physics/colliders/box_collider.html": {
    "href": "manual/physics/colliders/box_collider.html",
    "title": "Box Collider | Evergine Doc",
    "keywords": "Box Collider A Box shaped collider. BoxCollider3D component To use a Box Collider in Evergine, you only need to add a BoxCollider3D component to your entity: Properties Property Default Description Size 1,1,1 This property define the size of the Box collider. The Size value can be used in two ways: If the Entity has a mesh (with MeshComponent for example), the Size value is relative to the mesh extents. In that case, a value of 1,1,1 let the BoxCollider3D to fit the entity mesh. If the entity hasn't any meshes, the Size value is used as scene units. In that case, a value of 2,2,2 will create a box collider of 2x2x2 units. Offset 0,0,0 Offset the collider respect the owner entity. The Offset value can be used in two ways: If the Entity has a mesh (with MeshComponent for example), the Offset value is relative to the mesh extents. If the entity hasn't any meshes, the Offset value is used as scene units. RotationOffset 0,0,0 Apply to the Collider a rotation offset respect the owner entity. Margin 0.04 Physic Engine uses a small collision margin for collision shapes, to improve performance and reliability of the collision detection."
  },
  "manual/physics/colliders/capsule_collider.html": {
    "href": "manual/physics/colliders/capsule_collider.html",
    "title": "Capsule Collider | Evergine Doc",
    "keywords": "Capsule Collider A Capsule shaped collider. A capsule is a special shape defined by two properties: Radius: The radius of the collider's local width. Height: The total height of the collider. SphereCollider3D component To use a Sphere Collider in Evergine, you only need to add a SphereCollider3D component to your entity: Properties Property Default Description Radius 0.5 This property define the radius of the Capsule collider. The Radius value can be used in two ways: If the Entity has a mesh (with MeshComponent for example), the Radius value is relative to the mesh extents. If the entity hasn't any meshes, the Radius value is used as scene units. Height 1 This property define the total height of the Capsule collider. The Height value can be used in two ways: If the Entity has a mesh (with MeshComponent for example), the Height value is relative to the mesh height. If the entity hasn't any meshes, the Radius value is used as scene units. Offset 0,0,0 Offset the collider respect the owner entity. The Offset value can be used in two ways: If the Entity has a mesh (with MeshComponent for example), the Offset value is relative to the mesh extents. If the entity hasn't any meshes, the Offset value is used as scene units. Margin 0.04 Physic Engine uses a small collision margin for collision shapes, to improve performance and reliability of the collision detection."
  },
  "manual/physics/colliders/cone_collider.html": {
    "href": "manual/physics/colliders/cone_collider.html",
    "title": "Cone Collider | Evergine Doc",
    "keywords": "Cone Collider A Cone shaped collider. ConeCollider3D component To use a Sphere Collider in Evergine, you only need to add a ConeCollider3D component to your entity: Properties Property Default Description Radius 0.5 This property define the radius of the Cone collider. The Radius value can be used in two ways: If the Entity has a mesh (with MeshComponent for example), the Radius value is relative to the mesh extents. If the entity hasn't any meshes, the Radius value is used as scene units. Height 1 This property define the total height of the Cone collider. The Height value can be used in two ways: If the Entity has a mesh (with MeshComponent for example), the Height value is relative to the mesh height. If the entity hasn't any meshes, the Radius value is used as scene units. Offset 0,0,0 Offset the collider respect the owner entity. The Offset value can be used in two ways: If the Entity has a mesh (with MeshComponent for example), the Offset value is relative to the mesh extents. If the entity hasn't any meshes, the Offset value is used as scene units. Margin 0.04 Physic Engine uses a small collision margin for collision shapes, to improve performance and reliability of the collision detection."
  },
  "manual/physics/colliders/cylinder_collider.html": {
    "href": "manual/physics/colliders/cylinder_collider.html",
    "title": "Cylinder Collider | Evergine Doc",
    "keywords": "Cylinder Collider A Cylinder shaped collider. CylinderCollider3D component To use a Cylinder Collider in Evergine, you only need to add a CylinderCollider3D component to your entity: Properties Property Default Description Radius 0.5 This property define the radius of the Cylinder collider. The Radius value can be used in two ways: If the Entity has a mesh (with MeshComponent for example), the Radius value is relative to the mesh extents. If the entity hasn't any meshes, the Radius value is used as scene units. Height 1 This property define the total height of the Cylinder collider.The Height value can be used in two ways: If the Entity has a mesh (with MeshComponent for example), the Height value is relative to the mesh height. If the entity hasn't any meshes, the Radius value is used as scene units. Offset 0,0,0 Offset the collider respect the owner entity. The Offset value can be used in two ways: If the Entity has a mesh (with MeshComponent for example), the Offset value is relative to the mesh extents. If the entity hasn't any meshes, the Offset value is used as scene units. Margin 0.04 Physic Engine uses a small collision margin for collision shapes, to improve performance and reliability of the collision detection."
  },
  "manual/physics/colliders/index.html": {
    "href": "manual/physics/colliders/index.html",
    "title": "Colliders | Evergine Doc",
    "keywords": "Colliders Colliders are used to define the physical shape of a Physic Body. Colliders are invisible and don't need to be the exact same shape as the object mesh and in a fact, a rough approximation is often more efficient and indistinguishable in your application. Evergine provides various collider types, such as Box, Sphere or Capsule. Using simple collider shapes helps with application performance optimizations. However, if you need more detailed and exact collision for your object, use the Mesh collider. Physics Bodies and Colliders As we mentioned before in Physics Bodies section. A Collider needs a Physic Body to be attached, and a Physic Body itself needs Colliders to define their shape. A Physic Body search their attached collider in their child hierarchy, including the owner's entity of the physic body. This also implies that a Physic Body could have multiple colliders attached to it. Simple shapes This is the most common scenario. Your entity has a Physic Body component (RigidBody3D in that case), and one or more colliders attached to the same entity (BoxCollider3D in that case). Compound shapes This is another possible scenario. In that case we have an entity hierarchy. In that case, the parent entity has a StaticBody3D component, and we have added several entities containing several colliders (a BoxCollider3D and a SphereCollider3D). This cause that the final shape of the body is compounded of all attached colliders. Tip As a rule of thumb, create a physical body when you want to interact with it indistinguishable of other bodies. For example, to capture collision events. In this section Box Collider Sphere Collider Capsule Collider Cylinder Collider Cone Collider Mesh Collider"
  },
  "manual/physics/colliders/mesh_collider.html": {
    "href": "manual/physics/colliders/mesh_collider.html",
    "title": "Mesh Collider | Evergine Doc",
    "keywords": "Mesh Collider A collider represented by an arbitrary mesh. A Mesh Collider uses the owner entity meshes to define their shape. They use all MeshComponent to obtain meshes and create colliders with them. Types of Mesh Collider Default mode By default, a Mesh Collider uses the entire triangle mesh to generate a collider shape. This will create the better precision and fidelity. However, only Static Bodies can have Mesh Colliders in the default mode. Mesh Colliders in the default mode is more suitable to create collisions for static scenery objects, such as walls, terrain, props, etc... Important Only Static Bodies can have Mesh Colliders in the default mode. Convex Hull If you want to use Mesh Colliders in dynamic bodies like Rigid Bodies, you need to set your Mesh Collider to use a Convex Hull. After using this mode, the Physic Engine creates a convex approximation of the mesh, allowing it to be used in dynamic bodies. As a counterpart, the precision of the collision is degraded. Convex colliders are suitable for movable physics objects like, chairs, tables, stones, etc... MeshCollider3D component To use a Mesh Collider in Evergine, you only need to add a MeshCollider3D component to your entity. Note Is obvious, but to use a MeshCollider3D, the owner entity is required to have at least one MeshComponent Properties Property Default Description Size 1,1,1 This property allows you to scale the generated Mesh Collider. Offset 0,0,0 Position offset of the collider respect the owner entity. The units is relative of the size of the entity mesh. RotationOffset 0,0,0 Apply to the Collider a rotation offset respect the owner entity. Margin 0.04 Physic Engine uses a small collision margin for collision shapes, to improve performance and reliability of the collision detection. AsyncShapeCreation false Allows to create the Mesh colliders in an asynchronous way. Create mesh colliders could be very CPU intensive. By default the execution thread is blocked until the Mesh Collider is generated. If this property is set to true the generation is done in a thread apart, releasing the main thread. However, it is possible that some frames the collider won't work."
  },
  "manual/physics/colliders/sphere_collider.html": {
    "href": "manual/physics/colliders/sphere_collider.html",
    "title": "Sphere Collider | Evergine Doc",
    "keywords": "Sphere Collider A Sphere shaped collider. SphereCollider3D component To use a Sphere Collider in Evergine, you only need to add a SphereCollider3D component to your entity: Properties Property Default Description Radius 0.5 This property define the radius of the Sphere collider. The Radius value can be used in two ways: If the Entity has a mesh (with MeshComponent for example), the Radius value is relative to the mesh extents, to be more specific. In that case, a value of 0.5 let the SphereCollider3D to fit the entity mesh. If the entity hasn't any meshes, the Radius value is used as scene units. In that case, a value of 2 will create a Sphere collider of a radius of 2 units, and a diameter of 4 units. Offset 0,0,0 Offset the collider respect the owner entity. The Offset value can be used in two ways: If the Entity has a mesh (with MeshComponent for example), the Offset value is relative to the mesh extents. If the entity hasn't any meshes, the Offset value is used as scene units. Margin 0.04 Physic Engine uses a small collision margin for collision shapes, to improve performance and reliability of the collision detection."
  },
  "manual/physics/index.html": {
    "href": "manual/physics/index.html",
    "title": "Physics | Evergine Doc",
    "keywords": "Physics Evergine provides real-time physics simulation including collision, gravity and other forces. Using built-in physics engine helps with creating realistic behavior for your scene entities. This section explains how physics work, how to add them to your scene, and how to control them in your custom components. Built-in Physics engine Evergine uses the open-source Bullet Physics engine. For detailed information, see the Bullet User Note Evergine provides an abstraction API that allows you to provide your custom physics integration. In this section PhysicManager and Bullet Physics Bodies Colliders Queries"
  },
  "manual/physics/joints/cone_twist_joint.html": {
    "href": "manual/physics/joints/cone_twist_joint.html",
    "title": "Cone Twist Joint | Evergine Doc",
    "keywords": "Cone Twist Joint For ragdolls, the Cone-Twist Joint is useful for limbs like the upper arm. It is a special point-to-point joint that adds cone and twist axis limits. A Cone-Twist Joint is similar to a Point-to-Point Joint Relationship, but specifies an elliptical cone within which rotation may take place. The cone is described by specifying a maximum rotation on each axis. ConeTwistJoint3D In Evergine, a Cone-Twist Joint is implemented using the ConeTwistJoint3D component. Properties Common Properties Property Default Description ConnectedEntityPath null The entity path of the connected body. Only when the path is valid a Joint is established properly. Anchor 0, 0, 0 The point which defines the center of the joint in source entity local space. All physics-based simulations use this point as the center in calculations. AutoConfigureConnected true Enable this setting to automatically calculate the ConnectedAnchor position to match the global position of the anchor property. This is the default setting. Disable it to configure the position of the connected anchor manually. ConnectedAnchor auto-calculated Manually configure the connected anchor position, in the connected body local space. BreakPoint 0 If the value is greater than 0, Indicates the force that needs to be applied for this joint to break. CollideConnected false Determines whether a collision between the two bodies managed by the joint is enabled. Limit properties Property Default Description ConeTwistSettings 30º, 30º, 30º Defines the limitation angle in each axis. LimitSoftness 0.9 Once an angle is greater than softness * the maximum angle, the constraint begins to take effect. Lowering the value of softness softens the constraint boundaries. LimitBiasFactor 0.3 The rate at which the constraint corrects errors in orientation. A value of 1 will ensure that the constraint is always obeyed. It is recommended to keep bias between 0.2 and 0.5. LimitRelaxationFactor 1 The rate at which the angular velocity is changed by the constraint. A low value means the constraint will modify the velocities slowly, leaving the boundaries appearing softer. Motor Properties The following properties set the motor properties of the joint. Property Default Description UseMotor false If enabled, the motor makes the object spin around. MotorTargetRotation 0, 0, 0 The target angular speed on each axis. MotorTargetImpulse 0 The impulse applied in order to attain the speed. Using Cone-Twist Joint This snippet creates a serie of bodies attached using Cone-Twist joints. protected override void CreateScene() { this.Managers.RenderManager.DebugLines = true; // Load your material var cubeMaterial = this.Managers.AssetSceneManager.Load<Material>(EvergineContent.CrateMat); int chainLength = 4; Entity previousLink = null; // Create the chain... for (int i = 0; i < chainLength; i++) { /// The first object is kinematic (we don't want a falling chain :D) var rigidObjectType = (i == 0) ? RigidBodyType3D.Kinematic : RigidBodyType3D.Dynamic; // Create the link entities... var link = this.CreateCube(cubeMaterial, new Vector3(i, 0, 0), 0.75f, rigidObjectType); if (previousLink != null) { // Limit Joint to 40º or 10º... var angle = MathHelper.ToRadians(i % 2 == 0 ? 40 : 10); // Add a PointToPoint joint to the previous link connected to the current link... previousLink.AddComponent(new ConeTwistJoint3D() { ConnectedEntityPath = link.EntityPath, Anchor = new Vector3(0.5f, 0, 0), // Sets the anchor between the two objects ConeTwistSettings = Vector3.One * angle, // Sets the limit angle... }); } previousLink = link; this.Managers.EntityManager.Add(link); } } private Entity CreateCube(Material material, Vector3 position, float size, RigidBodyType3D rigidBodyType) { Entity cube = new Entity() .AddComponent(new Transform3D() { Position = position }) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new CubeMesh() { Size = size }) .AddComponent(new MeshRenderer()) .AddComponent(new RigidBody3D() // Add a RigidBody3D component... { PhysicBodyType = rigidBodyType, }) .AddComponent(new BoxCollider3D()); // Add a BoxCollider3D to the physic body... return cube; }"
  },
  "manual/physics/joints/fixed_joint.html": {
    "href": "manual/physics/joints/fixed_joint.html",
    "title": "Fixed Joint | Evergine Doc",
    "keywords": "Fixed Joint Fixed Joints restricts an object’s movement to be dependent upon another object. This is somewhat similar to Parenting but is implemented through physics rather than Transform hierarchy. The best scenarios for using them are when you have objects that you want to easily break apart from each other, or connect two object’s movement without parenting. FixedJoint3D In Evergine, a Fixed Joint is implemented using the FixedJoint3D component. Properties Property Default Description ConnectedEntityPath null The entity path of the connected body. Only when the path is valid a Joint is established properly. Axis 1, 0, 0 This is the relative axis which locates the joint frame relative to the target body. Anchor 0, 0, 0 The point which defines the center of the joint in source entity local space. All physics-based simulations use this point as the center in calculations. AutoConfigureConnected true Enable this setting to automatically calculate the ConnectedAnchor position and ConnectedAxis to match the global position of the anchor property. This is the default setting. Disable it to configure the position of the connected anchor and connected axis manually. ConnectedAxis auto-calculated The joint axis relative to the connected body. ConnectedAnchor auto-calculated Manually configure the connected anchor position, in the connected body local space. BreakPoint 0 If the value is greater than 0, Indicates the force that needs to be applied for this joint to break. CollideConnected false Determines whether a collision between the two bodies managed by the joint is enabled. Using Fixed Joint This snippet creates two bodies and add a fixed joint maintaining the relative position at start. This is because the AutoConfigureConnected property is set to true. protected override void CreateScene() { // Load your material var cubeMaterial = this.Managers.AssetSceneManager.Load<Material>(EvergineContent.Materials.CubeMaterial); var floorMaterial = this.Managers.AssetSceneManager.Load<Material>(DefaultResourcesIDs.DefaultMaterialID); // Create the floor var floor = this.CreateFloor(floorMaterial); var cubeA = this.CreateCube(cubeMaterial, new Vector3(0, 2, 0), 0.5f); var cubeB = this.CreateCube(cubeMaterial, new Vector3(0, 3, 0), 0.5f); // Add a Fixed Joint to cubeA... cubeA.AddComponent(new FixedJoint3D(){ ConnectedEntityPath = cubeB.EntityPath // Connect to cubeB }); // Register entities to EntityManager... this.Managers.EntityManager.Add(floor); this.Managers.EntityManager.Add(cubeA); this.Managers.EntityManager.Add(cubeB); } private Entity CreateFloor(Material material) { Entity floor = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new PlaneMesh() { Width = 10, Height = 10 }) // Create a 10x10 floor plane .AddComponent(new MeshRenderer()) .AddComponent(new StaticBody3D()) // Add a StaticBody component... .AddComponent(new BoxCollider3D()); // Add a BoxCollider3D to the physic body... return floor; } private Entity CreateCube(Material material, Vector3 position, float size) { Entity cube = new Entity() .AddComponent(new Transform3D() { Position = position }) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new CubeMesh() { Size = size }) .AddComponent(new MeshRenderer()) .AddComponent(new RigidBody3D()) // Add a RigidBody3D component... .AddComponent(new BoxCollider3D()); // Add a BoxCollider3D to the physic body... return cube; }"
  },
  "manual/physics/joints/gear_joint.html": {
    "href": "manual/physics/joints/gear_joint.html",
    "title": "Gear Joint | Evergine Doc",
    "keywords": "Gear Joint A Gear Joint is a type of constraint that allows you to create a relationship between two rigid bodies that is based on their angular velocities. Specifically, it lets you specify a gear ratio between the angular velocities of the two bodies, which can be used to create realistic mechanical systems like gears, pulleys, and other types of machinery. GearJoint3D In Evergine, a Gear Joint is implemented using the GearJoint3D component. Properties Property Default Description ConnectedEntityPath null The entity path of the connected body. Only when the path is valid a Joint is established properly. Axis 0, 1, 0 The axis rotation of the gear. Ratio 1 Sets the desired angular speed ratio between the two objects. For example, if you want one object to rotate at half the speed of the other object, set the value of Ratio to 0.5. BreakPoint 0 If the value is greater than 0, Indicates the force that needs to be applied for this joint to break. CollideConnected false Determines whether a collision between the two bodies managed by the joint is enabled. Using Gear Joint This snippet creates 3 gears of different sizes interconnected. protected override void CreateScene() { this.Managers.RenderManager.DebugLines = true; // Load your material var material = this.Managers.AssetSceneManager.Load<Material>(DefaultResourcesIDs.DefaultMaterialID); float sliderLength = 3; // Create the slider holder... Entity gear1 = new Entity() .AddComponent(new Transform3D()) .AddComponent(new Spinner() { AxisIncrease = new Vector3(0, 1f, 0) }) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new CylinderMesh() { Height = 0.1f, Diameter = 2}) .AddComponent(new MeshRenderer()) .AddComponent(new RigidBody3D() { PhysicBodyType = RigidBodyType3D.Kinematic, LinearFactor = Vector3.Zero, // Lock the object position to avoid gravity fall... }) .AddComponent(new CylinderCollider3D()); Entity gear2 = new Entity() .AddComponent(new Transform3D() { LocalPosition = new Vector3(1.5f, 0, 0) }) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new CylinderMesh() { Height = 0.1f, Diameter = 1 }) .AddComponent(new MeshRenderer()) .AddComponent(new RigidBody3D() { PhysicBodyType = RigidBodyType3D.Dynamic, LinearFactor = Vector3.Zero, // Lock the object position to avoid gravity fall... }) .AddComponent(new CylinderCollider3D()); Entity gear3 = new Entity() .AddComponent(new Transform3D() { LocalPosition = new Vector3(-1f, 0.25f, 0), LocalRotation = new Vector3(0, 0, MathHelper.PiOver2) }) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new CylinderMesh() { Height = 0.1f, Diameter = 0.5f, Tessellation = 8 }) .AddComponent(new MeshRenderer()) .AddComponent(new RigidBody3D() { PhysicBodyType = RigidBodyType3D.Dynamic, LinearFactor = Vector3.Zero, // Lock the object position to avoid gravity fall... }) .AddComponent(new CylinderCollider3D()); gear1.AddComponent(new GearJoint3D() { ConnectedEntityPath = gear2.EntityPath, Ratio = 0.5f, Axis = Vector3.UnitY, }); gear2.AddComponent(new GearJoint3D() { ConnectedEntityPath = gear3.EntityPath, Ratio = 0.5f, Axis = -Vector3.UnitX, }); this.Managers.EntityManager.Add(gear1); this.Managers.EntityManager.Add(gear2); this.Managers.EntityManager.Add(gear3); }"
  },
  "manual/physics/joints/generic_6dof_joint.html": {
    "href": "manual/physics/joints/generic_6dof_joint.html",
    "title": "Generic 6DoF Joint | Evergine Doc",
    "keywords": "Generic 6DoF Joint The Generic 6DoF Joint can emulate a variety of standard constraints if each of the six Degrees of Freedom (DoF) is configured. The first 3 DoFs axis are linear axis, which represent the translation of rigid bodies, while the latter 3 DoFs axis represent the angular motion. Each axis can be locked, free, or limited. By default, all axes are unlocked. Generic6DoFJoint3D In Evergine, a Generic 6DoF Joint is implemented using the Generic6DoFJoint3D component. Properties Common Properties Property Default Description ConnectedEntityPath null The entity path of the connected body. Only when the path is valid a Joint is established properly. Anchor 0, 0, 0 The point which defines the center of the joint in source entity local space. All physics-based simulations use this point as the center in calculations. AutoConfigureConnected true Enable this setting to automatically calculate the ConnectedAnchor position to match the global position of the anchor property. This is the default setting. Disable it to configure the position of the connected anchor manually. ConnectedAnchor auto-calculated Manually configure the connected anchor position, in the connected body local space. BreakPoint 0 If the value is greater than 0, Indicates the force that needs to be applied for this joint to break. CollideConnected false Determines whether a collision between the two bodies managed by the joint is enabled. Limit Properties The following properties set the limits of the object movement. Property Default Description UseLinearLimit false If enabled, the position of the connected object will be restricted within the LowerAngularLimit & UpperAngularLimit values.. LowerLinearLimit 0 The lower distance of the limit. UpperLinearLimit 0 The upper distance of the limit. UseAngularLimit false If enabled, the angular rotations will be restricted within the LowerAngularLimit & UpperAngularLimit values.. LowerAngularLimit 0 The lowest angle the rotation can go. UpperAngularLimit 0 The highest angle the rotation can go. Using Generic 6DoF Joint This snippet uses a Generic6DoF to replicate the same functionality showed in the Slider Joint example. protected override void CreateScene() { this.Managers.RenderManager.DebugLines = true; // Load your material var material = this.Managers.AssetSceneManager.Load<Material>(DefaultResourcesIDs.DefaultMaterialID); var cubeMaterial = this.Managers.AssetSceneManager.Load<Material>(EvergineContent.CrateMat); float sliderLength = 3; // Create the slider holder... Entity slider = new Entity() .AddComponent(new Transform3D() { Scale = new Vector3(sliderLength, 0.1f, 0.1f), Rotation = new Vector3(0, 0, MathHelper.ToRadians(-10)) // Rotate 10º the slider axis }) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new CubeMesh()) .AddComponent(new MeshRenderer()) .AddComponent(new RigidBody3D() { PhysicBodyType = RigidBodyType3D.Kinematic }); // Create the sliding object... Entity cube = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent() { Material = cubeMaterial }) .AddComponent(new CubeMesh() { Size = 0.5f }) .AddComponent(new MeshRenderer()) .AddComponent(new RigidBody3D() { PhysicBodyType = RigidBodyType3D.Dynamic, }); // Create the Joint slider.AddComponent(new Generic6DofJoint3D() { ConnectedEntityPath = cube.EntityPath, UseLinearLimit = true, // Limit the slider LowerLinearLimit = new Vector3(0, 0, -sliderLength / 2), UpperLinearLimit = new Vector3(0, 0, sliderLength / 2), }); this.Managers.EntityManager.Add(slider); this.Managers.EntityManager.Add(cube); }"
  },
  "manual/physics/joints/hinge_joint.html": {
    "href": "manual/physics/joints/hinge_joint.html",
    "title": "Hinge Joint | Evergine Doc",
    "keywords": "Hinge Joint The Hinge Joint restricts the translation and two additional angular degrees of freedom, so the body can only rotate around one defined axis. This joint is useful for representing doors or wheels rotating around an axis. The user can specify limits and motor settings for the hinge. HingeJoint3D In Evergine, a Hinge Joint is implemented using the HingeJoint3D component. Common Properties Property Default Description ConnectedEntityPath null The entity path of the connected body. Only when the path is valid a Joint is established properly. Axis 1, 0, 0 The direction of the axis around which the body swings. The direction is defined in local space. Anchor 0, 0, 0 The position of the axis around which the body swings. The position is defined in local space. AutoConfigureConnected true Enable this setting to automatically calculate the ConnectedAnchor position and ConnectedAxis to match the global position of the anchor property. This is the default setting. Disable it to configure the position of the connected anchor and connected axis manually. ConnectedAxis auto-calculated The joint axis relative to the connected body. ConnectedAnchor auto-calculated Manually configure the connected anchor position, in the connected body local space. BreakPoint 0 If the value is greater than 0, Indicates the force that needs to be applied for this joint to break. CollideConnected false Determines whether a collision between the two bodies managed by the joint is enabled. Limit Properties The following properties set the limits of the rotation movement. Property Default Description UseAngularLimit false If enabled, the angle of the hinge will be restricted within the LowerAngularLimit & UpperAngularLimit values.. LowerAngularLimit 0 The lowest angle the rotation can go. UpperAngularLimit 0 The highest angle the rotation can go. LimitSoftness 0.9 Once an angle is greater than softness * the maximum angle, the constraint begins to take effect. Lowering the value of softness softens the constraint boundaries. LimitBiasFactor 0.3 The rate at which the constraint corrects errors in orientation. A value of 1 will ensure that the constraint is always obeyed. It is recommended to keep bias between 0.2 and 0.5. LimitRelaxationFactor 1 The rate at which the angular velocity is changed by the constraint. A low value means the constraint will modify the velocities slowly, leaving the boundaries appearing softer. Motor Properties The following properties set the motor properties of the joint. Property Default Description UseMotor false If enabled, the motor makes the object spin around. MotorTargetVelocity 0 The speed the object tries to attain. MotorTargetImpulse 0 The impulse applied in order to attain the speed. Using Hinge Joint This snippet creates a small bridge of a series of connected bodies. protected override void CreateScene() { this.Managers.RenderManager.DebugLines = true; // Load your material var cubeMaterial = this.Managers.AssetSceneManager.Load<Material>(DefaultResourcesIDs.DefaultMaterialID); int chainLength = 6; Entity previousLink = null; var size = new Vector3(0.5f, 0.1f, 2); var separation = 0.6f; // Create the chain... for (int i = 0; i < chainLength; i++) { /// The first object is kinematic (we don't want a falling chain :D) var rigidObjectType = (i == 0) ? RigidBodyType3D.Kinematic : RigidBodyType3D.Dynamic; // Create the link entities... var link = this.CreateCube(cubeMaterial, new Vector3(i * separation, 0, 0), size, rigidObjectType); if (previousLink != null) { // Add a PointToPoint joint to the previous link connected to the current link... previousLink.AddComponent(new HingeJoint3D() { ConnectedEntityPath = link.EntityPath, Axis = Vector3.Forward, // Sets the hinge axis to Z Anchor = new Vector3(separation / 2, 0, 0) // Sets the Anchor between the Source and Connected body }); } previousLink = link; this.Managers.EntityManager.Add(link); } } private Entity CreateCube(Material material, Vector3 position, Vector3 size, RigidBodyType3D rigidBodyType) { Entity cube = new Entity() .AddComponent(new Transform3D() { Position = position, Scale = size }) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new CubeMesh()) .AddComponent(new MeshRenderer()) .AddComponent(new RigidBody3D() // Add a RigidBody3D component... { PhysicBodyType = rigidBodyType }) .AddComponent(new BoxCollider3D()); // Add a BoxCollider3D to the physic body... return cube; } Add Motor At the end of the previous CreateScene() method, add the following code: var firstHinge = this.Managers.EntityManager.FindFirstComponentOfType<HingeJoint3D>(); firstHinge.UseMotor = true; firstHinge.MotorTargetVelocity = 4; firstHinge.MotorTargetImpulse = 100; This will produce the following effect:"
  },
  "manual/physics/joints/index.html": {
    "href": "manual/physics/joints/index.html",
    "title": "Joints | Evergine Doc",
    "keywords": "Joints A Joint constrains the way two RigidBodies move relative to one another. A typical use for a joint would be to model a door hinge or the shoulder of a character. Evergine has a set of default joint types (point to point, hinge, etc...) that allows developers to make many different types of constraints simply by changing some properties. Joints and RigidBodies In Evergine you can create Joints by adding the proper component (HingeJoint3D for example) to an Entity that contains a RigidBody3D component. This body is treated as Source Rigid Body. A Joint always need to be connected to another RigidBody, which is named Connected Rigid Body. Joint Anchor The position offset of the joint respect the Source RigidBody is called Anchor. The offset position respect the connected Rigid Body is the Connected Anchor. Breaking joints You can use the BreakPoint properties to set limits for the joint's strength. If these are greater than 0, and a force greater than these limits are applied to the object, the joint will be destroyed and will no longer be confined by its restraints. Supported Joint types Joint Description Fixed Joint Fixed Joints restricts an object’s movement to be dependent upon another object. This is somewhat similar to a parent entity but is implemented through physics rather than Entity hierarchy. The best scenarios for using them are when you have objects that you want to easily break apart from each other, or connect two object’s movement without parenting. Point-to-Point Joint The Point-to-Point Joint limits the translation so that pivot points between the two rigid bodies match in world space. You can use the Point to Point joint to create effects, such as a chain-link, or to pin objects together. Hinge Joint The Hinge Joint restricts the translation and two additional angular degrees of freedom, so the body can only rotate around one defined axis. This joint is useful for representing doors or wheels rotating around an axis. The user can specify limits and motor settings for the hinge. Slider Joint The Slider Joint allows rigid bodies to rotate around one axis and translate along the same axis. Cone Twist Joint For ragdolls, the Cone-Twist Joint is useful for limbs like the upper arm. It is a special point-to-point joint that adds cone and twist axis limits. Gear Joint A Gear Joint is a type of constraint that allows you to create a relationship between two rigid bodies that is based on their angular velocities. Specifically, it lets you specify a gear ratio between the angular velocities of the two bodies, which can be used to create realistic mechanical systems like gears, pulleys, and other types of machinery. Generic 6DoF Joint The Six Degrees-Of-Freedom Joint can emulate a variety of standard constraints if each of the six Degrees of Freedom (DoF) is configured. The first 3 DoFs axis are linear axis, which represent the translation of rigid bodies, while the latter 3 DoFs axis represent the angular motion. Each axis can be locked, free, or limited. By default, all axes are unlocked. Spring Joint The Spring Joint is a variant of the previous Six Degrees-of-Freedom joint that includes the addition of springs for each of the degrees of freedom. Springs and motors cannot be combined on this constraint. In this section Fixed Joint Point to Point Joint Hinge Joint Slider Joint Cone Twist Joint Gear Joint Generic 6DoF Joint Spring Joint"
  },
  "manual/physics/joints/point_to_point_joint.html": {
    "href": "manual/physics/joints/point_to_point_joint.html",
    "title": "Point to Point Joint | Evergine Doc",
    "keywords": "Point to Point Joint The Point-to-Point Joint limits the translation so that pivot points between the two rigid bodies match in world space. You can use the Point to Point joint to create effects, such as a chain-link, or to pin objects together. FixedJoint3D In Evergine, a Point-to-Point Joint is implemented using the PointToPointJoint3D component. Properties Property Default Description ConnectedEntityPath null The entity path of the connected body. Only when the path is valid a Joint is established properly. Anchor 0, 0, 0 The point which defines the center of the joint in source entity local space. All physics-based simulations use this point as the center in calculations. AutoConfigureConnected true Enable this setting to automatically calculate the ConnectedAnchor position to match the global position of the anchor property. This is the default setting. Disable it to configure the position of the connected anchor manually. ConnectedAnchor auto-calculated Manually configure the connected anchor position, in the connected body local space. BreakPoint 0 If the value is greater than 0, Indicates the force that needs to be applied for this joint to break. CollideConnected false Determines whether a collision between the two bodies managed by the joint is enabled. Using Point-to-Point Joint This snippet creates two bodies and add a fixed joint maintaining the relative position at start. This is because the AutoConfigureConnected property is set to true. protected override void CreateScene() { // Load your material var cubeMaterial = this.Managers.AssetSceneManager.Load<Material>(EvergineContent.CrateMat); int chainLength = 6; Entity previousLink = null; // Create the chain... for (int i = 0; i < chainLength; i++) { /// The first object is kinematic (we don't want a falling chain :D) var rigidObjectType = (i == 0) ? RigidBodyType3D.Kinematic : RigidBodyType3D.Dynamic; // Create the link entities... var link = this.CreateCube(cubeMaterial, new Vector3(i, 0, 0), 0.75f, rigidObjectType); if (previousLink != null) { // Add a PointToPoint joint to the previous link connected to the current link... previousLink.AddComponent(new PointToPointJoint3D() { ConnectedEntityPath = link.EntityPath, }); } previousLink = link; this.Managers.EntityManager.Add(link); } } private Entity CreateCube(Material material, Vector3 position, float size, RigidBodyType3D rigidBodyType) { Entity cube = new Entity() .AddComponent(new Transform3D() { Position = position }) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new CubeMesh() { Size = size }) .AddComponent(new MeshRenderer()) .AddComponent(new RigidBody3D() // Add a RigidBody3D component... { PhysicBodyType = rigidBodyType }) .AddComponent(new BoxCollider3D()); // Add a BoxCollider3D to the physic body... return cube; }"
  },
  "manual/physics/joints/slider_joint.html": {
    "href": "manual/physics/joints/slider_joint.html",
    "title": "Slider Joint | Evergine Doc",
    "keywords": "Slider Joint The Slider Joint allows rigid bodies to rotate around one axis and translate along the same axis. SliderJoint3D In Evergine, a Hinge Joint is implemented using the SliderJoint3D component. Common Properties Property Default Description ConnectedEntityPath null The entity path of the connected body. Only when the path is valid a Joint is established properly. Axis 1, 0, 0 The direction of the axis around which the body slides. The direction is defined in local space. Anchor 0, 0, 0 The position of the axis around which the body slides. The position is defined in local space. AutoConfigureConnected true Enable this setting to automatically calculate the ConnectedAnchor position and ConnectedAxis to match the global position of the anchor property. This is the default setting. Disable it to configure the position of the connected anchor and connected axis manually. ConnectedAxis auto-calculated The joint axis relative to the connected body. ConnectedAnchor auto-calculated Manually configure the connected anchor position, in the connected body local space. BreakPoint 0 If the value is greater than 0, Indicates the force that needs to be applied for this joint to break. CollideConnected false Determines whether a collision between the two bodies managed by the joint is enabled. Limit Properties The following properties set the limits of the rotation movement. Property Default Description UseLinearLimit false If enabled, the position of the sliding object will be restricted within the LowerAngularLimit & UpperAngularLimit values.. LowerLinearLimit 0 The lower distance of the limit. UpperLinearLimit 0 The upper distance of the limit. UseAngularLimit false If enabled, the angle of the slider will be restricted within the LowerAngularLimit & UpperAngularLimit values.. LowerAngularLimit 0 The lowest angle the rotation can go. UpperAngularLimit 0 The highest angle the rotation can go. Motor Properties The following properties set the motor properties of the joint. Property Default Description UseLinearMotor false If enabled, the motor makes the object spin around. TargetLinearMotorVelocity 0 The linear speed the object tries to attain. MaxLinearMotorForce 0 The linear force applied in order to attain the speed. UseAngularMotor false If enabled, the motor makes the object spin around. TargetAngularMotorVelocity 0 The angular speed the object tries to attain. MaxAngularMotorForce 0 The angular impulse applied in order to attain the speed. Using Slider Joint This snippet creates a small bridge of a series of connected bodies. protected override void CreateScene() { this.Managers.RenderManager.DebugLines = true; // Load your material var material = this.Managers.AssetSceneManager.Load<Material>(DefaultResourcesIDs.DefaultMaterialID); var cubeMaterial = this.Managers.AssetSceneManager.Load<Material>(EvergineContent.CrateMat); float sliderLength = 3; // Create the slider holder... Entity slider = new Entity() .AddComponent(new Transform3D() { Scale = new Vector3(sliderLength, 0.1f, 0.1f), Rotation = new Vector3(0, 0, MathHelper.ToRadians(-10)) // Rotate 10º the slider axis }) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new CubeMesh()) .AddComponent(new MeshRenderer()) .AddComponent(new RigidBody3D() { PhysicBodyType = RigidBodyType3D.Kinematic }); // Create the sliding object... Entity cube = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent() { Material = cubeMaterial }) .AddComponent(new CubeMesh() { Size = 0.5f }) .AddComponent(new MeshRenderer()) .AddComponent(new RigidBody3D() { PhysicBodyType = RigidBodyType3D.Dynamic, }); // Create the Joint slider.AddComponent(new SliderJoint3D() { ConnectedEntityPath = cube.EntityPath, UseLinearLimit = true, // Limit the slider LowerLinearLimit = -sliderLength / 2, UpperLinearLimit = sliderLength / 2 }); this.Managers.EntityManager.Add(slider); this.Managers.EntityManager.Add(cube); }"
  },
  "manual/physics/joints/spring_joint.html": {
    "href": "manual/physics/joints/spring_joint.html",
    "title": "Spring Joint | Evergine Doc",
    "keywords": "Spring Joint The Spring Joint is a variant of the previous Six Degrees-of-Freedom joint that includes the addition of springs for each of the degrees of freedom. Springs and motors cannot be combined on this constraint. SpringJoint3D In Evergine, a Spring Joint Joint is implemented using the Generic6DoFJoint3D component. Properties Common Properties Property Default Description ConnectedEntityPath null The entity path of the connected body. Only when the path is valid a Joint is established properly. Anchor 0, 0, 0 The point which defines the center of the joint in source entity local space. All physics-based simulations use this point as the center in calculations. AutoConfigureConnected true Enable this setting to automatically calculate the ConnectedAnchor position to match the global position of the anchor property. This is the default setting. Disable it to configure the position of the connected anchor manually. ConnectedAnchor auto-calculated Manually configure the connected anchor position, in the connected body local space. BreakPoint 0 If the value is greater than 0, Indicates the force that needs to be applied for this joint to break. CollideConnected false Determines whether a collision between the two bodies managed by the joint is enabled. Limit Properties The following properties set the limits of the object movement. Property Default Description UseLinearLimit false If enabled, the position of the connected object will be restricted within the LowerAngularLimit & UpperAngularLimit values.. LowerLinearLimit 0 The lower distance of the limit. UpperLinearLimit 0 The upper distance of the limit. UseAngularLimit false If enabled, the angular rotations will be restricted within the LowerAngularLimit & UpperAngularLimit values.. LowerAngularLimit 0 The lowest angle the rotation can go. UpperAngularLimit 0 The highest angle the rotation can go. Spring Properties The following properties set the spring functionality of this joint. Property Default Description UseSpringTranslationX, Y, Z false If enabled, apply the spring functionality in the axis X, Y or Z respectively. EquilibriumPointTranslationX, Y, Z 0 Sets the equilibrium point of the spring forces along the X, y or Z translation axis. SpringStiffnessTranslationX, Y, Z 0 Sets the spring stiffness along the X, Y or Z translation axis. SpringDampingTranslationX, Y, Z 1 Sets the spring damping along the X, Y or Z translation axis. UseSpringRotationX, Y, Z false If enabled, apply the spring functionality in the axis X, Y or Z respectively. EquilibriumPointRotationX, Y, Z 0 Sets the equilibrium point of the spring forces along the X, y or Z Rotation axis. SpringStiffnessRotationX, Y, Z 0 Sets the spring stiffness along the X, Y or Z Rotation axis. SpringDampingRotationX, Y, Z 1 Sets the spring damping along the X, Y or Z Rotation axis. Using Spring Joint This snippet uses a SpringJoint to add some spring functionality given in the Generic6DoF Joint example. protected override void CreateScene() { this.Managers.RenderManager.DebugLines = true; // Load your material var material = this.Managers.AssetSceneManager.Load<Material>(DefaultResourcesIDs.DefaultMaterialID); var cubeMaterial = this.Managers.AssetSceneManager.Load<Material>(EvergineContent.CrateMat); float sliderLength = 3; // Create the slider holder... Entity slider = new Entity() .AddComponent(new Transform3D() { Scale = new Vector3(sliderLength, 0.1f, 0.1f), Rotation = new Vector3(0, 0, MathHelper.ToRadians(-45)) // Rotate 45º the slider axis }) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new CubeMesh()) .AddComponent(new MeshRenderer()) .AddComponent(new RigidBody3D() { PhysicBodyType = RigidBodyType3D.Kinematic }); // Create the sliding object... Entity cube = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent() { Material = cubeMaterial }) .AddComponent(new CubeMesh() { Size = 0.5f }) .AddComponent(new MeshRenderer()) .AddComponent(new RigidBody3D() { PhysicBodyType = RigidBodyType3D.Dynamic, }); // Create the Joint slider.AddComponent(new SpringJoint3D() { ConnectedEntityPath = cube.EntityPath, UseLinearLimit = true, // Limit the slider LowerLinearLimit = new Vector3(0, 0, -sliderLength / 2), UpperLinearLimit = new Vector3(0, 0, sliderLength / 2), UseSpringTranslationZ= true, // Add a linear spring in Z axis EquilibriumPointTranslationZ = 0, SpringStiffnessTranslationZ = 10, SpringDampingTranslationZ = 0.02f, }); this.Managers.EntityManager.Add(slider); this.Managers.EntityManager.Add(cube); }"
  },
  "manual/physics/physicmanager_bullet.html": {
    "href": "manual/physics/physicmanager_bullet.html",
    "title": "PhysicManager and Bullet | Evergine Doc",
    "keywords": "PhysicManager and Bullet In Evergine, Physics are implemented with the PhysicManager3D class, a SceneManager. This manager is responsible to create and maintain the Physic World. This is an abstract class that only offer the functionality that a Physic Manager should implement: Create the Physic World Allow to create Physic elements (Bodies, Colliders, etc...) OFfer the possibility to perform physic queries (ray casting, sweep test, etc...). Because PhysicManager3D is a SceneManager, every scene has their own Physic World. PhysicManager3D properties To properly setup your Physic World, the following properties are useful: Property Default Description Gravity 0, -9.8, 0 This is the global gravity vector, in m/s. The default value is 1G (-9.8m/s) PerformPhysicSteps true Indicates if the physic simulation will be updated evey frame. If your scene has dynamics objects (rigidbodies), and this property is set to false, this objects won't be simulated. If your applications only use static bodies, it is recommended to set this property to false. MaxSubSteps 1 In how many subdivisions the physic world updated are divided. Greater values produce better simulations, but the performance is degraded. PhysicWorldResolution 1 The general physic world scale. By default, 1 unit in Evergine is equal to 1 unit in Physic World. Changing this value allows to change this behavior. For example, a value of 10 indicates that a sphere with radius 1 will produce a physic object of 10. Changing this value is useful when the objects are too small or too big. FixedTimeStep 1.0/60 The physic steps will perform at a fixed timestep indicated in this property (in seconds). By default is 60Hz DrawFlags All, -DrawAabb, -DrawContactPoints This flags allows you to select which elements want to be rendered when RenderManager.DebugLines is set to true. By default all elements are included excepht Aabb and contact points: DrawWireframe: Debug draw mesh collider wireframe. DrawAabb: Debug draw bodies AABB. DrawContactPoints: Debug draw the contact points between bodies. DrawConstraints: Debug draw physic joints. DrawConstraintLimits: Debug draw the physic joints limits. DrawFastWireframe: Debug draw wireframes in a fast way. DrawNormals: Debug draws normals. Bullet and BulletPhysicManager3D Evergine provides an implementation for PhysicManagr3D using the open-source Bullet Physics engine. For detailed information, see the Bullet User. In Evergine, the Bullet implementation can be used with the BulletPhysicManager3D. Using BulletPhysicManager3D To start using Physics in your scene, you need to create an instance of BulletPhysicManager3D and register into your Scene. This implementation is provided in the Evergine.Bullet nuget package. Note Evergine project template will register the Bullet physic manager by default. public public class MyScene : Scene { public override void RegisterManagers() { base.RegisterManagers(); this.Managers.AddManager(new Evergine.Bullet.BulletPhysicManager3D()); } // Other scene methods... }"
  },
  "manual/physics/physics_bodies/character_controller.html": {
    "href": "manual/physics/physics_bodies/character_controller.html",
    "title": "Character Controller | Evergine Doc",
    "keywords": "Character Controller The character controller is an physic body type used for the player objects to provide collision-based physics but also to allow for more customizations dedicated to game characters (player or NPCs). It's a common choice for first-person and third-person games. In Evergine, we use the CharacterController3D component to provide this functionality. CharacterController3D Component General properties Property Default Description Restitution 0 Sets the amount of kinetic energy lost or gained after a collision. A typical value is between 0 and 1. If the restitution property of colliding bodies is 0, the bodies lose all energy and stop moving immediately on impact. If the restitution is 1, they lose no energy and rebound with the same velocity they collided at. Use this to change the \"bounciness\" of rigidbodies. Friction 0.5 Sets the surface friction. RollingFriction 0 Sets the rolling friction. IsSensor true Characters are special type of physics bodies and by default is created as a sensor. CollisionCategory Cat1 The CollisionCategory flag specify the category of this body. MaskBit All The MaskBits indicates with which categories this body will collide. Character properties The following properties affect the movement of the rigidbody. Property Default Description StepHeight 0.2 The maximum height the character can step onto. FallSpeed 55 Maximum velocity of a character in a free fall (m/s). By default is 55, the terminal velocity of a sky diver. JumpSpeed 10 Jump speed. MaxSlope 45 Limits the character to only climb slopes that are less steep (in degrees) than the indicated value. Gravity 29.4 3G gravity acceleration. Useful methods To control a character, you may find useful these methods: Method Description SetVelocity(velocity) Sets the character velocity. This is the most usual way to move and control a character. After setting a velocity vector, the character will move in that direction, and will interact with the physic world. Jump(), Jump(jumpDirection) The character perform a jump movement. You can specify an optional jump direction. Teleport(newPosition) Teleport the character to a new position. Create a Character Controller From code This code is just to create a character body: Entity character = new Entity() .AddComponent(new Transform3D(){ Position = characterPosition }) .AddComponent(new MaterialComponent() { Material = material }) // assign a material .AddComponent(new CapsuleMesh()) .AddComponent(new MeshRenderer()) .AddComponent(new CharacterController()) // Add a RigidBody3D component... .AddComponent(new CapsuleCollider3D()); // Assign a CapsuleCollider3D to the physic body... .AddComponent(new MoveCharacterBehavior()) // Add a component that will control the character. this.Managers.EntityManager.Add(character); But we need a component that move the CharacterController3D component. The following component just capture some keys to move and jump the character. public class MoveCharacterBehavior : Behavior { [BindComponent] private CharacterController3D character; public float Speed { get; set; } = 3; protected override void Update(TimeSpan gameTime) { // Gets the keyboard var keyboard = this.Managers.RenderManager.ActiveCamera3D.Display.KeyboardDispatcher; // Use keys (I, K, J, L) to move the character... var velocity = Vector3.Zero; if (keyboard.IsKeyDown(Evergine.Common.Input.Keyboard.Keys.I)) { velocity += Vector3.Forward; } if (keyboard.IsKeyDown(Evergine.Common.Input.Keyboard.Keys.K)) { velocity += Vector3.Backward; } if (keyboard.IsKeyDown(Evergine.Common.Input.Keyboard.Keys.J)) { velocity += Vector3.Left; } if (keyboard.IsKeyDown(Evergine.Common.Input.Keyboard.Keys.L)) { velocity += Vector3.Right; } // Update the character velocity... this.character.SetVelocity(velocity * this.Speed); // Jump if we press the space key... if (keyboard.IsKeyDown(Evergine.Common.Input.Keyboard.Keys.Space)) { this.character.Jump(); } }"
  },
  "manual/physics/physics_bodies/collisions.html": {
    "href": "manual/physics/physics_bodies/collisions.html",
    "title": "Collisions | Evergine Doc",
    "keywords": "Collisions A Collision is produced when two bodies enter in contact. Collision events To listen to all collision produced in a body, a Pysic Body component offers 3 events to be subscribed. Event Description BeginCollision(sender, collisionInfo) Event fired when two objects begin a collision. This event is fired everytime that the owner body collides with a new object. This event send a CollisionInfo3D to provide collision information. UpdateCollision(sender, collisionInfo) This event is fired every frame per colliding object. This event send a CollisionInfo3D to provide collision information. EndCollision(sender, collisionInfo) This event is fired when a body is not yet colliding with the owner body. This event send a CollisionInfo3D to provide collision information. CollisionInfo3D This class provides all collision information between two physics bodies (in physic engines, this is usually called a manifold). Property Description Id Manifold identification. State Indicates the state of this collision info: New, Updated, Deleted. ThisBody The owner body that produces the collision. OtherBody The other body that produces the collision. NumContacts The number of contact points. Methods Description GetContact(contacts) Return all contact points (ContactPoint3D) between this two bodies in the specified collection. GetContact(index, out contact) Return a contact point (ContactPoint3D) specified by its index. ContactPoint3D This structure contains all contact information produced between two objects: Property Description Distance Distance of the contact point to the body surface. Some times the contact can be produced before a real contact happens. In that case is useful to check the distance value. Normal The normal value in the contact. PositionOnThis The contact position in the owner body. PositionOnOther The contact position in the other contacted body. This can be different, due to collider differences, margin properties among other things. ThisCollider The contacted collider in the owner body. OtherCollider The contacted collider in the other body. Using contact events Little example that shows how to use collision events (BeginCollision): protected override void OnActivated() { base.OnActivated(); // Subscribe to the begin collision event... this.rigidBody.BeginCollision += this.BeginCollision; } protected override void OnDeactivated() { base.OnDeactivated(); // Unsubscribe to the begin collision event... this.rigidBody.BeginCollision -= this.BeginCollision; } private void BeginCollision(object sender, CollisionInfo3D collisionInfo) { var lineBatch = this.Managers.RenderManager.LineBatch3D; ContactPoint3D contactPoint; // Iterate over all contact points... for (int contactIndex = 0; contactIndex < collisionInfo.NumContacts; contactIndex++) { if (collisionInfo.GetContact(contactIndex, out contactPoint)) { // Draw the contact position lineBatch.DrawPoint(contactPoint.PositionOnThis, 0.2f, Color.Red); // Draw the contact normal lineBatch.DrawLine(contactPoint.PositionOnThis, contactPoint.PositionOnThis + (contactPoint.Normal * 0.5f), Color.Yellow); } } } Collision filtering So far in every scene we have made, all the physics bodies were able to collide with all the other bodies. That is the default behavior, but it's also possible to set up 'collision filters' to provide finer control over which bodies can collide with each other. Collision filtering is implemented by setting some properties. These flags are: CollisonCategory MaskBits Both flags are a 32 bit integer so you can have up to 32 different categories for collision (Cat1, Cat2, Cat3, .... Cat32). There is a little more to it than that though, because it is the combination of these values that determines whether two bodies will collide. Is it possible to combine some categories, and for that case there is a special value All, that is the aggregation of all categories. The CollisionCategory flag can be thought of as the physic body saying 'I am a ...', and the MaskBits is like saying 'I will collide with a ...'. The important point is that these conditions must be satisfied for both bodies in order for collision to be allowed. Collision filtering properties Every physic body (static, rigid or character) has the following properties: Property Default Description CollisionCategory Cat1 The CollisionCategory flag specify the category of this body. MaskBit All The MaskBits indicates with which categories this body will collide."
  },
  "manual/physics/physics_bodies/index.html": {
    "href": "manual/physics/physics_bodies/index.html",
    "title": "Physics Bodies | Evergine Doc",
    "keywords": "Physics Bodies A Physics Body is an object that can interact with the physic world. In some cases is being affected by dynamic forces such as gravity, or collides with other bodies. Physics Bodies use their attached Colliders to define their physical shape. Type of Physics Bodies Rigid Bodies Rigid bodies are moved around by forces such as collision and gravity. In general, rigidbodies are good choice for objects that can be moved or pushed like boxes, furniture, and obstacles. In Evergine we use RigidBody3D component. Static Bodies Static bodies are not affected by any physic force, and as a result it don't move. Rigid bodies can collide with static bodies. In general, static bodies can be used for objects that are immovable, like walls, floors, etc... In Evergine we use StaticBody3D component. [(static_bodies) Character Controller Is a special type of body used for player controlled characters. It is usually controlled by the user inputs. In Evergine we use CharacterController3D component. Vehicles Is it possible to simulate physical vehicles by using a standard Rigid Body and applying several components to set the Vehicle behavior. In Evergine, add a PhysicVehicle3D to a RigidBody entity, and add several PhysicWheel3D for each desired wheel. Physics Bodies and Colliders As we mentioned before, a physic body itself doesn't define its shape. The entity that owns the Physic Body need at least one Collider component to interact with other bodies. For example, a rigid body without colliders will pass through a floor, because it doesn't have any shape to cause collisions. In this section Rigid Bodies Static Bodies Character Controller Vehicle Physics Collisions Sensors Using Physics Bodies"
  },
  "manual/physics/physics_bodies/rigid_bodies.html": {
    "href": "manual/physics/physics_bodies/rigid_bodies.html",
    "title": "Rigid Bodies | Evergine Doc",
    "keywords": "Rigid Bodies Rigid bodies are moved around by forces such as collision and gravity. In general, rigidbodies are good choice for objects that can be moved or pushed like boxes, furniture, and obstacles. In Evergine, we use the RigidBody3D component to provide the functionality required to turn an Entity into a rigid body. Every frame, the PhysicManager update the state of every registered rigid body, changing their poses and detecting collisions. RigidBody3D Component In Evergine, we use the RigidBody3D component to provide the functionality required to turn an Entity into a rigid body. General properties Property Default Description Mass 1.0 Sets the mass of the rigid body. Try to avoid larger mass ratios (differences). Restitution 0 Sets the amount of kinetic energy lost or gained after a collision. A typical value is between 0 and 1. If the restitution property of colliding bodies is 0, the bodies lose all energy and stop moving immediately on impact. If the restitution is 1, they lose no energy and rebound with the same velocity they collided at. Use this to change the \"bounciness\" of rigidbodies. Friction 0.5 Sets the surface friction. RollingFriction 0 Sets the rolling friction. CollisionCategory Cat1 The CollisionCategory flag specify the category of this body. MaskBit All The MaskBits indicates with which categories this body will collide. Motion properties The following properties affect the movement of the rigidbody. Property Default Description LinearVelocity 0,0,0 Sets the initial linear velocity. LinearFactor 1,1,1 This property is used to limit the translation of a rigidbody. It's separated in X, Y, Z. If one of this axis is set to 1, the movement is not limited, and if a value is set to 0, this axis is locked. LinearDamping 0 Used to slow down the movement of an object. The higher the drag the more the object slows down. AngularVelocity 0,0,0 Sets the initial angular velocity. AngularFactor 1,1,1 This property is used to limit the rotation of a rigidbody. It's separated in X, Y, Z. If one of this axis is set to 1, the rotation is not limited, and if a value is set to 0, this axis is locked. AngularDamping 0 Used to slow down the rotation of an object. The higher the drag the more the rotation slows down. OverrideGravity false Indicates if this rigidbody will define their own gravity value, instead of using the common value defined in PhysicManager. Gravity 0,-9.8,0 If OverrideGravity is true, the rigidbody will use this value as the gravity intensity. IsSensor false If you set a physic body to be a sensor, other colliders no longer bump into it. Instead, they pass through. Sensors detects when bodies enter it, which you can use in your application. Kinematic Rigid Bodies There are two types of rigid bodies, dynamics and kinematic bodies. In Dynamic rigidbodies, their movement is driven by the physic simulation. Receive forces and can be pushed by other bodies. In contrary, Kinematic rigidbodies are not influenced by forces (such as gravity), and have no momentum. They cannot be pushed by other rigid bodies and their movement is only controlled by changing it's transform. An elevator platform is a good example of a kinematic rigid body. Although kinematic rigidbodies aren't moved by physics, other objects can still collide with them. For example, in the case of an elevator, objects placed inside won't fall through the elevator floor. Is it possible to specify the type of rigidbody with the PysicBodyType property: Property Default Description PhysicBodyType Dynamic There are two types of rigid bodies: Dynamic: Their movement is driven by the physic simulation. Receive forces and can be pushed by other bodies. Kinematic: A Kinematic rigidbodies are not influenced by forces (such as gravity), and have no momentum. They cannot be pushed by other rigid bodies and their movement is only controlled by changing it's transform. Create a RigidBody You only need to add a RigidBody3D to an entity, to become this into a rigid body. Don't forget to add a proper collider too. From code Little example to create a bouncing sphere: Entity bouncingBall = new Entity() .AddComponent(new Transform3D(){ Position = position }) .AddComponent(new MaterialComponent() { Material = material }) // assign a material .AddComponent(new SphereMesh()) .AddComponent(new MeshRenderer()) .AddComponent(new RigidBody3D() // Add a RigidBody3D component... { Restitution = 1.0f // Create a bouncing ball (restitution 1) }) .AddComponent(new SphereCollider3D()); // Assign a SphereCollider3D to the physic body... this.Managers.EntityManager.Add(bouncingBall);"
  },
  "manual/physics/physics_bodies/sensors.html": {
    "href": "manual/physics/physics_bodies/sensors.html",
    "title": "Sensors | Evergine Doc",
    "keywords": "Sensors If you set a physic body to be a sensor, other bodies no longer bump into it. Instead, they pass through. The sensor detects when other bodies enter it, which you can use in your application. For example, you can detect when a the user finger touch a button, and use it to launch an action. Sensor properties Every physic body (static, rigid or character) can be set to sensor with the IsSensor property: Property Default Description IsSensor false If you set a physic body to be a sensor, other colliders no longer bump into it. Instead, they pass through. Sensors detects when bodies enter it, which you can use in your application."
  },
  "manual/physics/physics_bodies/static_bodies.html": {
    "href": "manual/physics/physics_bodies/static_bodies.html",
    "title": "Static Bodies | Evergine Doc",
    "keywords": "Static Bodies Static bodies are not affected by any physic force, and as a result it don't move. Rigid bodies can collide with static bodies. In general, static bodies can be used for objects that are immovable, like walls, floors, etc... In Evergine, we use the StaticBody3D component to provide the functionality required to turn an Entity into a Static body. Note You can change the position of a static body changing the entity Transform, but it is not recommended if you want to apply motions. In that case use a Kinematic RigidBody Static Component In Evergine, we use the StaticBody3D component to provide the functionality required to turn an Entity into a Static body. General properties Property Default Description Restitution 0 Sets the amount of kinetic energy lost or gained after a collision. A typical value is between 0 and 1. If the restitution property of colliding bodies is 0, the bodies lose all energy and stop moving immediately on impact. If the restitution is 1, they lose no energy and rebound with the same velocity they collided at. Use this to change the \"bounciness\" of rigidbodies. Friction 0.5 Sets the surface friction. RollingFriction 0 Sets the rolling friction. IsSensor false If you set a physic body to be a sensor, other colliders no longer bump into it. Instead, they pass through. Sensors detects when bodies enter it, which you can use in your application. CollisionCategory Cat1 The CollisionCategory flag specify the category of this body. MaskBit All The MaskBits indicates with which categories this body will collide. Create a Static Body You only need to add a StaticBody3D to an entity, to become this into a static body. Don't forget to add a proper collider too. From code In the following code we will create a floor plane of 10x10 units: Entity floor = new Entity() .AddComponent(new Transform3D(){ Position = position }) .AddComponent(new MaterialComponent() { Material = floorMaterial }) // assign a material .AddComponent(new PlaneMesh(){ Width = 10, Height = 10 }) // Create a 10x10 floor plane .AddComponent(new MeshRenderer()) .AddComponent(new StaticBody3D()) // Add a StaticBody3D component... .AddComponent(new BoxCollider3D()); // Assign a BoxCollider3D to the physic body... this.Managers.EntityManager.Add(floor);"
  },
  "manual/physics/physics_bodies/using_physics_bodies.html": {
    "href": "manual/physics/physics_bodies/using_physics_bodies.html",
    "title": "Using Physics Bodies | Evergine Doc",
    "keywords": "Using Physics Bodies This is a small step by step to cover a variety of physics bodies. This is easily to replicate in Evergine Studio. 1. Create a pile of rigid bodies In the following example we will create a simple pile of 10 rigid bodies and a static body as a floor: protected override void CreateScene() { // Load your material var cubeMaterial = this.Managers.AssetSceneManager.Load<Material>(EvergineContent.Materials.CubeMaterial); var floorMaterial = this.Managers.AssetSceneManager.Load<Material>(DefaultResourcesIDs.DefaultMaterialID); // Create the floor this.CreateFloor(floorMaterial); // Create 10 cubes for (int i = 0; i < 10; i++) { this.SpawnCube(cubeMaterial, new Vector3(0, 1 + i, 0), 0.5f); } } private void CreateFloor(Material material) { Entity cube = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new PlaneMesh() { Width = 10, Height = 10 }) // Create a 10x10 floor plane .AddComponent(new MeshRenderer()) .AddComponent(new StaticBody3D()) // Add a StaticBody component... .AddComponent(new BoxCollider3D()); // Add a BoxCollider3D to the physic body... this.Managers.EntityManager.Add(cube); } private void SpawnCube(Material material, Vector3 position, float size) { Entity cube = new Entity() .AddComponent(new Transform3D() { Position = position }) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new CubeMesh() { Size = size }) .AddComponent(new MeshRenderer()) .AddComponent(new RigidBody3D()) // Add a RigidBody3D component... .AddComponent(new BoxCollider3D()); // Add a BoxCollider3D to the physic body... this.Managers.EntityManager.Add(cube); } 2. Adding a Kinematic body In this step we will add a kinematic body to the scene and we are going to add a simple shake movement: Add the following code to your scene: protected override void CreateScene() { // Previous code of step 1... // Create a kinematic body this.CreateKinematic(); } private void CreateKinematic() { // Load your material var material = this.Managers.AssetSceneManager.Load<Material>(EvergineContent.Materials.Kinematic); Entity cube = new Entity() .AddComponent(new Transform3D() { Position = new Vector3(0, 0.5f, 0) }) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new CubeMesh() { Size = 1 }) .AddComponent(new MeshRenderer()) .AddComponent(new RigidBody3D() // Add a RigidBody3D component... { PhysicBodyType = RigidBodyType3D.Kinematic // Kinematic rigid body... }) .AddComponent(new BoxCollider3D()) // Add a BoxCollider3D to the physic body... .AddComponent(new ShakeKinematic()); // Add a behavior that move this entity... this.Managers.EntityManager.Add(cube); } This is the ShakeKinematic component, that move the entity: public class ShakeKinematic : Behavior { [BindComponent] public Transform3D transform; [BindService] public Clock clock; private Vector3 initPosition; protected override void Start() { base.Start(); this.initPosition = this.transform.Position; } protected override void Update(TimeSpan gameTime) { this.transform.Position = initPosition + new Vector3((float)Math.Cos(5 * clock.TotalTime.TotalSeconds), 0, 0); } }"
  },
  "manual/physics/physics_bodies/vehicle_physics.html": {
    "href": "manual/physics/physics_bodies/vehicle_physics.html",
    "title": "Vehicle Physics | Evergine Doc",
    "keywords": "Vehicle Physics The car model is based on Real Car New made by Maker Game Studios Is it possible to simulate physical vehicles by using a standard Rigid Body and applying several components to set the Vehicle behavior. This is a simplified vehicle physic model. Instead of simulation each wheel and chassis as separate rigid bodies, connected by joints, it uses a simplified model. The entire vehicle is represented as a single Rigid Body, the chassis. The collision detection of the wheels is approximated using ray-casting, and the tire friction is a basic anisotropic friction model. To start using Physic vehicles in Evergine, we need to use two components: PhysicVehicle3D: Add this component to the chassis rigid body that we want to turn into a vehicle. PhysicWheel3D: Add this component to each entity that you want to act as a wheel for a vehicle. PhysicVehicle3D In Evergine, a vehicle is implemented using the PhysicVehicle3D component. General vehicle properties Property Default Description SuspensionStiffness 20 Suspension stiffness is a measure of how much force is required to compress the suspension of a vehicle. SuspensionCompression 4.4 This property determines how much the suspension will compress when the wheel hits a bump or obstacle. SuspensionCompression 2.3 This property determines the rate at which the suspension spring will compress and rebound. It controls the amount of damping force that is applied to the suspension when the vehicle encounters bumps or uneven terrain. MaxSuspensionTravel 5.0 It represents the maximum distance that the suspension can be compressed or extended. It is measured in units of length. FrictionSlip 1000 It refers to the amount of slip that occurs between the tire and the ground during motion. Friction slip is typically modeled using a friction coefficient that represents the ratio of the tangential force to the normal force between the tire and the ground. This coefficient can be adjusted to simulate different levels of slip and traction for different types of vehicles and surfaces. MaxSuspensionForce 6000 Specifies the maximum force that the suspension can apply to the chassis of the vehicle. It is used to limit the effect of the suspension when it is compressed or stretched, preventing the vehicle from bouncing too much or losing stability. Control the vehicle Using the following methods you can control your vehicle: Method Description ApplyEngineForce(force) This method is used to apply the engine force that will be applied to the vehicle. This can be adjusted to increase or decrease the speed of the vehicle, and it can be set to a negative value to go backwards. The engine force will be applied only to the drive wheels. SetSteeringValue(steering) This method is used to set the steering angle of the vehicle. Use this method to change the vehicle direction. The steering value is only applied to steerable wheels. SetBrake(brake) Indicates the brake force applied by the vehicle. The steering value is only applied to brakable wheels. PhysicWheel3D In Evergine, a wheel is implemented using the PhysicWheel3D component. Vehicle attachment Properties First of all, it's necessary to indicate how which vehicle entity the wheel will be associated. Property Default Description SearchVehicle FromParents Indicates the strategy to search the associated vehicle: FromParents: The wheel entity is a child of the Chassis vehicle. Find the first PhysicVehicle3D component in its descendants. FromEntityPath: The vehicle entity is selected by indicating its Entity Path. PhysicVehicleEntityPath null Indicates the Entity Path of the vehicle entity. Wheel Axis Properties The following properties helps to define how the wheel directions of suspension and rotation among other things Property Default Description WheelRadius 0.5 Determines the wheel radius. The WheelRadius value can be used in two ways: If the Entity has a mesh (with MeshComponent for example), the Size value is relative to the mesh extents. If the entity hasn't any meshes, the Size value is used as scene units. RotationAxis 1, 0, 0 The RotationAxis property is used to define the axis around which the wheel rotates. It also indicates the advance direction when applying engine force. It is specified in the local space of the wheel entity. SuspensionDirection 0, -1, 0 Indicates the direction in which the wheel's suspension operates. It also defines the steering wheel direction. It is specified in the local space of the wheel entity. SuspensionRestLength 0.2 Defines the length of the wheel's suspension when the vehicle is in its resting position. Flags Properties In the following properties you can specify how the wheel will be used in the car (steerable, breakable, etc...) Property Default Description IsFrontWheel false It indicates if this is a front wheel or not. This property, in combination with others, helps to define the behavior of the wheel. For example, a steerable wheel which is not a front wheel will be steered in the opposite direction that a front wheel. IsSteerableWheel false If true, the wheel will be affected by the vehicle steering values. IsDriveWheel true If true, the wheel will be affected by engine force. IsBrakableWheel true Determine if the wheel is able to brake or not. With the above properties, you can define different types of vehicles (4x4, font-wheel-drive, etc.) Controlling the wheel individually Above we described how to control your vehicle using a sets of methods to control the general steering, brake and engine force values of the vehicle. You can also avoid to use this methods and set individually these properties to the wheel directly. This allows you to set different values to each wheel (different steering values per wheel for example). Property Description Steering Indicates the steering angle of this wheel. EngineForce The force applied by the engine to this wheel. Brake The brake force acting to this wheel. Overriding vehicle settings As we mentioned before, the PhysicVehicle3D component defines a series of physical properties of the suspensions. By default, all of these properties are applied to all wheel equally, but you can override these properties and set your specific values. Property Default Description OverrideVehicleSettings false If true, this wheel will ignore the properties defined in the vehicle and will specify its own values. If false, the following property values will be ignored. SuspensionStiffness 20 Suspension stiffness is a measure of how much force is required to compress the suspension of a vehicle. SuspensionCompression 4.4 This property determines how much the suspension will compress when the wheel hits a bump or obstacle. SuspensionCompression 2.3 This property determines the rate at which the suspension spring will compress and rebound. It controls the amount of damping force that is applied to the suspension when the vehicle encounters bumps or uneven terrain. MaxSuspensionTravel 5.0 It represents the maximum distance that the suspension can be compressed or extended. It is measured in units of length. FrictionSlip 1000 It refers to the amount of slip that occurs between the tire and the ground during motion. Friction slip is typically modeled using a friction coefficient that represents the ratio of the tangential force to the normal force between the tire and the ground. This coefficient can be adjusted to simulate different levels of slip and traction for different types of vehicles and surfaces. MaxSuspensionForce 6000 Specifies the maximum force that the suspension can apply to the chassis of the vehicle. It is used to limit the effect of the suspension when it is compressed or stretched, preventing the vehicle from bouncing too much or losing stability. Using Physics Vehicles In the following snippet we are going to create a simple vehicle and start controlling it. Create the vehicle protected override void CreateScene() { this.Managers.RenderManager.DebugLines = true; // Load your material var floorMaterial = this.Managers.AssetSceneManager.Load<Material>(EvergineContent.FloorMat); var vehicleMaterial = this.Managers.AssetSceneManager.Load<Material>(DefaultResourcesIDs.DefaultMaterialID); // Add a floor var floor = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent() { Material = floorMaterial }) .AddComponent(new PlaneMesh() { Width = 20, Height = 20 }) .AddComponent(new MeshRenderer()) .AddComponent(new StaticBody3D()) .AddComponent(new BoxCollider3D()) ; // Create the vehicle entity var vehicle = new Entity() .AddComponent(new Transform3D() { LocalPosition = new Vector3(0, 2, 0) }) .AddComponent(new RigidBody3D() { Mass = 800, // 800Kg }) .AddComponent(new PhysicVehicle3D()); // Create the chassis var chassis = new Entity() .AddComponent(new Transform3D() { LocalScale = new Vector3(1.8f, 1, 3) }) .AddComponent(new MaterialComponent() { Material = vehicleMaterial }) .AddComponent(new CubeMesh()) .AddComponent(new MeshRenderer()) .AddComponent(new BoxCollider3D()); vehicle.AddChild(chassis); // Add Wheels vehicle.AddChild(this.AddWheel( vehicleMaterial, new Vector3(1, -0.5f, 1.5f), true, true)); vehicle.AddChild(this.AddWheel(vehicleMaterial, new Vector3(-1, -0.5f, 1.5f), true, true)); vehicle.AddChild(this.AddWheel(vehicleMaterial, new Vector3(1, -0.5f, -1.5f), false, false)); vehicle.AddChild(this.AddWheel(vehicleMaterial, new Vector3(-1, -0.5f, -1.5f), false, false)); this.Managers.EntityManager.Add(floor); this.Managers.EntityManager.Add(vehicle); } private Entity AddWheel(Material material, Vector3 position, bool isFront, bool isSteerable) { return new Entity() .AddComponent(new Transform3D() { LocalPosition = position, LocalRotation = new Vector3(0, 0, MathHelper.PiOver2) // Rotate the cylinder 90º }) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new CylinderMesh() { Height = 0.2f }) .AddComponent(new MeshRenderer()) .AddComponent(new PhysicWheel3D() { IsFrontWheel = isFront, IsSteerableWheel = isSteerable, RotationAxis = Vector3.UnitY, SuspensionDirection = -Vector3.UnitX }); } Control the car! We just need to add a simple custom behavior to start controlling the car: First of all, create a custom Behavior, with the following code: public class VehicleController : Behavior { [BindService] private GraphicsContext graphicsContext; [BindComponent] private PhysicVehicle3D vehicle; public float MaxForce = 1000; // Max Engine force public float Brake = 20; // Max breke force public float MaxSteering = MathHelper.ToRadians(35); public float SteeringSmooth = 0.5f; private float currentSteering = 0; private float steeringVelocity = 0; protected override void Update(TimeSpan gameTime) { var keyboard = this.Managers.RenderManager.ActiveCamera3D.Display.KeyboardDispatcher; float engineForce = 0; // If press the W or Up Arrow, move forward... if ((keyboard.ReadKeyState(Keys.W) == ButtonState.Pressed) || (keyboard.ReadKeyState(Keys.Up) == ButtonState.Pressed)) { engineForce = MaxForce; } // If press the S or Down Arrow, reverse direction... else if ((keyboard.ReadKeyState(Keys.S) == ButtonState.Pressed) || (keyboard.ReadKeyState(Keys.Down) == ButtonState.Pressed)) { engineForce = -MaxForce / 3; } // Apply the brake if we press the space bar var brake = keyboard.ReadKeyState(Keys.Space) == ButtonState.Pressed ? Brake : 0; // Sets the Steering value by aplying D,A or Left and Right Arrow... var steeringRotation = 0f; if ((keyboard.ReadKeyState(Keys.D) == ButtonState.Pressed) || (keyboard.ReadKeyState(Keys.Right) == ButtonState.Pressed)) { steeringRotation = MaxSteering; } else if ((keyboard.ReadKeyState(Keys.A) == ButtonState.Pressed) || (keyboard.ReadKeyState(Keys.Left) == ButtonState.Pressed)) { steeringRotation = -MaxSteering; } // Smooth the steering... this.currentSteering = MathHelper.SmoothDamp(this.currentSteering, steeringRotation, ref this.steeringVelocity, this.SteeringSmooth, (float)gameTime.TotalSeconds); // Apply the engine force, brake and steering values... this.vehicle.ApplyEngineForce(engineForce); this.vehicle.SetSteeringValue(this.currentSteering); this.vehicle.SetBrake(brake); } } Later, add this component to the vehicle entity created in the previous section: vehicle.AddComponent(new VehicleController());"
  },
  "manual/physics/queries.html": {
    "href": "manual/physics/queries.html",
    "title": "Physic Queries | Evergine Doc",
    "keywords": "Physic Queries You can use physics Raycast queries to determine whether a specific line segment intersects physics geometry. Similarly a SweepTest query tests whether a shape extruded along a line segment intersects with physics geometry. Example uses for these queries might include determining whether an object is in front of another object, or testing a line of sight. Raycast Raycast queries are the most common scene query, based on firing a ray from a start position a specified distance along a ray direction. There are two ways to perform a raycast: Closest Hit: Return the closest hit position detected by the raycast All Hits: Return all hits produced between the start and end position. Method Description Raycast(from, to, ...) Perform a raycast between the specified from and to positions. Returns a HitResult3D instance with the result. Raycast(ray, distance, ...) Perform a raycast defined a Ray (position and direction) and a maximum distance. Returns a HitResult3D instance with the result. RaycastAll(from, to, resultsOutput, ...) Perform a raycast between the specified from and to positions. Returns all hits in a HitResult3D collection, which need to be passed as argument. RaycastAll(ray, distance, resultsOutput, ...) Perform a raycast defined a Ray (position and direction) and a maximum distance. Returns all hits in a HitResult3D collection, which need to be passed as argument. Using raycast from code [BindComponent] private Transform3D transform; public float RayDistance {get; set;} = 10 private List<HitResult3D> hitCollection = new List<HitResult3D>(); protected override void Update(TimeSpan gameTime) { // Launch a raycast from the transform position, pointing to the transform forward... var from = this.transform.Position; var to = from + (this.transform.WorldTransform.Forward * this.RayDistance); // Perform a hit test, getting the closest result... var hitResult = this.Managers.PhysicManager3D.RayCast(ref from, ref to); if (hitResult.Succeeded) { this.DebugHit(hitResult); Console.WriteLine(\"Hit detected!\"); } // Perform a hit test, getting all results... // Clear the previous hits.. this.hitCollection.Clear(); this.Managers.PhysicManager3D.RayCastAll(ref from, ref to, this.hitCollection); foreach (var hit in this.hitCollection) { this.DebugHit(hit); Console.WriteLine(\"Process hit!\"); } } Sweep test A Sweep test query is similar to a raycast query except that a sweep query takes a Collider as well as a point and direction. The collider shape is swept along the ray to form a volume. Anything that intersects with this volume is returned from the query. Method Description ConvexSweepTest(colliderShape, from, to, ...) Perform a sweep test between the specified from and to transforms (Matrix4x4 values, because you can specify position, orientation, scale). Returns a HitResult3D instance with first position in which the collider hit. ConvexSweepTestAll(colliderShape, from, to, resultOutput, ...) Perform a sweep test between the specified from and to transforms (Matrix4x4 values, because you can specify position, orientation, scale). Returns all hits in a HitResult3D collection, which need to be passed as argument. Using Sweep test from code [BindComponent] private Collider3D collider; [BindComponent] private Transform3D transform; public float RayDistance {get; set;} = 10 private List<HitResult3D> hitCollection = new List<HitResult3D>(); protected override void Update(TimeSpan gameTime) { // Launch a sweeptest from the transform position, pointing to the transform forward... var from = this.transform.WorldTransform; var to = from * Matrix4x4.CreateTranslation(this.transform.WorldTransform.Forward * this.RayDistance); // Perform a sweep test, getting the closest result... var hitResult = this.Managers.PhysicManager3D.ConvexSweepTest(collider.InternalColliderShape,ref from, ref to); if (hitResult.Succeeded) { this.DebugHit(hitResult); Console.WriteLine(\"Hit detected!\"); } // Perform a sweep test, getting all results... // Clear the previous hits.. this.hitCollection.Clear(); this.Managers.PhysicManager3D.ConvexSweepTestAll(collider.InternalColliderShape, ref from, ref to, this.hitCollection); foreach (var hit in this.hitCollection) { this.DebugHit(hitResult); Console.WriteLine(\"Process hit!\"); } } HitResult3D All physic queries use the HitResult3D structure to return all hit information. This structure contains all the required information to process a hit result: Property Description Succeeded Indicates if the query has successfully hit another object. Point The hit position in world space. Normal The hit normal vector. HitFraction A value between [0-1] indicating if the hit is produced in the start position 0, or at least end position 1 PhysicBody The hit physic body Collider The hit collider. You need to take in consideration that a physic body can have multiple colliders. TriangleIndex If the hit collider is a MeshCollider, specify the intersected triangle index in this mesh. Using HitResult3D from code In the previous sample we used a DebugHit() method: private void DebugHit(HitResult3D hitResult) { var lineBatch = this.Managers.RenderManager.LineBatch3D; // Draw the hit position lineBatch.DrawPoint(hitResult.Point, 0.2f, Color.Red); // Draw the hit normal lineBatch.DrawLine(hitResult.Point, hitResult.Point + (hitResult.Normal * 0.5f), Color.Yellow); }"
  },
  "manual/platforms/android/index.html": {
    "href": "manual/platforms/android/index.html",
    "title": "Android platform | Evergine Doc",
    "keywords": "Android platform Create a new application Use Evergine launcher to add Android as one of your project platforms. Current Android template runs with Xamarin Android, and we have a new Android .NET6 template that runs with .NET6 for Android. Prerequisites If you choose Android template, you need to install Xamarin and cross-platform development tools using Visual Studio installer. In other hand, if you choose Android .NET6 template, you should install Android workload for .NET6. You can do it using a PowerShell console. dotnet workload install android Project structure For both flavors of Android templates, project structure is similar. The differences are: Project type, as .NET6 solution uses latest .NET SDK project style. For .NET6 project, Evergine libraries targets also copy dll map files, that have no effect in Android projects that runs under .NET6. Application deployment There is no support for deployment on Android emulators, so you should use a physical device for development. Evergine uses Vulkan as graphics backend for Android, that is included since Android Nougat and later versions."
  },
  "manual/platforms/index.html": {
    "href": "manual/platforms/index.html",
    "title": "Evergine platforms | Evergine Doc",
    "keywords": "Evergine platforms This section documents the different platforms that Evergine can be deployed on. In this section Windows Web UWP Linux Android iOS"
  },
  "manual/platforms/uwp/index.html": {
    "href": "manual/platforms/uwp/index.html",
    "title": "UWP platform | Evergine Doc",
    "keywords": "UWP platform Desktop and tablets You can add UWP as one of the platforms for your project, using Universal Windows Platform template. With this template, you can create an application that runs in devices such PC, tablet or Xbox. Project targets Windows 10 build 16299 as minimal version, and Windows 10 build 18362 as target version. You can also deploy applications to HoloLens devices, as stated in Mixed Reality section. Prerequisites You have to install target SDKs to build and run the application. You can find download links here. Application deployment As a standard UWP application, you can deploy your application in your local Windows device, in a remote device or in a physical device connected by USB to your development machine. Mixed Reality If you want to create an Evergine project for HoloLens you should use Mixed Reality template. Project targets Windows 10 build 16299 as minimal version, and Windows 10 build 18362 as target version. XR capabilities Evergine for Mixed Reality relies on a set on interfaces and classes defined in Evergine.Framework.XR.* namespace: QR code detection with IQRCodeWatcherService. Spatial anchors with SpatialAnchor. Spatial mapping with SpatialMappingObserver and SpatialMappingSurface. Application deployment In this case, there are two options for application deployment: Deploy in HoloLens emulator: before deploying, you need to install device emulator from official documentation page. Deploy in a HoloLens device: remember that you should mark ARM64 as active configuration to deploy an application for HoloLens 2. For more information, read official documentation."
  },
  "manual/platforms/web/getting_started.html": {
    "href": "manual/platforms/web/getting_started.html",
    "title": "Getting started with a Evergine web application | Evergine Doc",
    "keywords": "Getting started with a Evergine web application Create a new application From the Evergine launcher you can create a Web (WebGL2.0) project, or add the web profile from the Evergine Studio. This template adds two projects to the web solution, the Web one which is the web application client, that uses typescript to execute Evergine into a web canvas, and an optional Server project that it is just an Asp server application that implements some optimiziations for asset loading that can only be done from server side. After editing your scene from Evergine Studio as usual, run the Web.server project or the Web project from Visual Studio 2022 to see it running on the browser. Finally, check it out how to deploy the app and improve its performance."
  },
  "manual/platforms/web/index.html": {
    "href": "manual/platforms/web/index.html",
    "title": "Evergine on the web with WebGL and WebAssembly | Evergine Doc",
    "keywords": "Evergine on the web with WebGL and WebAssembly An Evergine application can be deployed as a web app, using WebGL 2.0 as rendering API. Evergine web applications can run statically on client side quite fast by the use of Blazor Web Assembly for native code execution. Furthermore, they are optimzed for fast loading, by parallelizing asset downloading and leveraging compressing techniques such as Brotli and Zip. Finally, to improve the loading as much as possible, which is crucial in web sites, we also provide an optional ASP server application that implements some optimizations that can only be done from server side. Prerequisites .Net6.0 SDK Install Visual Studio 2022 (Recommended) Alternatively, Install latest dotnet SDK release. Install wasm-tools From Visual Studio Installer, add web development workload and the .Net WebAssembly build tools individual component. Alternatively, from a root terminal: dotnet workload install wasm-tools --skip-manifest-update Limitations Due to the very recent support of WebGL2.0 by iOS devices, support on its Safari browser is still a work in progress. In this section Getting started DevOps Tips"
  },
  "manual/platforms/web/io.html": {
    "href": "manual/platforms/web/io.html",
    "title": "Comming soon | Evergine Doc",
    "keywords": "Comming soon"
  },
  "manual/platforms/web/ops.html": {
    "href": "manual/platforms/web/ops.html",
    "title": "Detailed information to develop & debug & deploy Evergine for the web | Evergine Doc",
    "keywords": "Detailed information to develop & debug & deploy Evergine for the web Build Use VS2022 or VSCode/Terminal. You can build and test only the client project (web), the server is only needed for publishing with compression (see below). dotnet build -c [Debug|Release] ./sample.web/sample.web.[Server.]csproj Run From VS2022 you can run the profile sample.web[.Server]. There is also an IIS Express profile for each cliente and server projects, but it is usually slower. Additionally you can publish the app dotnet publish -c [Debug|Release] ./sample.web/sample.web.csproj and run the app by populating the folder ./sample.web/bin/[Debug|Release]/net6.0/publish/wwwroot. In this second case we do recommend to use VSCode Live Server, instead of Fenix, as the second has known issues with Web Assembly. Debug Debug is now possible from VS2022 by running the application in debug mode only on Chrome browser. Publish with Compression To publish the application with automatic compression (Brotli & GZip), the AspNetCore server is needed. Use VS2022 or from Terminal: dotnet publish -c Release -r win-x86 --self-contained ./sample.web.Server/sample.web.Server.csproj If done from terminal, you can publish the files in sample.web.Server/bin/Release/net6.0/win-x86/publish to an AspNetCore server, from VSCode or manually."
  },
  "manual/platforms/web/tips.html": {
    "href": "manual/platforms/web/tips.html",
    "title": "Tips to optimize a web application | Evergine Doc",
    "keywords": "Tips to optimize a web application Web platform have some technology limitations compared to regular desktop apps. Specifically, loading time is much worse than applications stored in the local computer, and execution power is reduces.Therefore it is important to apply some optimizations to get good performance on the browser. Reduce texture sizes: Textures are usually one of the biguest assets. To improve the loading time, try to use reduce its resolution to the possible minimum."
  },
  "manual/xr/index.html": {
    "href": "manual/xr/index.html",
    "title": "XR (Extended-Reality) | Evergine Doc",
    "keywords": "XR (Extended-Reality) What is XR? XR (Extended Reality) is an umbrella terms that covers the following types of applications: Virtual Reality (VR) The application simulates a completely different environment around the user. Mixed Reality (MR) The application combines its own environment with the user's real-world environment and allows them to interact with each other. Augmented Reality (AR) The application ayers content over a digital view of the real world. Evergine XR In Evergine, you can develop with different XR platforms and solutions, and Evergine XR helps users to deal with multiples XR technologies. Evergine XR is an abstraction layer that allows multiple XR implementations (OpenXR, Mixed Reality, etc...) to be used in a standardized way in your interactive applications. Evergine XR is included by default in the Evergine.Framework and Evergine.Components NuGet packages, and exposes an abstract Service (XRPlatform) which is the entry point to start creating XR applications. Different XR technologies integrations gives an implementation of the XRPlatform service, and it is provided using Evergine extensions or add-ons. This functionality will be covered in the following documents of this section. In this section XRPlatform Input Devices Spatial Mapping Spatial Anchors Trackable Items"
  },
  "manual/xr/input_tracking/advancedtrackxrdevice.html": {
    "href": "manual/xr/input_tracking/advancedtrackxrdevice.html",
    "title": "AdvancedTrackXRDevice | Evergine Doc",
    "keywords": "AdvancedTrackXRDevice This component allows you to track all kind of XR Devices, not only controllers or hands, such as base stations, generic trackers, etc... Selection Strategy This component can select the device that will track in different ways, and it is defined by the SelectionStrategy property, that has the following values: ByHandedness: Specify the device using the handedness (left or right device). This is the most usual way to select a device. ByDeviceType: Select the device by specifying the type of the device and an number of occurrences. ByDeviceTypeAndHandedness: Select the device by specifying the type of the device and the handedness. ByDeviceIndex: The InputTracking maintain an ordered list of tracked devices. If you use this way the device will be selected by specifying an index on that list. Selection by Handedness If your selection strategy is ByHandedness, you will select your device by using its handedness. In that case you need to se these properties: Property Description Handedness With this property you will indicate the handedness of the device that you want to track: LeftHand to specify the left hand. RightHand to specify the right hand. Undefined if the device has no specific handedness. DeviceIndex In the case that there are more than one devices using the same handedness, with this property you will specify which occurence are going to be selected. Selection by Device Type If your selection strategy is ByDeviceType, you will select your device by using its handedness. In that case you need to se these properties: Property Description DeviceType Specify the type of device that you want to select: Controller if you want to select a tipical controller device. HMD in the case that you want to track the headset device. GenericTracker to select typical tracker devices (such as Vive Tracker). TrackingReference reffers to devices used as a tracking reference (such as Vive Base Stations or old Oculus Rift Camera Sensors). Hand if the system can detect the user hands. DeviceIndex In the case that there are more than one devices using the same device type, with this property you will specify which occurence are going to be selected. Selection by Device Type and Handedness If your selection strategy is ByDeviceTypeAndHandedness, you will select your device by using its handedness and the specified device type. In that case you need to se these properties: Property Description DeviceType Specify the type of device that you want to select: Controller if you want to select a tipical controller device. HMD in the case that you want to track the headset device. GenericTracker to select typical tracker devices (such as Vive Tracker). TrackingReference reffers to devices used as a tracking reference (such as Vive Base Stations or old Oculus Rift Camera Sensors). Hand if the system can detect the user hands. Handedness With this property you will indicate the handedness of the device that you want to track: LeftHand to specify the left hand. RightHand to specify the right hand. Undefined if the device has no specific handedness. DeviceIndex In the case that there are more than one devices using the same properties described above, with this property you will specify which occurence are going to be selected. Selection by Device Index If your selection strategy is ByDeviceIndex, you will select your device by its index that is registered in your system. Property Description DeviceIndex Integer that specify the index of the device. Tracking lost mode This component will updat the entity Transform to follow the selected device. But in the case that an issue occurs during the tracking process, you can specify how the entity behaves using the following property: Property Description TrackingLostMode Specify the strategy to follow in case that the device is not well tracked: DisableEntityOnPoseInvalid disable the entity if the tracked pose is not valid. In the case that the device is well tracked again, the entity will be enabled again. This is the default value. KeepLastPose stop to track the entity if the pose is not valid, maintaining the entity with the last pose received. DisableEntityOnDisconnection disable the entity only if the selected device is no longer connected. Using AdvancedTrackXRDevice In the following code you will find how to create some sort of entities to track left and right controllers, and two generic trackers: protected override void CreateScene() { base.CreateScene(); // Left entity var leftDevice = new Entity() .AddComponent(new Transform3D()) .AddComponent(new AdvancedTrackXRDevice() { SelectionStrategy = TrackXRDevice.SelectionDeviceStrategy.ByDeviceTypeAndHandedness, DeviceType = XRTrackedDeviceType.Controller, Handedness = XRHandedness.RightHand // select the left device }); // Right entity var rightDevice = new Entity() .AddComponent(new Transform3D()) .AddComponent(new AdvancedTrackXRDevice() { SelectionStrategy = TrackXRDevice.SelectionDeviceStrategy.ByDeviceTypeAndHandedness, DeviceType = XRTrackedDeviceType.Controller, Handedness = XR.XRHandedness.RightHand // select the right device }); this.Managers.EntityManager.Add(leftDevice); this.Managers.EntityManager.Add(rightDevice); // First Tracker var firstTracker = new Entity() .AddComponent(new Transform3D()) .AddComponent(new AdvancedTrackXRDevice() { SelectionStrategy = TrackXRDevice.SelectionDeviceStrategy.ByDeviceType, DeviceType = XRTrackedDeviceType.GenericTracker, DeviceIndex = 0 // First }); // Second Tracker var secondTracker = new Entity() .AddComponent(new Transform3D()) .AddComponent(new AdvancedTrackXRDevice() { SelectionStrategy = TrackXRDevice.SelectionDeviceStrategy.ByDeviceType, DeviceType = XRTrackedDeviceType.GenericTracker, DeviceIndex = 1 // Second }); this.Managers.EntityManager.Add(firstTracker); this.Managers.EntityManager.Add(secondTracker); }"
  },
  "manual/xr/input_tracking/index.html": {
    "href": "manual/xr/input_tracking/index.html",
    "title": "XR Input Tracking | Evergine Doc",
    "keywords": "XR Input Tracking In XR, it's usual that the user interact with different XR input devices such as Motion Controllers, Hands, Trackers, etc.. The InputTracking subsystem of XRPlatform keep control and track all connected devices in the XR session. Using XRPlatform.InputTracking you can obtain and list all connected devices of your system, and get access to its information. However, it is more straightforward to use Components that help developers to integrate the device tracking status into your Entity and components. Track XR Components The following components will change the entity Transform by setting the device position and orientation: TrackXRController: Represent and track a tipical motion controller. This component inherit from TrackXRDevice, and additionally, it will expose the state of its buttons, triggers and much more. TrackXRArticulatedHand: In the case that your XR system has hand tracking support, it will provide access to the articulated hand state. Additionally, it will expose a list of hand joint poses. AdvancedTrackXRDevice: This component allows you to track all kind of xr devices, such as base stations, generic trackers, etc... In this section Tracking Controllers (TrackXRController) Tracking Hands (TrackXRArticulatedHand) Advanced Tracking Devices (AdvancedTrackXRDevice)"
  },
  "manual/xr/input_tracking/trackxrarticulatedhand.html": {
    "href": "manual/xr/input_tracking/trackxrarticulatedhand.html",
    "title": "TrackXRArticulatedHand | Evergine Doc",
    "keywords": "TrackXRArticulatedHand This component is used to track and obtain the state of an articulated hand. A variety of MR and VR platforms supports hand tracking, like Mixed Reality (HoloLens) or Oculus Quest. Supported Hand Joints Evergine supports a list of hand joints to be tracked, it is described using the XRHandJointKind enumeration, and its values can be visually perceived in the following image: Note The entity transform will use the Palm joint pose. Properties The following properties has been added to this component in order to acces hand joint information: Property Description Handedness With this property you will indicate the handedness of the device that you want to track: LeftHand to specify the left hand. RightHand to specify the right hand. Undefined if the device has no specific handedness. TrackingLostMode Specify the strategy to follow in case that the device is not well tracked: DisableEntityOnPoseInvalid disable the entity if the tracked pose is not valid. In the case that the device is well tracked again, the entity will be enabled again. This is the default value. KeepLastPose stop to track the entity if the pose is not valid, maintaining the entity with the last pose received. DisableEntityOnDisconnection disable the entity only if the selected device is no longer connected. SupportedHandJointKind Returns a XRHandJointKind[] array of supported hand joints. Hand tracking in some devices can be limited and only support a limited joints TryGetArticulatedHandJoint() Obtains the articulated hand joint state specified using a XRHandJointKind value. Return true if the joint is susscessfully obtained. Using TrackXRArticulatedHand Create a TrackXRArticulatedHand from code In the following code you will find how to create an entity that track a controller from code: protected override void CreateScene() { base.CreateScene(); var material = this.Managers.AssetSceneManager .Load<Material>(EvergineContneet.DefaultMaterialID); // The hand material // Hand entity var leftHand = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new TeapotMesh() { Size = 0.15f }) .AddComponent(new MeshRenderer()) .AddComponent(new TrackXRArticulatedHand() { Handedness = XRHandedness.LeftHand // select the left hand }) .AddComponent(new DebugArticulatedhand()); this.Managers.EntityManager.Add(leftHand); } Read the Hand Joints Small sample to read the hand joint state: public class DebugArticulatedhand : Behavior { [BindComponent] private TrackXRArticulatedHand trackXRHand; protected override void Update(TimeSpan gameTime) { var lineBatch = this.Managers.RenderManager.LineBatch3D; if (trackXRHand.IsConnected) { // Iterate over all supported joints foreach (var supportedJoint in this.trackXRHand.SupportedHandJointKind) { // Obtain the joint pose and draw it... if (this.trackXRHand.TryGetArticulatedHandJoint(supportedJoint, out var handJoint)) { Matrix4x4.CreateFromTR(ref handJoint.Pose.Position, ref handJoint.Pose.Orientation, out var jointTransform); lineBatch.DrawAxis(jointTransform, 0.01f); // Draw 1cm axis with the joint transform } } } } } Render Hands Using the XRDeviceRenderableModel component add the possibility to obatains a renderable model associated to XR device. In the case of articulated hands, it provides a skinned mesh of the hand following the user hand poses. Note You can add to the entity an optional MaterialComponent component, to specify which material will be used to render the hand meshes. If this component is not provided, they will be rendered by the default material. Render Hands from code Small example that shows how to render both hands using the XRDeviceRenderableModel component. protected override void CreateScene() { base.CreateScene(); var material = this.Managers.AssetSceneManager .Load<Material>(EvergineContneet.DefaultMaterialID); // The hand material // Left hand var leftHand = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new TrackXRArticulatedHand() { Handedness = XRHandedness.LeftHand // select the left hand }) .AddComponent(new XRDeviceRenderableModel()); this.Managers.EntityManager.Add(leftHand); // Right hand var rightHand = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new TrackXRArticulatedHand() { Handedness = XRHandedness.RightHand // select the right hand }) .AddComponent(new XRDeviceRenderableModel()); this.Managers.EntityManager.Add(rightHand); }"
  },
  "manual/xr/input_tracking/trackxrcontroller.html": {
    "href": "manual/xr/input_tracking/trackxrcontroller.html",
    "title": "TrackXRController | Evergine Doc",
    "keywords": "TrackXRController This component is used to track and obtain the controller input state (buttons, triggers, etc...). Most of VR platforms supports controller devices, such as Oculus Touchs, Vive Controllers, or Valve Idnex Controllers. Properties When you add this component to your entity, you will show this in Evergine Studio: Property Description Handedness With this property you will indicate the handedness of the device that you want to track: LeftHand to specify the left hand. RightHand to specify the right hand. Undefined if the device has no specific handedness. TrackingLostMode Specify the strategy to follow in case that the device is not well tracked: DisableEntityOnPoseInvalid disable the entity if the tracked pose is not valid. In the case that the device is well tracked again, the entity will be enabled again. This is the default value. KeepLastPose stop to track the entity if the pose is not valid, maintaining the entity with the last pose received. DisableEntityOnDisconnection disable the entity only if the selected device is no longer connected. ControllerState Structure that expose the controller input state. Controller State Give the user access to the following properties through the XRControllerGenericState structure: Property Description IsConnected Indiacates whether this device is connected. Trigger Return a 0-1 float value of the trigger state. TriggerButton Gives the ButtonState enum of the trigger, to check if it is pressed or not. Grip Indicates if the grip button is pressed. ThumbStick 'Vector2' value that indicates the direction of the tipical mini-stick in the controller. ThumbStickButton 'ButtonState' That indicates if the thumbstick is pressed. Touchpad 'Vector2' value that indicates the direction of the touchpad placed in some VR controllers. Menu 'ButtonState' of the menu button. Button1 'ButtonState' of the typical first button of a controller (it can be usually labelled as A or X). Button2 'ButtonState' of the typical second button of a controller (it can be usually labelled as B or Y). Using TrackXRController Create a TrackXRController from code In the following code you will find how to create an entity that track a controller from code: protected override void CreateScene() { base.CreateScene(); var material = Application.Current.Container .Resolve<AssetsService>() .Load<Material>(DefaultResourcesIDs.DefaultMaterialID); // Left entity var leftDevice = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new TeapotMesh() { Size = 0.15f }) .AddComponent(new MeshRenderer()) .AddComponent(new TrackXRController() { Handedness = XRHandedness.LeftHand // select the left device }) .AddComponent(new ScaleEntityWithController()); // Described in the next section :) this.Managers.EntityManager.Add(leftDevice); } Read the Controller State Small sample to read the controller state: public class ScaleEntityWithController : Behavior { [BindComponent] private Transform3D transform; [BindComponent] private TrackXRController trackXRController; protected override void Update(TimeSpan gameTime) { if (trackXRController.IsConnected) { // Scale the entity if the user press the trigger (from 1 to 2) this.transform.LocalScale = Vector3.One * (1 + trackXRController.ControllerState.Trigger); } } }"
  },
  "manual/xr/integrations/index.html": {
    "href": "manual/xr/integrations/index.html",
    "title": "XR Integrations | Evergine Doc",
    "keywords": "XR Integrations Evergine comes with several XR platform integrations. OpenXR OpenXR is an open royalty-free API standard from Khronos, designed to unify the development across multiple VR/MR/AR (XR) devices. Behind this initiative, there are companies like Meta, Microsoft, HTC, Steam, Varjo, and Magic Leap, allowing support to all their devices. In this section OpenXR"
  },
  "manual/xr/integrations/openxr.html": {
    "href": "manual/xr/integrations/openxr.html",
    "title": "OpenXR | Evergine Doc",
    "keywords": "OpenXR OpenXR is an open royalty-free API standard from Khronos, designed to unify the development across multiple VR/MR/AR (XR) devices. Behind this initiative, there are companies like Meta, Microsoft, HTC, Steam, Varjo, and Magic Leap, allowing support to all their devices. The official support for OpenXR will allow the Evergine developer to create new applications that will support the incoming new XR devices. In this Evergine version, the OpenXR integration covers the main aspects of this standard. We will provide new functionality and specific extensions in future versions, such as eye gazing, and Spatial Mapping, which are very useful in HoloLens devices. Right now, Evergine allows you to create PC desktop applications with Evergine that cover almost the entire variety of wired VR devices and portable Android devices (such as Meta Quest). OpenXRPlatform The OpenXR implementation is provided by the OpenXRPlatform service, this class offer the implementation of XRPlatform. When you create the OpenXRPlatform instance, you have the possibility to specify which extensions do you want to enable. Meta Quest project template The Oculus Quest device is now called Meta Quest after the latest news from Facebook. This is the most popular VR headset right now after selling about 10 million units of the newest Quest 2 device. Thanks to the OpenXR standard, the latest Evergine version brings the possibility to deploy your VR applications into Meta Quest devices. For maximum performance and to support future graphics features, we use Vulkan as the only Graphics API in this platform based on Android. Create a Meta Quest template To start developing your Evergine project with Meta Quest, you only need to select the Android Meta Quest template when you create an Evergine project: On the other hand, if you have previously created an Evergine project, you can add the Meta Quest profile in the Project Settings:"
  },
  "manual/xr/overview.html": {
    "href": "manual/xr/overview.html",
    "title": "General overview | Evergine Doc",
    "keywords": "General overview Coming soon"
  },
  "manual/xr/spatial_anchors.html": {
    "href": "manual/xr/spatial_anchors.html",
    "title": "XR Spatial Anchors | Evergine Doc",
    "keywords": "XR Spatial Anchors A spatial anchor represents an important point in the real world that the system tracks over time. Each anchor has an adjustable coordinate system, based on other anchors or frames of reference, to ensure anchored holograms stay precisely in place. Rendering a hologram in an anchor's coordinate system gives you the most precise positioning for that hologram at any given time. This comes at the cost of small adjustments over time to the hologram's position as the system continually moves it back into place based on the real world. By saving local spatial anchors to disk and loading them back later, your application can calculate the same location in the real world across multiple application sessions on a single HoloLens. While spatial anchors are great for holograms that should remain fixed in the world, once an anchor is placed, it can't be moved. Supported Devices Currently in Evergine, the following devices has Spatial Anchors support: Device XR Platform HoloLens 2 Windows Mixed Reality. You can find useful information here. TrackSpatialAnchor Component In Evergine, you can create Spatial Anchors in your device using the TrackSpatialAnchor component. This component allows you to do: Detect a Spatial Anchor and place the owner entity in the tracked pose (position and orientation). Create a Spatial Anchor to a desired position and orientation. Note During the application running, the position is usually updated to adapt the anchor position to the best predicted pose. Properties and Methods The following properties provides you the way to setup a Spatial Anchor Property Description AnchorId The Anchor Identifier. This is the Id used to find the anchor in the XR Platform. If the XR Platform find a stored anchor with the given ID, it will track and update the position of the owner entity. StoreAnchor() Store the current entity position and orientation as an spatial anchor with the current AnchorId. After this method Using TrackSpatialAnchor from code protected override void CreateScene() { base.CreateScene(); // Spatial Anchor entity var anchor = new Entity() .AddComponent(new Transform3D()) .AddComponent(new TrackSpatialAnchor() { AnchorId = \"AwesomeAnchor\" // The anchor Id. }); this.Managers.EntityManager.Add(anchor); } Low-Level Spatial Anchors While the most usual way to access to Spatial Anchors is using the TrackSpatialAnchorcomponent, you can access to a low-level way that provides more functionality. The XRPlatform.SpatialAnchorStore gives access to all low-level functionality implemented by the XR Platform. If this property is null the XR device doesn't support Spatial Anchor. Another useful class is SpatialAnchor. This class is represent a Spatial Anchor in your XR System. With XRPlatform.SpatialAnchorStore class you can create/delete/update instances of SpatialAnchor. SpatialAnchor properties This class represent a spatial anchor that can be persisted or updated in your XR Platform Property Description Transform (only get) Gets the detected transform (Matrix4x4?) of the Spatial Anchor. XRPlatform.SpatialAnchorStore properties and methods Property Description SavedAnchors Read only dictionary <string, SpatialAnchor> that returns all stored anchors in this device. CreateSpatialAnchor() Create a SpatialAnchor instance in the specified position and orientation. This method only create the SpatialAnchor instance, but not persist this anchor in the device StoreAnchor(id, anchor) Store the specified SpatialAnchor instance with the given identifier. After this method, this anchor will be persisted and can be accessed every time the application runs. RemoveAnchor(id) Remove a stored anchor with the given identifier. RemoveAllAnchors() Remove all persisted anchors in this device. Note There can only be one SpatialAnchor for each identifier Store a Spatial Anchor from code [BindService] private XRPlatform xrPlatform; [BindComponent] private Transform3D transform; public void StoreAnchor(string id) { if (this.xrPlatform.SpatialAnchorStore == null) { throw new InvalidOperationException(\"XRPlatform doesn't support Spatial Anchor\"); } // Create the spatial anchor... var anchor = this.xrPlatform.SpatialAnchorStore.CreateSpatialAnchor(this.transform.Position, this.transform.Orientation); if(anchor != null) { // Store the spatial anchor... this.xrPlatform.SpatialAnchorStore.StoreAnchor(id, anchor); } } Debug all stored anchors from code [BindService] private XRPlatform xrPlatform; protected override void Start() { if (this.xrPlatform.SpatialAnchorStore == null) { throw new InvalidOperationException(\"XRPlatform doesn't support Spatial Anchor\"); } var material = this.Managers.AssetSceneManager .Load<Material>(EvergineContent.DefaultMaterialID); foreach (var anchorId in this.spatialAnchorStore.SavedAnchors.Keys) { Entity anchorEntity = new Entity() .AddComponent(new Transform3D()) .AddComponent(new MaterialComponent() { Material = material }) .AddComponent(new CubeMesh() { Size = 0.1f } ) .AddComponent(new MeshRenderer()) .AddComponent(new TrackSpatialAnchor(){ AnchorId = anchorId // The anchor Id. }); this.Managers.EntityManager.Add(anchorEntity); } }"
  },
  "manual/xr/spatial_mapping.html": {
    "href": "manual/xr/spatial_mapping.html",
    "title": "XR Spatial Mapping | Evergine Doc",
    "keywords": "XR Spatial Mapping Some XR Devices has built-in cameras that continuously scan the environment, allowing it to construct virtual world geometry for real-world objects. This ability allows developers to create convincing interactions between virtual content and the real world (such as occluding and physically interacting with real-world objects). The process of mapping real-world surfaces into the virtual world is called Spatial Mapping. Supported Devices Currently in Evergine, the following devices has Spatial Mapping support: Device XR Platform HoloLens 2 Windows Mixed Reality. SpatialMapping Component To start using Spatial Mapping in your application, you only need to create an Entity and add the SpatialMapping component: This component will request to the XR Platform a collection of Spatial Mapping surfaces, which consist in meshes that conforms the detected environment by the XR Device. Internally, this component will create a collection of child entities, each one is responsible to maintain and render a single surface with the giving properties which will be described after. Properties Mesh generation properties The following properties specify how the spatial mapping meshes will be generated: Property Description TrianglesPerCubicMeter (Default 500) The triangles per cubic meter of the generated Spatial Mapping Meshes. With this property you can control the overall quality of the generated meshes. Lower values generate meshes with less quality but increase the overall performance, and greater values provides a better spatial mapping quality, but with a performance penalty. Default value is 500 ObtainNormals (Default true) Indicates that the generated mesh will provides normals. Default value is true Extends (Default [6, 5, 6]) This property define a volume area (measured in meters) centered in the XR Device user in which all spatial mapping surfaces are updated and generated. Distant surfaces are discarded to prevent a performance penalty. Default value is an area of 6m x 5m x 6m Material Specify which material will be used to render the Spatial Mapping meshes. If no material is provided, the spatial mapping surfaces won't be rendered. Default value is null Update Mesh properties The following properties indicates how often the spatial mapping surfaces will be updated or generated: Property Description UpdateInterval (Default 0) Frequency (in seconds) in which SpatialMapping component will update its surfaces. In case of 0, the spatial mapping will not be updated (keeping the first requested spatial mapping surfaces). Increase this value Default value is 0 MaxSurfaceUpdatesPerFarame (Default 1) Indicate how many surfaces will be processed per frame. When SpatialMapping request an update, it gives a list of surfaces that require to update. Setting this value allows you to control how many surfaces will be processed in a single frame. This prevents the application to increase the CPU usage and cause a FPS drop. Default value is 1 Collision Mesh properties One of most important utilities for Spatial Mapping is to generate Physics colliders of the detected surfaces, allowing to physically interact with the environment. Property Description GenerateColliders (Default false) Indicates if physic colliders will be generated for detected spatial mapping surfaces. Generating colliders has an important performance penalty, so we recommend to only enable colliders if it is necessary Default value is false CollisionCategory (Default Cat1) Specify the CollisionCategory property of spatial mapping physics colliders (It has no effect if GenerateColliders value is false). Please, read Collision Filtering for further details. Default value is Cat1 MaskBits (Default All) Specify the MaskBits properly of spatial mapping physics colliders (It has no effect if GenerateColliders value is false). Please, read Collision Filtering for further details. Default value is All Using SpatialMapping from code protected override void CreateScene() { base.CreateScene(); // Spatial Mapping entity var spatialMapping = new Entity() .AddComponent(new Transform3D()) .AddComponent(new SpatialMapping() { UpdateInterval = 5 // Update spatial mapping surfaces every 5 seconds GenerateColliders = true // Generate colliders }); this.Managers.EntityManager.Add(spatialMapping); }"
  },
  "manual/xr/trackable_items.html": {
    "href": "manual/xr/trackable_items.html",
    "title": "XR Trackable Items | Evergine Doc",
    "keywords": "XR Trackable Items A trackable is something that can be tracked during application execution. This is, and object which changes on its position and orientation can be detected. Then, you can use that information to add 3D elements assoiciated to those trackables. While a trackable is detected, its world transform matrix is updated depending on real-world object changes. There are different types of trackables that can be detected by Evergine: planes, images and faces. They all extend XRTrackableItem as base class. Supported Devices Currently in Evergine, the following devices has Trackable Items support: Device XR Platform Android ARCore XRTrackableItem This base class holds information that is common to any trackable element. Properties and Methods Property Description ID Gets trackable identifier. This is internally generated by Evergine and will be unique for all detected trackables. Transform Stores position and orientation information for the detected trackable. Will be continuously updated while application is running with real-world object information. XRPlaneTrackable This class represents plane detection. Depending on XR platform implementation you can use XRPlaneDetectionType to detect just vertical or horizontal planes, or even enable both types of plane detection. Properties and Methods Property Description BoundaryPolygon Holds information of vertices for an approximation of polygon boundaries for a detected plane. Values are relative to XZ plane that should be transformed by trackable transform information to get boundaries in world-relative coordinates. Size Estimated width and length of the plane. Type Type of detected plane: horizontal with upward or downward facing, or vertical plane. XRImageTrackable Trackable for images, no matter they are static or in movement. Properties and Methods Property Description DatabaseIndex Indicates index of the image in the internal images database. Name Indicates name of the image in the internal images database. Size Size of detected image. TrackingMethod Depending on tracking state: if image is not being tracked, is tracked by last pose information or full tracking. XRFaceTrackable This class is used to face tracking. A single face at the same time would be detected. Properties and Methods Property Description MeshVerticesPointer Pointer to native face mesh vertices buffer. MeshVerticesCount Number of face mesh vertices. MeshTextureCoordinatesPointer Pointer to native face texture coordinates buffer. MeshTextureCoordinatesCount Number of texture coordinates. MeshNormalsPointer Pointer to native face mesh normals buffer. MeshNormalsCount Number of normals. MeshTriangleIndicesPointer Pointer to native mesh index buffer. MeshTriangleIndicesCount Number indexes. LeftForeheadPose Pose for the left forehead. RightForeheadPose Pose for the right forehead. NosePose Pose for the nose."
  },
  "manual/xr/xrplatform.html": {
    "href": "manual/xr/xrplatform.html",
    "title": "XR Platform | Evergine Doc",
    "keywords": "XR Platform XRPlatform is the main service in Evergine to provide XR functionality to your applications. This service (through its implementations) is responsible to: Create and maintain the XR session. Create and maintain all required graphic resources to properly render the scene in the XR environment. Update the scene Camera to track the HMD device. Offer access to different XR subsystems (such as Input Tracking, Spatial Mapping, etc..) Note Is it possible (and common), that some XRPlatform providers does not implement all functionality exposed by this service. For example: SteamVR implementation (Evergine.OpenVR extension) doesn't provide Spatial Mapping functionality or Hand Tracking) Camera3D and HMD tracking One of the main purposes of a XR platform is to track the head position and setup properly the render output to be displayed on the HMD device. With Evergine.XR, and using XRPlatform, you don't need to create special Components or instantiate XR Camera prefabs. You only need to create a common Camera 3D of your scene, and XRPlatform will update every frame its properties to match the HMD device properties. This approach has a lot of advantages, one of them are that you can develop your application with or without XR integration without any change in your scene. XRPlatform sets the following Camera properties: Camera Position & Orientation: Updates with the HMD device pose, including its position and orientation. Camera Projection: The Camera Projection properties is changed to match the projection required by the device. If you register the XRPlatform display as the default Display, the Camera will render directly to the HMD device display. Important Properties You can access to useful information of the current XR session by using the following properties: General properties Property Description Display Returns the display that will be configured to render in the XR device. In order to render your scene, you need to setup your camera to render in that Display. It is done by default if you register the XR Display as the \"DefaultDisplay\" MirrorDisplay This display will be used to mirror the content that is rendering in the XR device. Additionally, it is used too to provide access to Input dispatchers in your application. RenderMirrorTexture Boolean that indicates if you want to mirror the XR content into the Mirror Display. MSAASampleCount Setup the XR platform to create graphics resources using the specified MSAA Sample Count. This helps to increase the objects definition and sharpness, but reduce the application performance. HMD properties You can access to properties regarding the HMD device: Property Description TrackingState Gets the current device tracking state. It can indicates several tracking status, such as that everything is OK, or for contrary, the device is not initialized or can be out of range. HeadGaze Ray that point from the head position to the direction of the device is facing. EyeCount Number of eyes that this device will be rendered. In most common XR applications, the number is 2. Eye Gaze Some XR devices has the possibility to track the eye look direction (HoloLens2 for instance). in that cases Property Description EyeGaze Gets a ray that indicates the position and direction that the user are looking with their eyes. IsEyeGazeValid Boolean that indicates if the eye gaze is providing valid values. This can be false in case that the eye is not well tracked, or in the case that the device doesn't support this feature, or doesn't have permission to track eyes. RequestEyeGazePermission() Async method that request the devices to grant permission to track the user eyes. It will return true only if the device supports eye gazing and the user has approved this interaction (in most of devices it is usually to show a small dialog requesting this permission). A small example of how to use eye gazing in your application: protected override bool OnAttached() { if (base.OnAttached()) { // Request eye gaze permission... this.RequestPermission(); return true; } return false; } protected override void Update(TimeSpan gameTime) { if (this.xrPlatform.IsEyeGazeValid) { var eyeGaze = this.xrPlatform.EyeGaze.Value; // modify the transform to follow the eye gaze... this.transform.LocalPosition = eyeGaze.Position; this.transform.LookAt(eyeGaze.GetPoint(1)); } } private async void RequestPermission() { await this.xrPlatform.RequestEyeGazePermission(); } XR Subsystems XRPlatforms allows you to access more functionality that has been described before. In the following documents these areas will be covered: Input Tracking Spatial Mapping Spatial Anchors"
  }
}